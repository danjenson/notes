<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    Papers
  </title>
  <meta
    name="description"
    content="A collection of work and research by Daniel jenson."
  />

  <!-- Google Fonts -->
  <link
    href="//fonts.googleapis.com/css?family=Lato:400,400italic"
    rel="stylesheet"
    type="text/css"
  />

  <!-- https://docs.mathjax.org/en/latest/input/tex/extensions/index.html -->
  <script>
    MathJax = {
      loader: {
        load: [
          "[tex]/ams",
          "[tex]/gensymb",
          "[tex]/mathtools",
          "[tex]/physics",
        ],
      },
      tex: {
        packages: { "[+]": ["ams", "gensymb", "mathtools", "physics"] },
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
      },
      svg: {
        fontCache: "global",
      },
    };
  </script>
  <!-- https://www.mathjax.org/#gettingstarted -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
  <link rel="stylesheet" type="text/css" href="/notes/assets/css/main.css">

  <link
    rel="canonical"
    href="https://danjenson.github.io/notes/papers/"
  />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
  <nav class="group">
     
    <a href="/notes/books">books</a>
      
    <a href="/notes/papers">papers</a>
      
    <a href="/notes/courses">courses</a>
      
    <a href="/notes/mindmaps">mindmaps</a>
     
    <button
      type="button"
      id="theme-toggle"
      onclick="modeSwitcher()"
      style="cursor: pointer"
    ></button>
  </nav>
  <script src="/notes/assets/js/theme-toggle.js"></script>
</header>

    <article class="group">
<h1 class="title">Papers</h1>
  <ul>
  <li><a href="attention-is-all-you-need">Attention is All You Need</a>: Details the transformer architecture and describes the key-value-based attention mechanism.</li>
  <li><a href="deep-recurrent-q-learning-with-double-q-learning">Deep Recurrent Q-Learning with Double Q-Learning</a>: Using the same network to select and evaluate the maximum value action in a given state leads to overestimation. This paper corrects that by training separate networks for selection and evaluation and periodically swapping them.</li>
  <li><a href="dropout">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
  <li><a href="dueling-network-architectures-for-deep-reinforcement-learning">Dueling Network Architectures for Deep Reinforcement Learning</a>: This paper improves performance on the Atari benchmark by training two models â€“ one for the value of a state and another for the advantage (\(Q_\pi(s,a) - V_\pi(s)\) for a given policy). This allows sharing the value of a state between many actions and allows training to scale better as the number of actions per state increases.</li>
  <li><a href="word2vec">Efficient Estimation of Word Representations in Vector Space</a>: Creating word vectors based on the continuous bag-of-words (CBOW) and skip-gram models dramatically improves performance on word representations in NLP tasks. CBOW predicts the current word from context and skip-gram predicts the context from the current word.</li>
  <li><a href="flow-network-based-generative-models-for-non-iterative-diverse-candidate-generation">Flow Network based Generative Models for Non Iterative Diverse Candidate Generation</a></li>
  <li><a href="gflownet-foundations">GFlowNet Foundations</a></li>
  <li><a href="graph-representations-for-higher-order-logic-and-theorem-proving">Graph Representations for Higher-Order Logic and Theorem Proving</a>: Use GNNs to embed goals and premises and use it to select tactics and premises at each step.</li>
  <li><a href="holist">HOList: An Environment for Machine Learning of Higher-Order Theorem Proving</a>: Presents an open source environment for higher-order theorem proving and a reinforcement learning based model trained on it.</li>
  <li><a href="how-to-read-a-paper">How to Read a Paper</a></li>
  <li><a href="layer-normalization">Layer Normalization</a></li>
  <li><a href="learning-to-prove-theorems-via-interacting-with-proof-assistants">Learning to Prove Theorems via Interacting with Proof Assistants</a>: Introduces CoqGym for training deep learning models using the interactive theorem prover Coq. It also introduces ASTactic, a deep neural network that generates tactics and hence proofs in Coq using a top-down TreeLTSM.</li>
  <li><a href="learning-to-reason-in-large-theories-without-imitation">Learning to Reason in Large Theories without Imitation</a>: Premise selection and exploration using TF-IDF outperforms models trained solely on human proofs and approaches hybrid performance.</li>
  <li><a href="magnetic-control-of-tokamak-plasmas-through-deep-reinforcement-learning">Magnetic control of tokamak plasmas through deep reinforcement learning</a></li>
  <li><a href="mulit-objective-bayesian-optimization-over-high-dimensional-search-spaces">Multi-Objective Bayesian Optimization over High-Dimensional Search Spaces</a></li>
  <li><a href="neurocompositional-computing">Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems</a>: The two types of thinking are Compositional and Continuous and incorporating the former will enable neural networks to reason. This discusses a novel embedding using Tensor Product Representations (TPR) that allows continuous vector representations of compositional objects. This leads to a type of computing called Neurally-Encoded Compositionally-Structured Tensor (NECST) computing. The fundamental idea is that structural roles are encoded along with the terms.</li>
  <li><a href="playing-atari-with-deep-reinforcement-learning">Playing Atari with Deep Reinforcement Learning</a>: Presents the first Deep Q-Network (DQN) to learn control policies for the Atari benchmark.</li>
  <li><a href="prioritized-experience-replay">Prioritized Experience Replay</a>: Prioritizing the replay buffer by the magnitude of the temporal-difference error leads to better performance.</li>
  <li><a href="rainbow">Rainbow: Combining Improvements in Deep Reinforcement Learning</a></li>
  <li><a href="tabular-data">Tabular Data: Deep Learning is Not All You Need</a></li>
  <li><a href="the-consciousness-prior">The Consciousness Prior</a></li>
</ul>

</article>
    <span class="print-footer"
  >Papers - Daniel Jenson
</span>
 <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="daniel.a.jenson@gmail.com"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="https://www.linkedin.com/in/daniel-jenson-7a002a30/"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="https://github.com/danjenson"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2023 &nbsp;&nbsp;DANIEL JENSON</span></br> <br>
<span>This site created with the <a href="//github.com/danjenson/et">Edward Tufte theme for Daniel Jenson </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>

  </body>
</html>
