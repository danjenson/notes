<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    STATS 270: Bayesian Statistics
  </title>
  <meta
    name="description"
    content="A collection of work and research by Daniel jenson."
  />

  <!-- Google Fonts -->
  <link
    href="//fonts.googleapis.com/css?family=Lato:400,400italic"
    rel="stylesheet"
    type="text/css"
  />

  <!-- https://docs.mathjax.org/en/latest/input/tex/extensions/index.html -->
  <script>
    MathJax = {
      loader: {
        load: [
          "[tex]/ams",
          "[tex]/gensymb",
          "[tex]/mathtools",
          "[tex]/physics",
        ],
      },
      tex: {
        packages: { "[+]": ["ams", "gensymb", "mathtools", "physics"] },
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
      },
      svg: {
        fontCache: "global",
      },
    };
  </script>
  <!-- https://www.mathjax.org/#gettingstarted -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
  <link rel="stylesheet" type="text/css" href="/notes/assets/css/main.css">

  <link
    rel="canonical"
    href="http://localhost:4000/notes/courses/stats-270-bayesian-statistics.html"
  />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
  <nav class="group">
     
    <a href="/notes/books">books</a>
      
    <a href="/notes/papers">papers</a>
      
    <a href="/notes/courses">courses</a>
      
    <a href="/notes/mindmaps">mindmaps</a>
     
    <button
      type="button"
      id="theme-toggle"
      onclick="modeSwitcher()"
      style="cursor: pointer"
    ></button>
  </nav>
  <script src="/notes/assets/js/theme-toggle.js"></script>
</header>

    <article class="group">
<h1>STATS 270: Bayesian Statistics</h1>

<p class="subtitle"></p>

<ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#typed-notes">Typed Notes</a>
<ul>
<li class="toc-entry toc-h2"><a href="#terminology">Terminology</a>
<ul>
<li class="toc-entry toc-h3"><a href="#terms">Terms</a></li>
<li class="toc-entry toc-h3"><a href="#notation">Notation</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#chapter-1-statistical-inference">Chapter 1: Statistical Inference</a>
<ul>
<li class="toc-entry toc-h3"><a href="#likelihood-and-inference">Likelihood and Inference</a>
<ul>
<li class="toc-entry toc-h4"><a href="#overview">Overview</a></li>
<li class="toc-entry toc-h4"><a href="#minimum-mean-squares">Minimum Mean Squares</a></li>
<li class="toc-entry toc-h4"><a href="#maximum-likelihood">Maximum Likelihood</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#lectures">Lectures</a>
<ul>
<li class="toc-entry toc-h2"><a href="#lecture-1">Lecture 1</a></li>
<li class="toc-entry toc-h2"><a href="#lecture-2">Lecture 2</a></li>
</ul>
</li>
</ul><h1 id="typed-notes">
<a class="anchor" href="#typed-notes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Typed Notes</h1>

<h2 id="terminology">
<a class="anchor" href="#terminology" aria-hidden="true"><span class="octicon octicon-link"></span></a>Terminology</h2>

<h3 id="terms">
<a class="anchor" href="#terms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Terms</h3>

<ul>
  <li>
<strong>Bias</strong>: The difference between the expected value of an estimator and the
true value of the parameter. There can be mean or median bias.
Mean-unbiasedness is not preserved under non-linear transformations, while
median-unbiasedness is.</li>
  <li>
<strong>Conditional likelihood</strong>: A likelihood of data given a sufficient statistic
for the nuisance parameters so that the likelihood does not depend on them.</li>
  <li>
<strong>Consistency</strong>: Consistent estimators converge to the true value of the
parameter, but may be biased or unbiased.</li>
  <li>
<strong>Expected value</strong>: A generalization of the weighted average (weighted by
probability). Informally, it is the arithmetic mean of a large number of
independently sampled outcomes of a random variable.</li>
  <li>
<strong>Inference</strong>: The process of drawing reliable conclusions from
data subject to random variation. In particular, based on data, inference
draws a conclusion about a parameter $\theta$. Note that in ML/Deep Learning,
“inference” is used slightly differently and often refers to passing input
through a trained model to get predictions.</li>
  <li>
<strong>Likelihood function</strong>: Any function
$\mathcal{L}(\theta;\mathbf{y})=c(\mathbf{y})p(\theta;\mathbf{y})$ that is
proportional to $p(\theta;\mathbf{y})$ for any function $c(\mathbf{y})&gt;0$ that
is independent of the parameter $\theta$ but may depend on $\mathbf{y}$.
Typically, this is written as $\mathcal{L}(\theta\mid\mathbf{y})$, which
emphasizes that it is a function of $\theta$ conditional on observed data.
This is also written as $P(\mathbf{y}\mid\theta)$ or the probability of the
data given a value for $\theta$. Further, note that this is invariant under
linear transformations, i.e. if $\mathbf{z}=g(\mathbf{y})$, mapping $\mathbb{R}^n\to
\mathbb{R}^n$, then
$\mathcal{L}(\theta;\mathbf{z})=p\left(\theta;g^{-1}(\mathbf{z})\right)\left|\pdv{\mathbf{y}}{\mathbf{z}}\right|$
where the last term is the absolute value of the determinant of the Jacobian.</li>
  <li>
<strong>Marginal likelihood</strong>: A likelihood based only on part of the information in
the data to remove the nuisance parameters.</li>
  <li>
<strong>Maximum likelihood estimate (MLE)</strong>: A maximizer of the likelihood function.</li>
  <li>
<strong>Nuisance paramter</strong>: A parameter that the function depends on but that is
not a parameter of interest.</li>
  <li>
<strong>Posterior</strong>: Likelihood $\times$ Prior / Evidence, i.e.
$\frac{\mathcal{L}(\theta\mid\mathbf{y})\times P(\theta)}{\int
P(\mathbf{y}\mid\theta)P(\theta)\dd\theta}$.</li>
  <li>
<strong>Statistic</strong>: a measurable function of $\mathbf{Y}$, i.e. $S=S(\mathbf{Y})$.</li>
  <li>
<strong>Sufficient Statistic</strong>: A statistic such that no other statistic calculated
form the same sample can provide any more information about the parameter of
interest. If a density can be factorized as
$f_{\mathbf{X}}(\mathbf{x})=h(\mathbf{x})g(\theta,T(\mathbf{x}))$, then
$T(\mathbf{x})$ is a sufficient statistic. In particular, from this
factorization, you can see that the MLE estimate, $\arg\max_\theta
h(\mathbf{x})g(\theta,T(\mathbf{x}))$ depends only on
$g(\theta,T(\mathbf{x}))$. Another way of understanding this is that the
conditional distribution of $\mathbf{X}$ given $T(\mathbf{X})$ remains the
same over $\mathcal{F}=\{\Pr_\theta(\mathbf{x}): \theta\in\Omega\}$, i.e.
$\Pr(\mathbf{X}\mid T(\mathbf{X}),\theta)=\Pr(\mathbf{X}\mid T(\mathbf{X}))$.</li>
</ul>

<h3 id="notation">
<a class="anchor" href="#notation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notation</h3>

<ul>
  <li>$p(\theta;\mathbf{y})$: $\theta$ given $\mathbf{y}$, not necessarily
conditional on in an event sense. This is often used in optimization.</li>
  <li>$p(\mathbf{y}\mid\theta)$: $\mathbf{y}$ conditional on the state of affairs or
event that $\theta$ has happened. This usage is restricted to statistics.</li>
  <li>$\Pr_\theta(\mathbf{y})$: Joint probability of $\mathbf{y}$ under parameters
$\theta$, equivalent to $\Pr(\mathbf{y}\mid\theta)$.</li>
  <li>$\mathcal{L}(\theta;\mathbf{y})$: The likelihood function of $\theta$ given $\mathbf{y}$.</li>
  <li>$\mathbb{E}_{\theta}\left[T_n\right]=\mathbb{E}_{y\mid\theta}\left[T_n\right]$:
The expected value of statistic $T_n=T_n(\mathbf{X})$ over $\mathbf{x}$
conditional on $\theta$, i.e. $\int_{-\infty}^\infty
T_n(\mathbf{x})p(\mathbf{x}\mid\theta)\dd x$.</li>
</ul>

<h2 id="chapter-1-statistical-inference">
<a class="anchor" href="#chapter-1-statistical-inference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Chapter 1: Statistical Inference</h2>

<h3 id="likelihood-and-inference">
<a class="anchor" href="#likelihood-and-inference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Likelihood and Inference</h3>

<h4 id="overview">
<a class="anchor" href="#overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview</h4>

<ul>
  <li>Given $\mathbf{Y}=(Y_1,\ldots,Y_n)$ is observed, i.e.
$\mathbf{y}=(y_1,\ldots,y_n)$, our goal is to make an inferential statement
about $\theta$, the parameters of the data generating function.</li>
  <li>Example 1.1: Sample statistic $S=S(\mathbf{Y})=\sum_i Y_i$ with Bernoulli
data:
    <ul>
      <li>Given a sample of $\mathbf{Y}$ where each $Y_i\sim
\operatorname{Ber}(\theta)$, then the probability of the joint distribution
is \(\Pr_\theta(\mathbf{y}) \equiv \Pr(\mathbf{y} \mid
\theta)=\prod_{i=1}^n
\theta^{y_i}(1-\theta)^{1-y_i}=\theta^s(1-\theta)^{n-s}\).</li>
      <li>$\mathbf{Y}$ Can also be viewed as following $\operatorname{Bin}(n;\theta)$,
namely $\Pr_\theta(S=s)=\binom{n}{s}\theta^s(1-\theta)^{n-s}$.</li>
      <li>Now, $\Pr_\theta(\mathbf{Y}=\mathbf{y}\mid
S=s)=\frac{\Pr_\theta(\mathbf{Y}=\mathbf{y},S=s)}{\Pr(S=s)}=\frac{\theta^s(1-\theta)^{n-s}}{\binom{n}{s}\theta^s(1-\theta)^{n-s}}=\frac{1}{\binom{n}{s}}$,
which doesn’t depend on $\theta$. Thus, $S$ is a sufficient statistic for
$\theta$.</li>
      <li>Next, let $N$ be the number of trials required to observe $S=s$ successes,
i.e. negative binomial:
$\Pr_\theta(N=s\mid\theta)=\binom{n-1}{s-1}\theta^s(1-\theta)^{n-s}$.</li>
      <li>Thus, you can see that the following functions share the same likelihood
function, $\mathcal{L}(\theta;\mathbf{y})=\theta^{\sum_{i=1}^n
y_i}(1-\theta)^{n-\sum_{i=1}^n y_i}=\theta^s(1-\theta)^{n-s}$:
        <ul>
          <li>$\Pr(\mathbf{y}\mid\theta)=\prod_{i=1}^n\theta^{y_i}(1-\theta)^{1-y_i}=\theta^s(1-\theta)^{n-s}\to
c\Pr(\mathbf{y}\mid\theta)=\mathcal{L}(\theta;\mathbf{y})\quad \text{i.e. } c=1$</li>
          <li>$\Pr(S=s\mid\theta)=\binom{n}{s}\theta^s(1-\theta)^s\to
c’\Pr(S=s\mid\theta)=\mathcal{L}(\theta;\mathbf{y})\quad\text{i.e.
}c’=\frac{1}{\binom{n}{s}}$</li>
          <li>$\Pr(N\mid\theta)=\binom{n-1}{s-1}\theta^s(1-\theta)^{n-s}\to
c’’ \mathcal{L}(\theta;\mathbf{y})\quad\text{i.e.
}c’’=\frac{1}{\binom{n-1}{s-1}}$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Example 1.2 Linear regression:</p>

    <ul>
      <li>Given: \(Y=\beta_0+\sum_{j=1}^p \beta_j x_j+\varepsilon, \quad \varepsilon \sim N\left(0, \sigma^2\right)\)</li>
      <li>
        <p>You can see from the following that the function depends on $\mathbf{Y}$
through
$RSS=(\mathbf{y}-\mathbf{X}\hat{\beta})^\intercal(\mathbf{y}-\mathbf{X}\hat{\beta})$
(residual sum square) and $\hat{\beta}=(\mathbf{X}^\intercal
\mathbf{X})^{-1}\mathbf{X}^\intercal \mathbf{Y}$, which means $Y\mid
(RSS,\hat{\beta})$ is independent of $\theta$, and, thus $(RSS,\hat{\beta})$
is a sufficient statistic for $\theta$:</p>

\[\begin{aligned} p(\theta ; \mathbf{y}) &amp;=\left(2 \pi \sigma^2\right)^{-n / 2}
\exp \left(-\frac{1}{2 \sigma^2}(\mathbf{y}-\mathbf{X}
\beta)^T(\mathbf{y}-\mathbf{X} \beta)\right) \\ &amp;\left.=\left(2 \pi
\sigma^2\right)^{-n / 2} \exp \left(-\frac{1}{2
\sigma^2}(\mathbf{y}-\mathbf{X} \widehat{\beta})^T(\mathbf{y}-\mathbf{X}
\widehat{\beta})+(\widehat{\beta}-\beta)^T\left(\mathbf{X}^T
\mathbf{X}\right)(\widehat{\beta}-\beta)\right)\right) \\ &amp;=\left(2 \pi
\sigma^2\right)^{-n / 2} \exp \left(-\frac{1}{2 \sigma^2}\left(R S
S+(\widehat{\beta}-\beta)^T\left(\mathbf{X}^T
\mathbf{X}\right)(\widehat{\beta}-\beta)\right)\right), \end{aligned}\]
      </li>
      <li>
<a href="https://stats.stackexchange.com/a/419934/358356">Proof</a> of independence of $\hat{\beta}$ and $RSS$.</li>
    </ul>
  </li>
  <li>In many situations, a likelihood is a function of multiple parameters, but
only a small number of parameters are of interest, the remainder being
<strong>nuisance parameters</strong>. To eliminate these, there are several alternatives,
including the marginal, conditional, and profile likelihoods.</li>
  <li>In general, constructing the conditional and marginal likelihoods may be
non-trivial. The following is a useful method:
    <ul>
      <li>Given $\mathbf{Y}=(V,W)$ can be partitioned into two parts with the
likelihood function of $\mathbf{Y}$ being $p((\theta,\phi);\mathbf{y})$ where
$\theta$ is the parameter of interest and $\phi$ is the nuisance parameter.</li>
      <li>Consider the marginal likelihood of $V$ and the conditional likelihood of
$W$ given $V$; we may choose $V$ so that there is no information about
$\phi$ on $W$. <label for="m-vw" class="margin-toggle sidenote-number"></label><input type="checkbox" id="m-vw" class="margin-toggle"><span class="sidenote">How would you do this? </span>
</li>
    </ul>
  </li>
  <li>Example 1.3 Separating Bernoulli and Poisson Likelihoods:
    <ul>
      <li>Given $\mathbf{Y}=(Y_1,\ldots,Y_N)$ where
$Y_i\sim\operatorname{Bernoulli}(\theta)$ and
$N\sim\operatorname{Poisson}(\phi)$, you can see that the likelihood
functions decompose (below) and that $N$ is a sufficient statistic for
$\phi$; thus, given $n$, the conditional likelihood $\mathcal{L}_{S\mid
N}(\theta; s,n)$ becomes a function of $\theta$ alone.</li>
    </ul>
  </li>
</ul>

\[\begin{aligned}
\mathcal{L}((\theta, \phi) ; \mathbf{y}) &amp; \propto p(\theta ; \mathbf{y} \mid
N=n) p(\phi ; n) \\
&amp;=\prod_{i=1}^n \theta^{y_i}(1-\theta)^{1-y_i} \phi^n \exp (-n
\phi) / n ! \\ &amp; \propto \theta^s(1-\theta)^{n-s} \phi^n \exp (-n \phi) \\
&amp;=\mathcal{L}_{S \mid N}(\theta ; s , n) \mathcal{L}_N(\phi ; n)
\end{aligned}\]

<ul>
  <li>
    <p>Example 1.4 Marginal Likelihood with Normal Distribution:
<label for="m-indep," class="margin-toggle sidenote-number"></label><input type="checkbox" id="m-indep," class="margin-toggle"><span class="sidenote">Go through this example more slowly </span></p>

    <ul>
      <li>Let $Y_ji\sim \operatorname{Normal}(\mu_i,\sigma^2)$ for
$j=1,2,\;i=1,\dots,n$.</li>
      <li>The parameter of interest is $\sigma^2$ and the nuisance parameters are
$\mu_i$ for $i=1,\ldots,n$; hence $\theta=\sigma^2$ and
$\phi=(\mu_i,\ldots,\mu_n)$</li>
      <li>$\bar{y}_i=(y_{1i}+y_{2i})/2$.</li>
      <li>The likelihood function is:
\(L((\theta, \phi) ; \mathbf{y})=\left(2 \pi \sigma^2\right)^{-n} \exp
\left(-\frac{1}{2 \sigma^2}\left(2
\sum_{i=1}^n\left(\bar{y}_i-\mu_i\right)^2+\sum_{i=1}^n \sum_{j=1}^2\left(y_{
ji}-\bar{y}_i\right)^2\right)\right)\)</li>
      <li>When $V=\sum_{i=1}6n\sum_{j=1}^2(Y_{ji}-\bar{Y}_i)^2$ and
$W=(\bar{Y}_1,\ldots,\bar{Y}_n)$, $V$ carries no information about the
nuisance parameters $(\mu_1,\ldots,\mu_n)$.</li>
      <li>Further, the following orthogonal transformations
$V_i=(Y_{1i}-Y_{2i})/\sqrt{2}$ and $W_i=(Y_{1i}+Y_{2i})/\sqrt{2}$ imply
$V_i\sim \operatorname{Normal}(0,\sigma^2)$ and $W_i\sim
\operatorname{Normal}(\mu_i,\sigma^2)$. Also note that $V_i \perp W_i$, and
thus $V=\sum_{i=1}^n V_i^2$ and $W$ are independent.</li>
      <li>Using the preceding, you can see that the marginal likelihoods of $V$ and
$W$ are $\mathcal{L}_V(\sigma^2;v)$, depending only on $\sigma^2$, and
$\mathcal{L}_W(\mu_1,\ldots,\mu_n,\sigma^2;w)$, or the conditional likelihood
of $W$ given $V$ because $V$ and $W$ are independent. This means that the
MLE based on $\mathcal{L}_V(\sigma^2;v)$ yields a consistent estimate of
$\hat{\sigma}^2=n^{-1}\sum_{i=1}^n\sum_{j=1}^2(Y_{ji}-\bar{Y}_i)^2$ of
$\sigma^2$. This is in contrast to the inconsistent MLE based on the full
likelihood
$\hat{\sigma}^2=(2n)^{-1}\sum_{i=1}^n\sum_{j=1}^2(Y_{ji}-\bar{Y}_i)^2$.
Curiously, the inconsistency is “magically” resolved when the marginal
likelihood of $V$ is used.</li>
    </ul>
  </li>
</ul>

\[\begin{aligned}
\mathcal{L}_V\left(\sigma^2 ; v\right) &amp;=\exp \left(-\frac{1}{2
\sigma^2}\left(\sum_{i=1}^n \sum_{j=1}^2\left(y_{i
j}-\bar{y}_i\right)^2\right)\right), \\ \mathcal{L}_W\left(\mu_1, \cdots, \mu_n, \sigma^2
; w\right) &amp;=\exp \left(-\frac{1}{2 \sigma^2}\left(2
\sum_{i=1}^n\left(\bar{y}_i-\mu_i\right)^2\right)\right) \\ \mathcal{L}((\theta, \phi) ;
\mathbf{y}) &amp;\propto \mathcal{L}_V\left(\sigma^2 ; v\right)
\mathcal{L}_W\left(\mu_1, \cdots, \mu_n,
\sigma^2 ; w\right)
\end{aligned}\]

<ul>
  <li>There are several inferential techniques, including MLE, Bayesian posterior,
and the decision-theoretical approach.</li>
</ul>

<h4 id="minimum-mean-squares">
<a class="anchor" href="#minimum-mean-squares" aria-hidden="true"><span class="octicon octicon-link"></span></a>Minimum Mean Squares</h4>

<ul>
  <li>This technique minimizes the mean squared error (MSE)
$r(T_n,\theta)=\mathbb{E}_\theta\left[T_n-\theta\right]$ where
$\mathbb{E}_\theta$ is the expectation under $\Pr_\theta$ and
$T_n=T_n(\mathbf{Y})$.
    <ul>
      <li>For any given $\theta$, you may be able to obtain $T_n$ by solving the
minimization, i.e. $T_n$ is a vector of weights $\boldsymbol\theta$.</li>
      <li>$T_n=T_n(\theta)$ may depend on an underlying class $T_n$ and more generally
$\theta$. <label for="s-cls-tn" class="margin-toggle sidenote-number"></label><input type="checkbox" id="s-cls-tn" class="margin-toggle"><span class="sidenote">What does this mean? </span> Fortunately,
however, $\frac{1}{n}\sum_{i=1}^n Y_i$ is the “optimal estimator” for all
$\theta$ in the sense of minimizing the MSE within the class of unbiased
estimates.</li>
    </ul>
  </li>
  <li>When you cannot minimize $r(T_n,\theta)$ independently of $\theta$, one
partial solution is to exclude at least the $T_n$s worse than another.</li>
</ul>

<h4 id="maximum-likelihood">
<a class="anchor" href="#maximum-likelihood" aria-hidden="true"><span class="octicon octicon-link"></span></a>Maximum Likelihood</h4>

<ul>
  <li>The maximum likelihood estimator (MLE) seeks $T_n$ to maximize
$\mathcal{L}_n(\theta)$ with respect to $\theta$. Mathematically,
$T_n=\hat{\theta}=\arg\max_\theta \mathcal{L}_n(\theta;\mathbf{y})$.</li>
  <li>
<strong>Lemma 1.1: Optimality</strong>: In Example 1.1, $S\sim
\operatorname{Binomial}(n,\theta)$ with
$\mathbb{E}_{y\mid\theta}\left[\frac{S}{n}\right]=\theta$. For any $\theta$
and an unbiased estimator $T_n=\hat{\theta}$ with $\mathbb{E}_{y\mid
\theta}\left[T_n\right]=\theta$, $\frac{S}{n}$ is optimal in that
$r(T_n,\theta)=\mathbb{E}_{y\mid\theta}\left[T_n-\theta\right]\ge
\mathbb{E}_{y\mid\theta}\left[\frac{S}{n}-\theta\right]^2=r\left(\frac{S}{n},\theta\right)$.
The following is a proof:</li>
</ul>

\[\begin{aligned}
\mathbb{E}_{y\mid\theta}\left[T_n-\theta\right]
&amp;=\mathbb{E}_{y\mid\theta}\left[\left(T_n-\frac{S}{n}\right)+\left(\frac{S}{n}-\theta\right)\right]^2
\\
&amp;=\mathbb{E}_{y\mid\theta}\left[T_n-\frac{S}{n}\right]^2+\mathbb{E}_{y\mid\theta}\left[\frac{S}{n}-\theta\right]^2+2
\mathbb{E}_{y\mid\theta}\left[\left(T_n-\frac{S}{n}\right)\left(\frac{S}{n}-\theta\right)\right]
\end{aligned}\]

<ul>
  <li>Now let $W_n=T_n-\frac{S}{n}$, which makes the last term above become $2
\mathbb{E}_theta\left[W_n S\right]$, i.e. plug $S=n(T_n-W_n)$ in to
$\frac{S}{n}-\theta$ yields TODO</li>
</ul>

<h1 id="lectures">
<a class="anchor" href="#lectures" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lectures</h1>

<h2 id="lecture-1">
<a class="anchor" href="#lecture-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lecture 1</h2>

<h2 id="lecture-2">
<a class="anchor" href="#lecture-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lecture 2</h2>

</article>
    <span class="print-footer"
  >STATS 270: Bayesian Statistics - Daniel Jenson
</span>
 <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="daniel.a.jenson@gmail.com"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="https://www.linkedin.com/in/daniel-jenson-7a002a30/"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="https://github.com/danjenson"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2022 &nbsp;&nbsp;DANIEL JENSON</span></br> <br>
<span>This site created with the <a href="//github.com/danjenson/et">Edward Tufte theme for Daniel Jenson </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>

  </body>
</html>
