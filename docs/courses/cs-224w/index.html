<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    CS 224W: Machine Learning with Graphs
  </title>
  <meta
    name="description"
    content="A collection of work and research by Daniel jenson."
  />

  <!-- Google Fonts -->
  <link
    href="//fonts.googleapis.com/css?family=Lato:400,400italic"
    rel="stylesheet"
    type="text/css"
  />

  <!-- https://docs.mathjax.org/en/latest/input/tex/extensions/index.html -->
  <script>
    MathJax = {
      loader: {
        load: [
          "[tex]/ams",
          "[tex]/gensymb",
          "[tex]/mathtools",
          "[tex]/physics",
        ],
      },
      tex: {
        packages: { "[+]": ["ams", "gensymb", "mathtools", "physics"] },
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
      },
      svg: {
        fontCache: "global",
      },
    };
  </script>
  <!-- https://www.mathjax.org/#gettingstarted -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
  <link rel="stylesheet" type="text/css" href="/notes/assets/css/main.css">

  <link
    rel="canonical"
    href="https://danjenson.github.io/notes/courses/cs-224w/"
  />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
  <nav class="group">
     
    <a href="/notes/books">books</a>
      
    <a href="/notes/papers">papers</a>
      
    <a href="/notes/courses">courses</a>
      
    <a href="/notes/mindmaps">mindmaps</a>
     
    <button
      type="button"
      id="theme-toggle"
      onclick="modeSwitcher()"
      style="cursor: pointer"
    ></button>
  </nav>
  <script src="/notes/assets/js/theme-toggle.js"></script>
</header>

    <article class="group">
<h1 class="title">CS 224W: Machine Learning with Graphs</h1>
  <h1 id="lecture-1-introduction">Lecture 1: Introduction</h1>

<ul>
  <li>How do we take advantage of relational structure for better prediction?</li>
  <li>Modern deep learning is predicated on simple sequences and grids</li>
  <li>Map nodes to d-dimensional embeddings such that similar nodes in the network
are embedded close together</li>
  <li>Common tasks:
    <ul>
      <li>node classification</li>
      <li>link prediction</li>
      <li>graph classification</li>
      <li>clustering</li>
      <li>graph generation</li>
      <li>graph evolution</li>
    </ul>
  </li>
  <li><strong>strongly connected</strong>: a path from every node to ever other ndoe</li>
  <li><strong>weakly connected</strong>: connected if we disregard edge directions</li>
</ul>

<h1 id="lecture-2-feature-engineering-for-ml-in-graphs">Lecture 2: Feature Engineering for ML in Graphs</h1>

<ul>
  <li>Traditional features for ML in graphs with focus on undirected graphs</li>
  <li>Node level features:
    <ul>
      <li><strong>Goal</strong>: characterize the structure and position of a node in the network</li>
      <li><strong>importance-based features</strong>:
        <ul>
          <li>node degree: counts neighboring nodes without capturing their importance</li>
          <li>node centrality: takes node importance in a graph into account
            <ul>
              <li>eigenvector centrality: a node is important if it is us surrounded by
important neighboring nodes; the largest eigenvalue is always positive and
unique</li>
              <li>betweenness centrality: a node is important if it lies on many shortest
paths between other nodes</li>
              <li>closeness centrality: a node is important if it has small shortest path
lengths to all other nodes</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>structure-based features</strong>:
        <ul>
          <li>node degree</li>
          <li>clustering coefficient: measures how connected neighboring nodes are
(triangles)</li>
          <li>graphlets: extends clustering coefficient to graph shapes beyond triangles;
creates a graphlet degree vector (GDV); <strong>graphlets</strong> are rooted,
connected, induced, non-isomorphic subgraphs</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>induced subgraph</strong>: another graph formed from a subset of vertices and all
the edges connecting the vertices in the subset</li>
  <li><strong>graph isomorphism</strong>: two graphs which contain the same number of nodes
connected in the same way are said to be isomorphic</li>
</ul>

<h2 id="link-prediction">Link prediction</h2>

<ul>
  <li>For each pair of nodes, predict the score c(x,y) and sort
by decreasing score, predict the top n pairs as new links, validate with true
edges</li>
  <li>Tasks:
    <ul>
      <li>Links missing at random: remove a random set of links and then aim to
predict them</li>
      <li>Links over time: predict links that will manifest at the next time step</li>
    </ul>
  </li>
  <li>Methods:
    <ul>
      <li>distance-based features
        <ul>
          <li>shortest-path distance between two nodes</li>
        </ul>
      </li>
      <li>local neighborhood overlap
        <ul>
          <li>common neighbors</li>
          <li>Jaccard’s coefficient</li>
          <li>Adamic-Adar index</li>
        </ul>
      </li>
      <li>global neighborhood overlap
        <ul>
          <li>Katz index: count the number of walks of all lengths between a given pair
of nodes; based on powers of adjacency matrix; if you used a discount
factor, there is a closed form solution</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="graph-level-features">Graph level features</h2>

<ul>
  <li><strong>Goal</strong>: We want features that characterize the structure of an entire graph.</li>
  <li>Kernel methods are widely-used for traditional ML for graph-level prediction
    <ul>
      <li>Design kernels instead of feature vectors?</li>
    </ul>
  </li>
  <li>Kernel K(G, G’) in R measures the similarity between graphs
    <ul>
      <li><strong>Goal</strong>: design a graph feature vector $\phi(G)$</li>
      <li>Kernel matrix <strong>K</strong>=(K(G, G’)) must always be positive semi-definite</li>
      <li>There exists a feature representation such that
$K(G,G’)=\phi(G)^\intercal\phi(G’)$</li>
      <li>Once the kernel is defined, off-the-shelf ML models such as kernel SVM can
be used to make predictions</li>
      <li>Examples:
        <ul>
          <li>Graphlet kernel</li>
          <li>Weisfeiler-Lehman kernel</li>
          <li>Random-walk kernel</li>
          <li>Shortest-path graph kernel</li>
        </ul>
      </li>
      <li>Key idea: <strong>bag-of-words</strong> for a graph
        <ul>
          <li>BoW simply uses the word counts as feature for documents, without regard
for order</li>
          <li>A naive extension to graph: regard nodes as words</li>
          <li>Bag of…
            <ul>
              <li>node colors (features)</li>
              <li>node degrees</li>
              <li>graphlet counts (<strong>graphlet-kernel</strong>); if a graph’s node degree is
bounded by d, then there exists an $O(nd^{k-1})$ algorithm to count all
graphlets of size k</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Subgraph isomorphism is NP-hard</li>
  <li><strong>Goal</strong>: can we design an efficient graph feature descriptor $\phi(G)$?
    <ul>
      <li>Can we generalize bag-of-node-degrees? Yes, this is called <strong>color
refinement</strong></li>
      <li>Color refinement summarizes the structure of the K-hop neighborhood (uses a
hash function for message passing aggregation)</li>
      <li>After color refinement, Weisfeiler-Lehman (WL) kernel counts number of nodes with
a given color.</li>
      <li>WL kernel is the inner product of the color refinement count vectors
        <ul>
          <li>$O(\lvert E\rvert)$ running time</li>
          <li>$O(\lvert V\rvert)$ number of colors in memory</li>
          <li>counting colors takes linear time with respect to $\lvert V\rvert$</li>
          <li>total runtime is linear in $\lvert E\rvert$</li>
          <li>far more computationally efficient than graphlet kernel</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="lecture-3-node-embeddings">Lecture 3: Node Embeddings</h1>

<ul>
  <li>Graph representation learning eliminates the need to do feature engineering</li>
  <li>Want to encode nodes so they have certain properties (proximities) in the
embedding space</li>
  <li>You train these embeddings with some measure of similarity, and a simple
decoder would simply be the dot product of these embeddings, i.e. cosine
similarity in the embedding space</li>
  <li><strong>Shallow embeddings</strong> just lookup a node ID in a matrix and return the
corresponding embedding, i.e. the encoder is just an embedding lookup
    <ul>
      <li>DeepWalk and node2vec are instances of this</li>
    </ul>
  </li>
  <li>The following generates <strong>task independent</strong> embeddings that represent only
their network structure; in particular, they are not using node labels for
features</li>
</ul>

<h2 id="how-do-you-define-similarity">How do you define similarity?</h2>

<ul>
  <li>Random walks: initiate random walks from all nodes in the graph and then
$\mathbf{z}_u^\intercal \mathbf{z}_v$ represents the likelihood that nodes u
and v co-occur on a random walk over the graph
    <ul>
      <li>Steps:
        <ol>
          <li>Estimate the probability of visiting node v on a random walk starting from
node u using some random walk strategy</li>
          <li>Optimize embeddings to encode these random walk statistics</li>
        </ol>
      </li>
      <li>Benefits:
        <ul>
          <li>Expressive: incorporates both local and higher order neighborhood information</li>
          <li>Efficient: do not need to consider all node pairs when training, only node
pairs that have co-occurred on random walks</li>
        </ul>
      </li>
      <li>Objective: \(\mathcal{L}=\sum_{u\in V}\sum_{v\in N_R(u)}-\log\left(\frac{\exp(\mathbf{z}_u^\intercal\mathbf{z}_v)}{\sum_{n\in V}\exp(\mathbf{z}_u^\intercal\mathbf{z}_n)}\right)\)
        <ul>
          <li>$\max_f\sum_{u\in V}\log\Pr(N_R(u)\mid \mathbf{z}_u)$</li>
          <li>$\mathcal{L}=\sum_{u\in V}\sum_{v\in N_R(u)}-\log\Pr(v\mid
\mathbf{z}_u)$</li>
          <li>
\[\Pr(v\mid \mathbf{z}+u)=\frac{\exp(\mathbf{z}_u^\intercal\mathbf{z}_v)}{\sum_{n\in
V}\exp(\mathbf{z}_u^\intercal\mathbf{z}_n)}\]
          </li>
          <li>The denominator is expensive, so it can be approximated with noise
contrastive estimation (NCE), i.e. instead of normalizing with respect to
all nodes, just normalize against $k$ random negative samples</li>
          <li>
\[\approx \log \left(\sigma\left(\mathbf{z}_u^{\mathrm{T}} \mathbf{z}_v\right)\right)-\sum_{i=1}^k \log \left(\sigma\left(\mathbf{z}_u^{\mathrm{T}} \mathbf{z}_{n_i}\right)\right), n_i \sim P_V\]
            <ul>
              <li>k is often chosen to be 5-20 and technically nodes on the random walk
shouldn’t be chosen</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Update with (stochastic) gradient descent: $\mathbf{z}_u\leftarrow
\mathbf{z}_u-\eta\pdv{L}{\mathbf{z}_u}$</li>
    </ul>
  </li>
  <li>DeepWalk: run fixed-length, unbiased random walks starting from each node
    <ul>
      <li>Problem: this notion of similarity is too constrained</li>
    </ul>
  </li>
  <li>Node2Vec:
    <ul>
      <li>Goal: Embed nodes with similar network neighborhoods close in the feature
space. Frame this as a maximum likelihood optimization problem.</li>
      <li>Key observation: flexible notion of network neighborhood $N_R(u)$ of node u
leads to rich node embeddings</li>
      <li>Develop biased 2nd order random walk R to generate network neighborhood
$N_R(u)$ of node u</li>
      <li>Idea: use flexible, biased random walks that can trade off between local
and global views of the network, i.e. <strong>use a mix of DFS (macro-view) and BFS (micro-view)</strong>
        <ul>
          <li>Instrument this with parameters p and q; p determines probability of
returning to previous node and q is the ratio of BFS to DFS, i.e. moving
outwards vs. inwards</li>
          <li>This requires remembering where the walk came from (just the last step)</li>
          <li>Lecture 3, slide 45 has an example of how p and q are used</li>
        </ul>
      </li>
      <li>Steps:
        <ol>
          <li>Compute random walk probabilities</li>
          <li>Simulate $r$ random walks of length $l$ starting from each node $u$</li>
          <li>Optimize the node2vec objective using SGD</li>
        </ol>
      </li>
      <li>Benefits:
        <ul>
          <li>Linear-time complexity</li>
          <li>All 3 steps are individually parallelizable</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Other random walks (links to papers on Lecture 3, slide 47):
    <ul>
      <li>based on node attributes</li>
      <li>based on learned weights</li>
      <li>based on 1-hop and 2-hop random walk probabilities</li>
      <li>random walks on modified version of original network, i.e. struct2vec, HARP</li>
    </ul>
  </li>
</ul>

<h2 id="embedding-entire-graphs">Embedding Entire Graphs</h2>

<ul>
  <li>Examples:
    <ul>
      <li>Classifying toxic vs. non-toxic molecules</li>
      <li>Identifying anomalous graphs</li>
    </ul>
  </li>
  <li>Approaches:
    <ul>
      <li>Run graph embedding technique and sum or average embeddings</li>
      <li>Introduce a virtual node to represent the subgraph (linked to the nodes in
that subgraph) and run graph embedding
technique</li>
    </ul>
  </li>
</ul>

<h2 id="matrix-factorization-and-node-embeddings">Matrix Factorization and Node Embeddings</h2>

<ul>
  <li><strong>Inner product decoder with node similarity defined by edge connectivity is
equivalent to matrix factorization of adjacency matrix $\mathbf{A}$</strong></li>
  <li>Objective: extract a factorization $\mathbf{A}=\mathbf{Z}^\intercal\mathbf{Z}$
where $\mathbf{A}$ is the adjacency matrix and $\mathbf{Z}$ is the embedding
matrix
    <ul>
      <li>Generally, we can only learn $\mathbf{Z}$ approximately</li>
    </ul>
  </li>
  <li>DeepWalk is equivalent of the matrix factorization of the following
(explanation on Lecture 3, Slide 61):
\(\log \left(\operatorname{vol}(G)\left(\frac{1}{T} \sum_{r=1}^T\left(D^{-1} A\right)^r\right) D^{-1}\right)-\log b\)</li>
  <li>Node2vec can also be formulated as a more complex matrix factorization; paper
links on Lecture 3, Slide 61</li>
  <li>How do you use node embeddings?
    <ul>
      <li>Clustering/community detection</li>
      <li>Node classification</li>
      <li>Link prediction based on $f(\mathbf{z}_i,\mathbf{z}_j)$, where $f$ can be
concatenate, Hardamard, sum/average, or distance</li>
    </ul>
  </li>
</ul>

<h2 id="limitations">Limitations</h2>

<ul>
  <li><strong>Cannot obtain embeddings for nodes not in the training set</strong>, i.e. shallow
embeddings are only transductive</li>
  <li><strong>Cannot capture structural similarity</strong>: neither DeepWalk nor node2vec
capture structural similarity in node embeddings; this can be remedied by methods like
struct2vec</li>
</ul>

<h1 id="lecture-4-graph-neural-networks">Lecture 4: Graph Neural Networks</h1>

<ul>
  <li>Limitations of shallow embeddings:
    <ul>
      <li>$O(\lvert V\rvert d)$ parameters are required</li>
      <li>No parameters are shared between nodes</li>
      <li>Every node has its own unique embedding</li>
      <li>Inherently transductive, i.e. can’t generate embeddings for nodes that are
not seen during training</li>
      <li>Does not incorporate node features</li>
    </ul>
  </li>
  <li>GNNs are node encoders based on multiple layers of non-linear transformations
based on graph structure; these deep encoders can be combined with node
similarity functions</li>
  <li>GNNs can embed nodes, graphs, and subgraphs</li>
  <li>GNN tasks:
    <ul>
      <li>node classification</li>
      <li>link prediction</li>
      <li>community detection</li>
      <li>network similarity</li>
    </ul>
  </li>
  <li>Machine learning can be formulated as an optimization problem:
\(\min_\theta\mathcal{L}(\mathbf{y},f(\mathbf{x}))\) where $\theta$ could be
our shallow embeddings $\mathbf{Z}$ and the loss could be L2 loss:
$\mathcal{L}(\mathbf{y},f(\mathbf{x}))=\lVert y-f(x)\rVert_2$
    <ul>
      <li>other loss functions include L1, Huber loss, max-margin (hinge) loss, cross
entropy, etc.</li>
      <li>$f$ could be a linear layer, MLP, or other NN like a GNN</li>
    </ul>
  </li>
  <li>When there are no node features, you can do a one hot encoding of a nodes</li>
  <li>Because graphs have no spatial or temporal assignment by default, we should
constrain our efforts to methods that are permutation invariant, this means
that two “order plans” should be the same for the same graphs with differently
labeled nodes/edges</li>
  <li>If $f(\mathbf{A}_i, \mathbf{X}_i)=f(\mathbf{A}_j, \mathbf{X}_j)$ for any order
plan $i$ and $j$, we say that $f$ is a permutation invariant function</li>
  <li><strong>Definition</strong>: For any graph function $f:\mathbb{R}^{\lvert V\rvert\times
m}\times\mathbb{R}^{\lvert V\rvert \times\lvert V\rvert}\to\mathbb{R}^d$, $f$
is <strong>permutation-invariant</strong> if $f(\mathbf{A}, \mathbf{X})=f(\mathbf{P}\mathbf{A}\mathbf{P}^\intercal, \mathbf{P}\mathbf{X})$ for any
permutation $\mathbf{P}$, i.e. the value is the same regardless of whether you
permute the adjacency matrix and features.</li>
  <li>For node representation, we learn a function that maps ndoes of $G$ to a
matrix $\mathbb{R}^{m\times d}$.</li>
  <li>If we learn a function $f$ that maps a graph $G=(\mathbf{A},\mathbf{X})$ to a
matrix $\mathbb{R}^{m\times d}$ and the output vector of a node at the same
position in the graph remains unchanged for any order plan, then $f$ is
<strong>permutation equivariant</strong></li>
  <li><strong>Definition</strong>: for any node function $f:\mathbb{R}^{\lvert V\rvert\times
m}\times \mathbb{R}^{\lvert V\rvert\times\lvert V\rvert}\to\mathbb{R}^{\lvert
V\rvert\times m}$, $f$ is <strong>permutation-equivariant</strong> if
$\mathbf{P}f(\mathbf{A},\mathbf{X})=f(\mathbf{PAP}^\intercal, \mathbf{PX})$
for any permutation $\mathbf{P}$, i.e. when you shuffle the input the output
is shuffled in the same fashion.</li>
  <li><strong>Examples</strong>:
    <ul>
      <li>$f(\mathbf{A}, \mathbf{X})=\mathbf{1}^\intercal \mathbf{X}$ is
permutation-invariant
        <ul>
          <li>$f(\mathbf{PAP}^\intercal, \mathbf{PX})=\mathbf{1}^\intercal
\mathbf{PX}=\mathbf{1}^\intercal \mathbf{X}=f(\mathbf{A}, \mathbf{X})$</li>
        </ul>
      </li>
      <li>$f(\mathbf{A}, \mathbf{X})=\mathbf{X}$ is permutation-equivariant
        <ul>
          <li>$f(\mathbf{PAP}^\intercal, \mathbf{PX})=\mathbf{PX}=\mathbf{P}f(\mathbf{A}, \mathbf{X})$</li>
        </ul>
      </li>
      <li>$f(\mathbf{A}, \mathbf{X})=\mathbf{AX}$ is permutation-equivariant
        <ul>
          <li>$f(\mathbf{PAP}^\intercal, \mathbf{PX})=\mathbf{PAP}^\intercal\mathbf{PX}=\mathbf{PAX}=\mathbf{P}f(\mathbf{A}, \mathbf{X})$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>GNNs consist of multiple permutation equivariant and invariant functions,
unlike most deep ML, i.e. MLPs</li>
  <li><strong>Idea</strong>: a node’s neighborhood defines a computation graph and the goal is to
learn how to propagate information across the graph to compute node features</li>
  <li><strong>Key idea</strong>: generate node embeddings based on local network neighborhoods,
each network neighborhood defines a computation graph (imagine trees rooted at
nodes, where the children are the neighbors of nodes)</li>
  <li>Basic approach: average neighbor messages and apply a NN</li>
  <li>Given <strong>a node</strong>, the GCN that computes its embedding is <strong>permutation
invariant</strong></li>
  <li>Considering <strong>all nodes</strong>, the GCN computation is permutation equivariant</li>
  <li>$\mathbf{h}_v^{(0)}=\mathbf{x}_v$</li>
  <li>
\[\mathbf{h}_v^{(k+1)}=\sigma\left(\mathbf{W}_k \sum_{u \in \mathbf{N}(v)} \frac{\mathbf{h}_u^{(k)}}{|\mathbf{N}(v)|}+\mathbf{B}_k \mathbf{~h}_v^{(k)}\right), \forall k \in\{0 . . K-1\}\]
    <ul>
      <li>Train $\mathbf{W}_k$ and $\mathbf{B}_k$ using SGD</li>
    </ul>
  </li>
  <li>$\mathbf{z}_v=\mathbf{h}_v^{(K)}$</li>
  <li>The entire update in matrix form: \(H^{(k+1)}=\sigma\left(\tilde{A} H^{(k)} W_k^{\mathrm{T}}+H^{(k)} B_k^{\mathrm{T}}\right)\) where $\tilde{\mathbf{A}}=\mathbf{D}^{-1}\mathbf{A}$.
    <ul>
      <li>In practice, this implies that efficient sparse matrix multiplication can be
used $\tilde{\mathbf{A}}$ is sparse.</li>
      <li><strong>Not all GNNs can be expressed in matrix form when the aggregation function
is complex.</strong></li>
    </ul>
  </li>
</ul>

<h2 id="unsupervised-training">Unsupervised training</h2>

<ul>
  <li>When you don’t have labels, you can use the graph structure as supervision</li>
  <li>If you say that “similar” nodes should have similar embeddings, then</li>
  <li>
\[\mathcal{L}=\sum_{z_u, z_v} \operatorname{CE}\left(y_{u, v}, \operatorname{DEC}\left(z_u, z_v\right)\right)\]
    <ul>
      <li>$y_{u,v}=1$ when node $u$ and $v$ are similar</li>
      <li>$\operatorname{CE}$ is the cross entropy loss</li>
      <li>$\operatorname{DEC}$ is a decoder, such as inner product</li>
      <li>node similarity can be based on:
        <ul>
          <li>Random walks (node2vec, DeepWalk, struct2vec)</li>
          <li>Matrix factorization</li>
          <li>Node proximity in the graph</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="supervised-training">Supervised Training</h2>

<ul>
  <li>Directly train for a supervised task like node classification, e.g. is a drug
safe or toxic?</li>
  <li>
\[\mathcal{L}=-\sum_{v \in V} y_v \log \left(\sigma\left(\mathrm{z}_v^{\mathrm{T}} \theta\right)\right)+\left(1-y_v\right) \log \left(1-\sigma\left(\mathrm{z}_v^{\mathrm{T}} \theta\right)\right)\]
    <ul>
      <li>$y_v$ are labels</li>
      <li>$\theta$ are classification weights</li>
      <li>$\mathbf{z}_v$ is a node embedding</li>
    </ul>
  </li>
</ul>

<h2 id="model-design">Model Design</h2>

<ol>
  <li>Define a neighborhood aggregation function.</li>
  <li>Define a loss function on the embeddings.</li>
  <li>Train on a set of nodes, i.e. a batch of compute graphs.</li>
  <li>Generate embeddings for nodes as needed (even those we never trained on!)</li>
</ol>

<h2 id="inductive-capability">Inductive Capability</h2>

<ul>
  <li>The model is capable of induction when the same aggregation parameters are
shared for all nodes (GraphSAGE), an added benefit is that the number of
parameters is sublinear in $\lvert V\rvert$ and we can generalize to unseen
nodes.</li>
  <li>A example is when you train on a protein interaction graph from model organism
A and generate embeddings on a newly collected data about organism B</li>
</ul>

<h2 id="gnns-vs-cnns">GNNs vs CNNs</h2>

<ul>
  <li>The key difference is that we can learn an different weight function for each
pixel surrounding the target node</li>
  <li>GNN formulation: \(\mathbf{h}_v^{(l+1)}=\sigma\left(\underbrace{\mathbf{W}_l}_{\text{node agnostic}} \sum_{u \in \mathbf{N}(v)} \frac{\mathbf{h}_u^{(l)}}{\lvert\mathbf{N}(v)\rvert}+\mathbf{B}_l \mathbf{h}_v^{(l)}\right), \forall l \in\{0, \ldots, L-1\}\)</li>
  <li>CNN formulation: \(\mathbf{h}_v^{(l+1)}=\sigma\left(\sum_{u \in \mathbf{N}(v)} \underbrace{\mathbf{W}_l^u}_{\text{pixel specific}} \mathbf{~h}_u^{(l)}+\mathbf{B}_l \mathbf{~h}_v^{(l)}\right), \forall l \in\{0, \ldots, L-1\}\)</li>
  <li>A CNN can be seen as a special GNN with fixed neighbor size and ordering
    <ul>
      <li>The size of the filter is pre-defined for a CNN</li>
      <li>The advantage of GNN is it postprocesses arbitrary graphs with different
degrees for each node</li>
    </ul>
  </li>
  <li>CNN is not permutation invariant/equivariant, i.e. switching the order of
pixels will lead to different outputs</li>
</ul>

<h2 id="summary">Summary:</h2>

<ul>
  <li>Use multiple layers for embedding nodes, propagating the previous hidden state
to the next layer</li>
  <li>Mean aggregation for a GCN can be expressed in matrix form</li>
  <li>GNN is a general architecture of which CNN is a special case</li>
</ul>

<h1 id="lecture-5-a-general-perspective-on-gnns">Lecture 5: A General Perspective on GNNs</h1>

<h2 id="general-gnn-framework">General GNN Framework</h2>

<ol>
  <li>Message</li>
  <li>Aggregation</li>
  <li>Layer connectivity</li>
  <li>Graph augmentation</li>
  <li>Learning objective</li>
</ol>

<ul>
  <li>GNN Layer: compresses a set of vectors into a single vector
    <ul>
      <li>Message: $\mathbf{m}_u^{(l)}=\mathrm{MSG}^{(l)}\left(\mathbf{h}_u^{(l-1)}\right), u \in{N(v) \cup v}$, where the message could be a simple linear layer: $\mathbf{m}_u^{(l)}=\mathbf{W}^{(l)} \mathbf{h}_u^{(l-1)}$</li>
      <li>Aggregation: \(\mathbf{h}_v^{(l)}=\operatorname{AGG}^{(l)}\left(\left\{\mathbf{m}_u^{(l)}, u \in N(v)\right\}\right)\), where the aggregation function could be any permutation invariant operator like sum, mean, min, or max
        <ul>
          <li>Note you must first aggregate neighbors then aggregate that representation
with the original node representation</li>
        </ul>
      </li>
      <li>One issue is that information about the source (locally rooted) node could
get lost if it doesn’t depend on its own embedding, so you should include it
when computing the message
        <ul>
          <li>Usually different weights will be applied to the neighbors’ messages and
the source weights message/state</li>
          <li>You can combine these using either concatenation or summation: \(\mathbf{h}_v^{(l)}=\operatorname{AGG}\left(\operatorname{AGG}\left(\left\{\mathbf{m}_u^{(l)}, u \in N(v)\right\}\right), \mathbf{m}_v^{(l)}\right)\)</li>
        </ul>
      </li>
      <li>Add non-linearity (activation) expressiveness, i.e. sigmoid, ReLU, or
Sigmoid</li>
    </ul>
  </li>
  <li><strong>Graph Convolutional Networks (GCN)</strong>: \(\mathbf{h}_v^{(l)}=\sigma\left(\underbrace{\sum_{u \in N(v)}}_\text{aggregation} \underbrace{\mathbf{W}^{(l)} \frac{\mathbf{h}_u^{(l-1)}}{\lvert N(v)\rvert}}_\text{message}\right)\)
    <ul>
      <li>Note this is normalized by node degree</li>
      <li>GCN is also assumed to have self-edges</li>
    </ul>
  </li>
  <li><strong>GraphSAGE</strong>: \(\mathbf{h}_v^{(l)}=\sigma\left(\mathbf{W}^{(l)} \cdot \operatorname{CONCAT}\left(\mathbf{h}_v^{(l-1)}, \mathrm{AGG}\left(\left\{\mathbf{h}_u^{(l-1)}, \forall u \in N(v)\right\}\right)\right)\right)\)
    <ul>
      <li>Note that neighbors are typically sampled</li>
      <li>3 types of neighborhood aggregation:
        <ul>
          <li><strong>Mean</strong>: $\operatorname{AGG}=\sum_{u\in \lvert
N(v)\rvert}\frac{\mathbf{h}_u^{l-1}}{\lvert N(v)\rvert}$</li>
          <li><strong>Pool</strong>: \(\operatorname{AGG}=\operatorname{Mean/Max}\left(\left\{\operatorname{MLP}\left(\mathbf{h}_u^{(l-1)}\right), \forall u \in N(v)\right\}\right)\)</li>
          <li><strong>LSTM</strong>: \(\mathrm{AGG}=\operatorname{LSTM}\left(\left[\mathbf{h}_u^{(l-1)}, \forall u \in \pi(N(v))\right]\right)\)</li>
        </ul>
      </li>
      <li>L2 normalization is applied at every layer:\(\mathbf{h}_v^{(l)} \leftarrow \frac{\mathbf{h}_v^{(l)}}{\left\|\mathbf{h}_v^{(l)}\right\|_2} \forall v \in V \text { where }\|u\|_2=\sqrt{\sum_i u_i^2}\)
        <ul>
          <li>Without this, the embedding vectors would have different scales</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Graph Attention Networks</strong>: \(\mathbf{h}_v^{(l)}=\sigma\left(\sum_{u \in N(v)} \underbrace{\alpha_{v u}}_\text{attention weights} \mathbf{W}^{(l)} \mathbf{h}_u^{(l-1)}\right)\)
    <ul>
      <li>In GCN/GraphSage, $\alpha_{v u}=\frac{1}{\lvert N(v)\rvert}$ is the
weighting factor of node $u$’s message to node $v$, which implies that all
neighbors are equally important</li>
      <li>The idea with attention is that only a small portion of input maters and the
rest should not affect the decision/calculation</li>
      <li><strong>Goal</strong>: specify arbitrary importance to different neighbors of each node
in the graph</li>
      <li><strong>Idea</strong>: Compute the embedding $\mathbf{h}_v^{(l)}$ of each node in the
graph following an attention strategy
        <ol>
          <li>Let $a_{vu}$ be computed as a byproduct of the attention mechanism:
            <ul>
              <li>$e_{vu}$ indicates the importance of $u$’s message to $v$: \(e_{v u}=a\left(\mathbf{W}^{(l)} \mathbf{h}_u^{(l-1)}, \mathbf{W}^{(l)} \boldsymbol{h}_v^{(l-1)}\right)\)</li>
              <li>
\[e_{A B}=a\left(\mathbf{W}^{(l)} \mathbf{h}_A^{(l-1)}, \mathbf{W}^{(l)} \mathbf{h}_B^{(l-1)}\right)\]
              </li>
            </ul>
          </li>
          <li>Normalize $e_{uv}$ into the final attention weights $\mathbf{\alpha}<em>{vu}$
using the softmax: $\alpha</em>{uv}=\frac{\exp\left(e_{vu}\right)}{\sum_{k\in N(v)}\exp\left(e_{vk}\right)}$</li>
          <li>Calculated the weighted sum of neighbors: \(\mathbf{h}_v^{(l)}=\sigma\left(\sum_{u \in N(v)} \alpha_{v u} \mathbf{W}^{(l)} \mathbf{h}_u^{(l-1)}\right)\)</li>
        </ol>
      </li>
      <li>This approach is agnostic to the form of attention mechanism, $a$ - Could use a simple single-layer neural network, i.e. concatenate hidden
state for target and neighbor node, run it through a linear layer to produce
scalar $e$: \(\begin{aligned}
&amp; e\_{A B}=a\left(\mathbf{W}^{(l)} \mathbf{h}\_A^{(l-1)}, \mathbf{W}^{(l)} \mathbf{h}\_B^{(l-1)}\right) \\
&amp; =\operatorname{Linear}\left(\operatorname{Concat}\left(\mathbf{W}^{(l)} \mathbf{h}\_A^{(l-1)}, \mathbf{W}^{(l)} \mathbf{h}\_B^{(l-1)}\right)\right)
\end{aligned}\)
        <ul>
          <li><strong>Multi-head attention</strong> stabilizes the learning process of the attention
mechanism
            <ul>
              <li>Create multiple attention scores (each replica with different parameters):
\(\begin{aligned}
&amp; \mathbf{h}_v^{(l)}[1]=\sigma\left(\sum_{u \in N(v)} \alpha_{v u}^1 \mathbf{W}^{(l)} \mathbf{h}_u^{(l-1)}\right) \\
&amp; \mathbf{h}_v^{(l)}[2]=\sigma\left(\sum_{u \in N(v)} \alpha_{v u}^2 \mathbf{W}^{(l)} \mathbf{h}_u^{(l-1)}\right) \\
&amp; \mathbf{h}_v^{(l)}[3]=\sigma\left(\sum_{u \in N(v)} \alpha_{v u}^3 \mathbf{W}^{(l)} \mathbf{h}_u^{(l-1)}\right) \\
\end{aligned}\)</li>
              <li>Aggregate the output by concatenation or summation: $\mathbf{h}_v^{(l)}=\mathrm{AGG}\left(\mathbf{h}_v^{(l)}[1], \mathbf{h}_v^{(l)}[2], \mathbf{h}_v^{(l)}[3]\right)$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Benefits:
        <ul>
          <li>Allows for implicitly specifying different importance to different
neighbors</li>
          <li>Computationally efficient: the computation can be parallelized across all
edges, and the same with aggregation</li>
          <li>Storage efficient: sparse matrix operations do not require more than
$O(V+E)$ entries to be stored</li>
          <li><strong>Fixed</strong> number of parameters, irrespective of graph size</li>
          <li>Localized: only attends over local network neighborhoods</li>
          <li>Inductive: it is a shared edge-wise mechanism and does not depend on
global graph structure</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="gnn-layer-in-practice">GNN Layer in Practice</h2>

<ul>
  <li>IN practice the following classic GNN layers are a great starting point
    <ul>
      <li>linear</li>
      <li>batch norm: stabilizes training</li>
      <li>dropout: prevents overfitting</li>
      <li>activation: more expressive</li>
      <li>attention: control relative importances</li>
      <li>aggregation</li>
    </ul>
  </li>
  <li><strong>Batch Normalization</strong>: feature-wise normalization using mean and standard
deviation by batch</li>
  <li><strong>Dropout</strong>: regularizes network to prevent overfitting; during training,
randomly drop neurons with probability $p$, at testing time, multiply all
outputs by $p$
    <ul>
      <li><strong>In GNNs, dropout is applied to the linear layer in the message function</strong>: \(\mathbf{m}_u^{(l)}=\mathbf{W}^{(l)} \mathbf{h}_u^{(l-1)}\)</li>
    </ul>
  </li>
  <li><strong>Non-linear Acviation</strong>:
    <ul>
      <li>ReLU: $\max(x, 0)$</li>
      <li>Sigmoid: $\sigma(x)=\frac{1}{1+e^{-x}}$</li>
      <li>Parametric ReLU (PReLU): $\max(x, 0)+a\cdot\min(x, 0)$ where $a$ is a
trainable parameter
        <ul>
          <li>Performs better than ReLU</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Summary: modern deep learning modules can be included in GNN layer for better
performance</li>
</ul>

<h2 id="stacking-gnn-layers">Stacking GNN Layers</h2>

<ul>
  <li>The standard way: stack GNN layers sequentially</li>
  <li>The issue: <strong>GNNs suffer from over-smoothing problem where all the node
embeddings converge to the same value</strong></li>
  <li><strong>Receptive field</strong>: the set of nodes that determine the embedding of a node
of interest
    <ul>
      <li>In a $K$-layer GNN, each node has a receptive field of $K$-hop neighborhood</li>
      <li>The number of shared neighbors increases when you increase $K$</li>
      <li>When the receptive field of two nodes have high-overlap, they are likely to
have highly similar embeddings</li>
      <li>Many GNN layers -&gt; increase in receptive fields of nodes -&gt; embeddings
become highly similar -&gt; over-smoothing</li>
      <li>Lessons: be cautions when adding GNN layers; adding more does not always
help
        <ol>
          <li>Analyze the necessary receptive field to solve your problem</li>
          <li>Set the number of GNN layers $L$ to be a bit more than the receptive
field we like, but not much larger</li>
        </ol>
      </li>
      <li>Question: how do we enhance the expressive power of a GNN if the number of
layers is small, i.e. how to make a shallow GNN more expressive?
        <ol>
          <li>Increase the expressive power within each layer by making transformation
and aggregation a deep neural network, i.e. multi-layer MLP</li>
          <li>Add layers that do not pass messages, i.e. pre and post-processing
layers, which work very well in practice
            <ul>
              <li><strong>pre-processing layers</strong>: important when encoding node features like
text/image</li>
              <li><strong>post-processing layers</strong>: important when reasoning/transformation over
node embeddings are needed, e.g. graph classification, knowledge graphs</li>
            </ul>
          </li>
        </ol>
      </li>
      <li>Question: what if my problem still requires many GNN layers?
        <ul>
          <li>Add skip connections in GNNs</li>
          <li>Node embeddings in earlier GNN layers can sometimes better differentiate
nodes</li>
          <li>Increase the impact of earlier layers in the final node embeddings by
adding shortcuts in the GNN,i.e. $F(x) + x$ instead of just $F(x)$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Skip connections</strong>: intuitively, these create a mixture of models
    <ul>
      <li>$N$ skip connections implies $2^N$ possible paths</li>
      <li>Each path could have up to $N$ modules</li>
      <li>We automatically get a mixture of shallow and deep GNNs</li>
      <li>GCN layer with a skip connection: \(\mathbf{h}_v^{(l)}=\sigma\left(\underbrace{\sum_{u \in N(v)} \mathbf{W}^{(l)} \frac{\mathbf{h}_u^{(l-1)}}{\lvert N(v)\rvert}}_{F(x)}+\underbrace{\mathbf{h}_v^{(l-1)}}_{x}\right)\)</li>
    </ul>
  </li>
  <li>Another option is to directly skip to the last layer where the final layer aggregates from all
the node embeddings from previous layers</li>
</ul>

<h2 id="graph-manipulation-in-gnns">Graph Manipulation in GNNs</h2>

<ul>
  <li>Graph feature augmentation</li>
  <li>Graph structure manipulation</li>
  <li>Reasons for breaking the equality between the raw input graph and the
computational graph
    <ul>
      <li>Feature level:
        <ul>
          <li>Input lacks features -&gt; feature augmentation
            <ul>
              <li>Standard approaches:
                <ul>
                  <li>Assign constant values to nodes</li>
                  <li>Assign unique IDs (one hot encodings) to nodes</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Structure level:
        <ul>
          <li>Graph is too sparse -&gt; inefficient message passing
            <ul>
              <li>Add virtual nodes / edges</li>
              <li>Connect 2-hop neighbors via virtual edges
                <ul>
                  <li>Intuition: instead of just using $A$, use $A+A^2$</li>
                  <li>Works well on bipartite graphs, e.g. authors-papers</li>
                </ul>
              </li>
              <li>Connect all nodes to a virtual node, after which all nodes will be a
2-hop distance from one another (greatly improves message passing)</li>
            </ul>
          </li>
          <li>Graph is too dense -&gt; message passing is too costly
            <ul>
              <li>Sample neighbors when doing message passing</li>
              <li>Reduces computational cost and works well</li>
            </ul>
          </li>
          <li>Graph is too large -&gt; cannot fit the computational graph into a GPU
            <ul>
              <li>Sample subgraphs to compute embeddings</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>It’s unlikely that the input graph happens to be the optimal computation graph
for embeddings</li>
  <li>Constant node features:
    <ul>
      <li>Expressive power: medium. All nodes are identical but GNN can lear from
graph structure.</li>
      <li>Inductive learning: High. Simply assign constant to new nodes and run GNN.</li>
      <li>Computational cost: Low. Only a 1 dimensional feature.</li>
      <li>Use cases: any graph, inductive settings.</li>
    </ul>
  </li>
  <li>One-hot node features:
    <ul>
      <li>Expressive power: High. Each node has a unique ID, so node specific
information can be stored.</li>
      <li>Inductive learning: Low. Cannot generalize to new nodes - new nodes
introduce new IDs and GNN does’t know how to embed unseen IDs.</li>
      <li>Use cases: small graphs, transductive settings.</li>
    </ul>
  </li>
  <li>Certain structures are hard to learn by GNN
    <ul>
      <li>Example: cycle counts
        <ul>
          <li>Can a GNN learn the length of a cycle that $v_1$ resides in?
Unfortunately, no</li>
          <li>Regardless of whether $v_1$ is in a tree, square, or infinite length
(line) cycle, the computation graph (edges to two neighbors) is the same</li>
          <li><strong>Could augment nodes with cycle counts</strong></li>
          <li><strong>Could also augment with degree distribution clustering coefficient, PageRank, Centrality, etc</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="lecture-6-gnn-augmentation-and-training">Lecture 6: GNN Augmentation and Training</h1>

<h2 id="gnn-prediction">GNN Prediction</h2>

<ul>
  <li>Add a prediction head to node embeddings output
    <ul>
      <li>Different tasks require different prediction heads</li>
    </ul>
  </li>
  <li>Suppose we wan to make a $k$-way prediction
    <ul>
      <li>Classification: classify among $k$ categories</li>
      <li>Regression: regression $k$ targets</li>
    </ul>
  </li>
  <li>Node-level prediction: \(\hat{\mathbf{y}}_v=\operatorname{Head}_\text{node}(\mathbf{h}_v^{(L)})=\mathbf{W}^{(H)}\mathbf{h}_v^{(L)}\)
    <ul>
      <li>$\mathbf{W}^{(H)}\in \mathbb{R}^{k\times d}$: maps node embeddings from
$\mathbf{h}_v^{(L)}\in \mathbb{R}^d$ to $\hat{\mathbf{y}}_v\in \mathbb{R}^k$
so that we can compute loss</li>
    </ul>
  </li>
  <li>Edge-level prediction: \(\widehat{\boldsymbol{y}}_{u v}=\operatorname{Head}_{\text {edge } e}\left(\mathbf{h}_u^{(L)}, \mathbf{h}_v^{(L)}\right)\)
    <ul>
      <li>Options for HEAD:
        <ol>
          <li>Concatenation + Linear: \(\widehat{\boldsymbol{y}}_{u v}=\operatorname{Linear}\left(\operatorname{Concat}\left(\mathbf{h}_u^{(L)}, \mathbf{h}_v^{(L)}\right)\right)\)</li>
          <li>Dot product:
            <ul>
              <li>One-way: $\hat{\mathbf{y}}_{uv}=\left(\mathbf{h}_u^{(L)}\right)^\intercal \mathbf{h}_v^{(L)}$</li>
              <li>$k$-way: similar to multi-head attention, we use different $\mathbf{W}^{(i)}$: \(\begin{gathered}
   \widehat{\boldsymbol{y}}_{u v}^{(1)}=\left(\mathbf{h}_u^{(L)}\right)^T \mathbf{W}^{(1)} \mathbf{h}_v^{(L)} \\
   \widehat{\boldsymbol{y}}_{u v}^{(k)}=\left(\mathbf{h}_u^{(L)}\right)^T \mathbf{W}^{(k)} \mathbf{h}_v^{(L)} \\
   \widehat{\boldsymbol{y}}_{u v}=\operatorname{Concat}\left(\widehat{\boldsymbol{y}}_{u v}^{(1)}, \ldots, \widehat{\boldsymbol{y}}_{u v}^{(k)}\right) \in \mathbb{R}^k
   \end{gathered}\)</li>
            </ul>
          </li>
        </ol>
      </li>
    </ul>
  </li>
  <li>
    <p>Graph-level prediction:</p>

    <ul>
      <li>Make predictions using all the node embeddings in our graph: \(\widehat{\boldsymbol{y}}_G=\operatorname{Head}_{\text {graph }}\left(\left\{\mathbf{h}_v^{(L)} \in \mathbb{R}^d, \forall v \in G\right\}\right)\)</li>
      <li>
        <p>The HEAD for graph prediction is similar to the AGG operation in a GNN layer</p>

        <ol>
          <li>Global mean pooling: \(\widehat{\boldsymbol{y}}_G=\operatorname{Mean}\left(\left\{\mathbf{h}_v^{(L)} \in \mathbb{R}^d, \forall v \in G\right\}\right)\)</li>
          <li>
            <p>Global max pooling: \(\widehat{\boldsymbol{y}}_G=\operatorname{Max}\left(\left\{\mathbf{h}_v^{(L)} \in \mathbb{R}^d, \forall v \in G\right\}\right)\)</p>
          </li>
          <li>Global sum pooling: \(\widehat{\boldsymbol{y}}_G=\operatorname{Max}\left(\left\{\mathbf{h}_v^{(L)} \in \mathbb{R}^d, \forall v \in G\right\}\right)\)</li>
        </ol>
      </li>
      <li>Issues:
        <ul>
          <li>Global pooling over large graphs will lose information</li>
          <li>Example pathology with sum aggregation:
            <ul>
              <li>
\[G_1=\{-1, -2, 0, 1, 2\}\implies \hat{y}_G=\operatorname{Sum}(\{-1,-2,0,1,2\})=0\]
              </li>
              <li>
\[G_2=\{-10, -20, 0, 10, 20\}\implies \hat{y}_G=\operatorname{Sum}(\{-1,-2,0,1,2\})=0\]
              </li>
              <li>Cannot differentiate $G_1$ and $G_2$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Solution:
        <ul>
          <li>Aggregate all node embeddings hierarchically</li>
          <li>For instance, above, if you partition the graphs into the first two and
last three values and apply sum and ReLU to that, you get 3 for the first
graph and 30 for the second (Lecture 6, Slide 35)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>DiffPool</strong>: hierarchically pool node embeddings
    <ul>
      <li>GNN A: computes node embeddings</li>
      <li>GNN B: computes the cluster that a node belongs to (based on current layer
embeddings)</li>
      <li>These GNNs can be executed in parallel</li>
      <li>Use clustering assignments from GNN B to aggregate node embeddings generated
by GNN A</li>
      <li>Create a single new node for each cluster, maintaining edges between
clusters to generate a new pooled network</li>
      <li>Jointly train GNN A and GNN B</li>
    </ul>
  </li>
  <li>Supervised learning: external labels</li>
  <li>Unsupervised or self-supervised learning: labels come from graph itself, i.e.
links</li>
  <li>Sometimes supervision is still used in unsupervised learning, e.g. train a GNN
to predict node clustering coefficients</li>
  <li><strong>Examples of supervision signals</strong>:
    <ul>
      <li>Node labels: which subject a citation belongs to</li>
      <li>Edge labels: whether an edge is fraudulent</li>
      <li>Graph labels: among molecular graphs, the drug likeness of graphs</li>
    </ul>
  </li>
  <li><strong>Examples of unsupervised signals</strong>:
    <ul>
      <li>Node level: node statistics such as clustering coefficient, PageRank, etc</li>
      <li>Edge level: link prediction, i.e. hide edges and predict if it should be
there</li>
      <li>Graph level: graph statistics like whether two graphs are isomorphic</li>
    </ul>
  </li>
  <li><strong>Advice</strong>: Reduce your task to node/edge/graph labels, since they are easy to
work with; e.g. we know some nodes form a cluster, we can treat the cluster
that a node belongs to as a node label</li>
  <li>How to compute loss?
    <ul>
      <li>Classification loss, e.g. cross-entropy loss</li>
      <li>Regression loss, e.g. MSE</li>
    </ul>
  </li>
  <li>How to evaluate or measure success?
    <ul>
      <li>Accuracy</li>
      <li>ROC AUC</li>
      <li>Root mean squared error (RMSE)</li>
      <li>Mean absolute error (MAE)</li>
    </ul>
  </li>
  <li>Evaluating classification tasks:
    <ul>
      <li>Multi-class classification: accuracy \(\frac{1\left[\operatorname{argmax}\left(\widehat{\boldsymbol{y}}^{(i)}\right)=\boldsymbol{y}^{(i)}\right]}{N}\)</li>
      <li>Binary classification:
        <ul>
          <li>Accuracy: (TP + TN) / (TP + TN + FP + FN) = (TP + TN) /
$\lvert\text{Dataset}\rvert$</li>
          <li>Precision (P): TP / (TP + FP)</li>
          <li>Recall (R): TP / (TP + FN)</li>
          <li>F1-Score: 2P * R / (P + R)</li>
          <li>Metric agnostic classification threshold: ROC AUC, which captures the
TPR/FPR tradeoff
            <ul>
              <li>Intuition: The probability that a classifier will rank a randomly
chosen positive instance higher than a randomly chosen negative one</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="dataset-splitting">Dataset Splitting</h2>

<ul>
  <li>Fixed split:
    <ul>
      <li>Training used for optimizing GNN parameters</li>
      <li>Validation used to evaluate model/tune hyper-parameters</li>
      <li>Test used to report final performance
        <ul>
          <li>Sometimes, we cannot guarantee that the test set will really be held out</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Random split:
    <ul>
      <li>Split $k$ times randomly into training, validation, test and report average
performance using random seeds for each</li>
    </ul>
  </li>
  <li><strong>Problem</strong>: nodes/edges in a graph are not independent, unlike sentences,
images, etc</li>
  <li><strong>Solutions</strong>:
    <ul>
      <li><strong>Transductive setting</strong>:
The input graph can be observed in all dataset splits, but we split on node
labels
        <ul>
          <li>At training time, compute embeddings using the entire graph and train
using the training set’s node labels</li>
          <li>At validation time, we compute embeddings using the entire graph and
evaluate on a different subset of node labels</li>
          <li>Only applicable to node/edge prediction tasks</li>
        </ul>
      </li>
      <li><strong>Inductive setting</strong>: We create multiple graphs by breaking edges
        <ul>
          <li>Now we have 3 independent graphs</li>
          <li>At training time, we compute embeddings using the graph over the
training graph, using only those labels</li>
          <li>At validation time, we compute embeddings using only the validation
graph and evaluate performance on those labels</li>
          <li>Applicable to node, edge, and graph tasks; works for graph tests because
we have to test on unseen graphs</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Example: Link Prediction
    <ul>
      <li>Setting up link prediction requires hiding some edges and letting the GNN
predict if the edges exist</li>
      <li>Technique:
        <ol>
          <li>Assign 2 types of edges in the original graph:
            <ul>
              <li>Message edges: used for GNN message passing</li>
              <li>Supervision edges: used for computing objectives and not fed into GNN</li>
              <li>After this step, only message edges will remain in the graph</li>
            </ul>
          </li>
          <li>Split edges into train, validation, and test
            <ul>
              <li>Option 1: inductive link prediction split, i.e. 3 independent graphs, each
of which will have supervision and message edges and the objective is to
predict supervision edges with the respective subgraph</li>
              <li>Option 2: transductive link prediction split (default setting), i.e.
graph is visible in all splits, but you hold out various supervision
edges for each data split</li>
            </ul>
          </li>
          <li>At training time, use training message edges to predict training
supervision edges</li>
          <li>At validation time, use training messages <strong>and</strong> training
supervision edges to predict validation edges</li>
          <li>At test time, use training message edges, training supervision edges,
and validation edges to predict test edges</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<h1 id="lecture-7-theory-of-graph-neural-networks">Lecture 7: Theory of Graph Neural Networks</h1>

<h2 id="understanding-expressiveness-and-limitations">Understanding Expressiveness and Limitations</h2>

<ul>
  <li>How do we measure and maximize the expressive power of GNNs?</li>
  <li><strong>Key questions</strong>:
    <ul>
      <li>How well can a GNN distinguish different graph structures?</li>
      <li>How well can GNN node embeddings distinguish different node’s local
neighborhood structures?</li>
    </ul>
  </li>
  <li>Fundamentally, a GNN generates node embeddings through a computational graph
defined by the neighborhood (picture on Slide 19, Lecture 7)
    <ul>
      <li>Typical GNNs only see node features, not IDs</li>
      <li>GNNs will generate the same embeddings for nodes whose computation graphs
are identical even if the nodes are not</li>
    </ul>
  </li>
  <li>Computational graphs are identical to rooted subtree structures around each
node</li>
  <li>Most expressive GNNs map different rooted subtrees to different node
embeddings (image on Slide 21, Lecture 7)</li>
  <li><strong>Key concept</strong>: a function $f$ is injective if it maps different elements to
different outputs, i.e. is 1:1; in this case, $f$ retains all the information
about the input</li>
  <li>A maximally expressive GNN should map subtrees to node embeddings injectively</li>
  <li><strong>Key observation</strong>: Subtrees of the same depth can be recursively
characterized from the leaf nodes to the root nodes, e.g. (left: (2 neighbors),
right: (3 neighbors)) and so on up the tree
    <ul>
      <li>If each step of the GNN’s aggregation can fully retain the neighboring
information, the generated node embeddings can distinguish different rooted
subtrees, i.e. if each aggregation step of neighbors is injective, then the embedding
process is injective</li>
    </ul>
  </li>
  <li><strong>Key observation</strong>: The expressive power of GNNs can be characterized by
that of neighborhood aggregation functions and injective aggregation functions
lead to the most expressive GNNs</li>
  <li>A neighbor aggregation can be abstracted as a function over a multi-set</li>
  <li>Analysis:
    <ul>
      <li>GCN uses element-wise mean-pooling, i.e. element-wise mean, linear layer,
ReLU activation
        <ul>
          <li>Failure case is when it collapses distributions (example on Slide 32-34,
Lecture 7)</li>
        </ul>
      </li>
      <li>GraphSAGE uses element-wise max-pooling
        <ul>
          <li>Failure case is when multi-sets contain the same base set of colors, i.e.
it ignores the distribution of colors and collapses to just the minimal set
of colors (image on Slide 36, Lecture 7)</li>
        </ul>
      </li>
      <li><strong>mean and max pooling are not injective</strong> and hence any model based on them
is not maximally expressive</li>
    </ul>
  </li>
  <li><strong>Solution</strong>: design a NN that can model injective multi-set functions
    <ul>
      <li>Any injective multi-set function can be expressed as
\(\underbrace{\phi}_\text{non-linear function}\left(\underbrace{\sum_{x\in S}}_\text{sum over multi-set}\underbrace{f(x)}_\text{some non-linear function}\right)\)</li>
      <li>Proof intuition: $f$ produces one-hot encodings of colors, and summation of
one-hot encodings retains all the information about the input multi-set.</li>
    </ul>
  </li>
  <li><strong>Universal approximation theorem</strong>: 1-hidden-layer MLP with
sufficiently-large hidden dimensionality and appropriate non-linearity
$\sigma(\cdot)$ including ReLU and sigmoid can approximate any continuous
function to an arbitrary accuracy.
    <ul>
      <li>We can use this to model the injective multi-set function:
$\operatorname{MLP}<em>\phi\left(\sum</em>{x\in S}\operatorname{MLP}_f(x)\right)$</li>
      <li>In practice, 100-500 hidden dimensions are sufficient</li>
    </ul>
  </li>
</ul>

<h2 id="graph-isomorphism-network-gin-the-most-expressive-gnn">Graph Isomorphism Network (GIN): The most expressive GNN</h2>

<ul>
  <li>Uses the results above to define the following injective aggregation function: $\operatorname{MLP}<em>\phi\left(\sum</em>{x\in S}\operatorname{MLP}_f(x)\right)$</li>
  <li>No failure cases!</li>
  <li>GIN is THE most expressive GNN in the class of message-passing GNNs we have
introduced</li>
</ul>

<h2 id="relationship-of-expressiveness-to-wl-graph-kernel">Relationship of Expressiveness to WL Graph Kernel</h2>

<ul>
  <li>tl;dr: GIN is a neural network version of the WL graph kernel</li>
  <li><strong>Color refinement algorithm in WL Kernel</strong>:
    <ul>
      <li>Given a graph $G$ with a set of nodes $V$:
        <ul>
          <li>Assign an initial color $c^{(0)}(v)$ to each node $v$.</li>
          <li>Iteratively refine node colors: \(c^{(k+1)}(v)=\operatorname{HASH}\left(c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right)\) where HASH is a perfect has injectively mapping different inputs to different colors</li>
          <li>After $K$ steps of color refinement, $c^{(K)}(v)$ summarizes the structure
of the $K$-hop neighborhood.</li>
        </ul>
      </li>
      <li>Process continues until a stable coloring is reached</li>
      <li>Two graphs are considered isomorphic if they have the same set of colors</li>
      <li>Illustration on Slides 46-48, Lecture 7</li>
    </ul>
  </li>
  <li><strong>GIN uses a neural network to model the injective HASH function</strong>;
specifically, it models the injective function over the tuple \((\underbrace{c^{(k)}(v)}_\text{root colors},
\underbrace{\{c^{(k)}(u)\}_{u\in N(v)}}_\text{neighbor colors})\)
    <ul>
      <li>All together, the model is:
\(\operatorname{GINConv}\left(c^{(k)}(v),\{c^{(k)}(u)\}_{u\in N(v)}\right)=\underbrace{\operatorname{MLP}_{\Phi}}_\text{provides one-hot input for next layer}\left((1+\epsilon) \cdot \operatorname{MLP}_f\left(c^{(k)}(v)\right)+\sum_{u \in N(v)} \operatorname{MLP}_f\left(c^{(k)}(u)\right)\right)\)
        <ul>
          <li>Here, $\epsilon$ is a learnable scalar</li>
        </ul>
      </li>
      <li>The full algorithm is, given a graph $G$ with a set of nodes $V$:
        <ul>
          <li>Assign an initial vector $c^{(0)}(v)$ to each node $v$.</li>
          <li>Iteratively update node vectors with \(c^{(k+1)}(v)=\operatorname{GINConv}\left(\left\{c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right\}\right)\)
            <ul>
              <li>Where GINConv maps different inputs to different embeddings, i.e. it is
a differentiable color HASH function</li>
            </ul>
          </li>
          <li>After $K$ steps of GIN iterations, $c^{(K)}(v)$ summarizes the structure
of the $K$-hop neighborhood.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>GIN can be understood as a differentiable neural version of the WL graph
kernel</strong>:</li>
</ul>

<table>
  <thead>
    <tr>
      <th>model</th>
      <th>update target</th>
      <th>update function</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>WL Graph Kernel</td>
      <td>Node colors (one-hot)</td>
      <td>HASH</td>
    </tr>
    <tr>
      <td>GIN</td>
      <td>Node embeddings (low-dim vectors)</td>
      <td>GINConv</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Advantages of GIN over WL graph kernel are:
    <ul>
      <li>Node embeddings are <strong>low-dimensional</strong> hence than can capture fine-grained
similarity of different nodes.</li>
      <li>Parameters of the update function can be learned for the downstream tasks.</li>
    </ul>
  </li>
  <li>Because of the relationship between GIN and the WL graph kernel, their
expressivity is exactly the same; namely, if two graphs can be distinguished
by GIN, then they can be by the WL Kernel and vice versa</li>
  <li>How powerful is this? Why is this important?
    <ul>
      <li>WL kernel has been both theoretically and empirically shown to distinguish
most real-world graphs</li>
      <li>Hence, GIN is also power enough to distinguish most real-world graphs</li>
    </ul>
  </li>
</ul>

<h2 id="looking-forward">Looking forward</h2>

<ul>
  <li>Can the expressive power of GNNs be further improved? Some basic graph
structures like difference in cycles cannot be distinguished by current GNNs
(will address in Lecture 15)</li>
</ul>

<h2 id="summary-1">Summary</h2>

<ul>
  <li>GIN designs a NN that can model an injective multi-set function</li>
  <li>GIN is the most expressive GNN model</li>
  <li>The key is to use element-wise sum pooling instead of mean/max-pooling</li>
  <li>GIN is closely related to the WL kernel</li>
  <li>Both GIN and WL graph kernels can distinguish most real-world graphs</li>
</ul>

<h1 id="lecture-8-label-propagation-on-graphs">Lecture 8: Label Propagation on Graphs</h1>

<ul>
  <li><strong>Question</strong>: Given a network with labels on some nodes, how do we assign
labels to all other nodes in the network, e.g. fraudsters in a social network?</li>
  <li>Node embeddings is one method to solve this problem; can we further use
network topology?</li>
  <li>Given the labels of some nodes, let’s predict the labels of unlabelled nodes
    <ul>
      <li>Transductive node classification (also called semi-supervised) node
classification</li>
    </ul>
  </li>
  <li><strong>Intuition</strong>: correlations exist in networks, i.e. connected nodes tend to
share the same label.</li>
  <li>3 Techniques:
    <ul>
      <li>Label propagation</li>
      <li>Correct &amp; Smooth</li>
      <li>Masked label prediction</li>
    </ul>
  </li>
</ul>

<h1 id="lecture-9-machine-learning-with-heterogeneous-graphs">Lecture 9: Machine Learning with Heterogeneous Graphs</h1>

<h1 id="lecture-10-knowledge-graph-embeddings">Lecture 10: Knowledge Graph Embeddings</h1>

<ul>
  <li>Heterogeneous graphs are graphs with multiple relation types, each of which
gets different network weights</li>
  <li>Nodes are labeled with types and edges capture relationships</li>
  <li>Examples:
    <ul>
      <li>Nodes: drug, disease, event, protein pathways</li>
      <li>Relation types: has_func, causes, assoc, treats, is_a</li>
    </ul>
  </li>
  <li><strong>Problem</strong>: knowledge graphs are often incomplete and many true edges are
missing; enumerating all relationships and/or facts may also be intractable</li>
  <li><strong>Task</strong>: given an enormous KG, can we complete the KG, i.e. for a given
(head, relation), can we predict the tail?</li>
  <li><strong>Key ideas</strong>:
    <ul>
      <li>model entities and relations in an embedding/vector space</li>
      <li>associate entities and relations with shallow embeddings</li>
      <li>no GNN is learned here</li>
    </ul>
  </li>
  <li>
    <p><strong>Models</strong>:</p>

    <table>
      <thead>
        <tr>
          <th>model</th>
          <th>score</th>
          <th>embedding</th>
          <th>sym</th>
          <th>antisym.</th>
          <th>inv.</th>
          <th>compos.</th>
          <th>1:N</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>TransE</td>
          <td>$-\lVert\mathbf{h} + \mathbf{r}-\mathbf{t}\rVert$</td>
          <td>$\mathbf{h},\mathbf{r},\mathbf{t}\in \mathbb{R}^k$</td>
          <td>x</td>
          <td>o</td>
          <td>o</td>
          <td>o</td>
          <td>x</td>
        </tr>
        <tr>
          <td>TransR</td>
          <td>$-\lVert\mathbf{M}_r\mathbf{h} + \mathbf{r} - \mathbf{M}_r\mathbf{t}\rVert$</td>
          <td>$\mathbf{h},\mathbf{r},\mathbf{t}\in \mathbb{R}^k,\mathbf{M}_r\in \mathbb{R}^{d\times k}$</td>
          <td>o</td>
          <td>o</td>
          <td>o</td>
          <td>o</td>
          <td>o</td>
        </tr>
        <tr>
          <td>DistMult</td>
          <td>$&lt;\mathbf{h},\mathbf{r},\mathbf{t}&gt;$</td>
          <td>$\mathbf{h},\mathbf{r},\mathbf{t}\in \mathbb{R}^k$</td>
          <td>o</td>
          <td>x</td>
          <td>x</td>
          <td>x</td>
          <td>o</td>
        </tr>
        <tr>
          <td>ComplEx</td>
          <td>$Re(&lt;\mathbf{h},\mathbf{r},\overline{\mathbf{t}}&gt;)$</td>
          <td>$\mathbf{h},\mathbf{r},\mathbf{t}\in \mathbb{C}^k$</td>
          <td>o</td>
          <td>o</td>
          <td>o</td>
          <td>x</td>
          <td>o</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Relationships:</p>

    <ul>
      <li>Symmetric:
        <ul>
          <li>$r(h,t)\implies r(t,h)\quad\forall h,t$</li>
          <li>mother $\xrightarrow[\text{spouse}]{}$ father, father $\xrightarrow[\text{spouse}]{}$ mother</li>
        </ul>
      </li>
      <li>Antisymmetric:
        <ul>
          <li>$r(h,t)\implies \lnot r(t,h)\quad\forall h,t$</li>
          <li>father $\xrightarrow[\text{child}]{}$ son, then <strong>not</strong> son $\xrightarrow[\text{child}]{}$ father</li>
        </ul>
      </li>
      <li>Inverse:
        <ul>
          <li>$r_2(h,t)\implies r_1(t,h)$</li>
          <li>professor $\xrightarrow[\text{advisor}]{}$ student $\implies$ student $\xrightarrow[\text{advisee}]{}$ professor</li>
        </ul>
      </li>
      <li>Composable:
        <ul>
          <li>$r_1(x,y)\land r_2(y,z)\implies r_3(x,z)\quad\forall x,y,z$</li>
          <li>father$\xrightarrow[\text{wife}]{}$mother$\xrightarrow[\text{mother}]{}$mother-in-law</li>
        </ul>
      </li>
      <li>1:N:
        <ul>
          <li>$r(h,t_1),r(h,t_2),\ldots, r(h,t_n)$ are all true.</li>
          <li>father $\xrightarrow[\text{child}]{}$ son <strong>and</strong> father $\xrightarrow[\text{chld}]{}$ daughter</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>TransE</strong>: models translation of any relation in the <strong>same</strong> embedding space
    <ul>
      <li>cannot model symmetry, i.e. family, roommate, unless $h=t$ or $r=0$</li>
      <li>cannot model 1:N relations, $t_1=h+r=t_2$ when $t_1\ne t_2$</li>
      <li>can model antisymmetric: $h + r = t$, but $t + r \ne h$</li>
      <li>can model inverse relationships by flipping sign of $r$</li>
      <li>can model composition</li>
    </ul>
  </li>
  <li><strong>TransR</strong>: model entities as vectors in the entity space $\mathbb{R}^d$ and
model each relation as a vector in relation space $\mathbf{r}\in \mathbb{R}^k$
with $\mathbf{M}_r\in \mathbb{R}^{k\times d}$ as the projection matrix.
    <ul>
      <li>can model symmetric relations by projecting head and tail to same location
in relation space (note that different symmetric relationships may have
different $\mathbf{M}_r$)</li>
      <li>can model antisymmetric relations the same as TransE, but in the relation space</li>
      <li>can model 1:N by projecting all tails to the same location in the relation
space</li>
      <li>can model inverse relations the same as TransE, but in the relation space</li>
      <li>can model composition relations; TransR models a triple with linear
functions and they are chainable, proof on slide 37 of 10-kg.pdf</li>
    </ul>
  </li>
  <li><strong>DistMult</strong>:
    <ul>
      <li>TransE and TransR use negative of L1/L2 distance</li>
      <li>Another strategy is to adopt <strong>bilinear</strong> modeling, making the score
function a 3-way dot product
        <ul>
          <li><strong>Intuition</strong>: can be viewed as a cosine similarity between $h\cdot r$ and
$t$ where $h\cdot r$ is defined as $h_i\cdot r_i$</li>
        </ul>
      </li>
      <li>This defines half spaces, where if you are on the same side of the half
space as $h\cdot r$ you are positive, otherwise negative</li>
      <li>cannot model antisymmetric relations</li>
      <li>cannot model inverse relations (i.e. advisor, advisee would be the same
relation)</li>
      <li>cannot model composition relations
        <ul>
          <li><strong>intuition</strong>: DistMult defines a hyperplane for each (head, relation),
and the union of the hyperplane induced by multi-hops of relations, e.g.
($r_1,r_2$) cannot be expressed using a single hyperplane, i.e. the union
of hyperplanes cannot be captured as a hyperplane that captures the
desired half-space</li>
        </ul>
      </li>
      <li>can model 1:N relations</li>
      <li>can model symmetric relations</li>
    </ul>
  </li>
  <li><strong>ComplEx</strong>: models entities and relations in $\mathbb{C}^k$
    <ul>
      <li>score function is $f_r(h,t)=Re(\sum_i \mathbf{h}_i\cdot \mathbf{r}_i\cdot
\overline{\mathbf{t}}_i)$</li>
      <li>similar to DistMult, ComplEx cannot model compositions</li>
      <li>can learn antisymmetric relations due to complex conjugate
        <ul>
          <li>high: $f_r(h,t)=f_r(h,t)=Re(\sum_i \mathbf{h}_i\cdot \mathbf{r}_i\cdot \overline{\mathbf{t}}_i)$</li>
          <li>low: $f_r(h,t)=f_r(h,t)=Re(\sum_i \mathbf{t}_i\cdot \mathbf{r}_i\cdot \overline{\mathbf{h}}_i)$</li>
        </ul>
      </li>
      <li>can learn symmetric relations - when $Im(\mathbf{r})=0$:
\(\begin{aligned}
f_r(\mathbf{h}, \mathbf{t})
&amp;=\operatorname{Re}\left(\sum_i \mathbf{h}_i \cdot \mathbf{r}_i \cdot \overline{\mathbf{t}}_i\right) \\
&amp;=\sum_i \operatorname{Re}\left(\mathbf{r}_i \cdot \mathbf{h}_i \cdot \overline{\mathbf{t}}_i\right) \\
&amp;=\sum_i \mathbf{r}_i \cdot \operatorname{Re}\left(\mathbf{h}_i \cdot \overline{\mathbf{t}}_i\right) \\
&amp;=\sum_i \mathbf{r}_i \cdot \operatorname{Re}\left(\overline{\mathbf{h}}_i \cdot \mathbf{t}_i\right) \\
&amp;=\sum_i \operatorname{Re}\left(\mathbf{r}_i \cdot \overline{\mathbf{h}}_i\cdot \mathbf{t}_i\right)=f_r(t, h)
\end{aligned}\)</li>
      <li>can model inverse relations with $\mathbf{r}_1=\overline{\mathbf{r}}_2$
        <ul>
          <li>
\[\mathbf{r}_2 = \arg\max_\mathbf{r} Re(&lt;\mathbf{h},\mathbf{r},\overline{\mathbf{t}}&gt;)\]
          </li>
          <li>
\[\mathbf{r}_1 = \arg\max_\mathbf{r} Re(&lt;\mathbf{t},\mathbf{r},\overline{\mathbf{h}}&gt;)\]
          </li>
        </ul>
      </li>
      <li>can model 1:N relations like DistMult</li>
    </ul>
  </li>
  <li>
    <p><strong>RotatE</strong>: TransE in Complex space</p>
  </li>
  <li>General rules:
    <ul>
      <li>Use TransE if the KG does not have many symmetric relationships</li>
    </ul>
  </li>
</ul>

<h1 id="lecture-11-knowledge-graphs">Lecture 11: Knowledge Graphs</h1>

<h1 id="lecture-12-fast-neural-subgraph-matching-and-counting">Lecture 12: Fast Neural Subgraph Matching and Counting</h1>

<h1 id="lecture-13-gnns-for-recommender-systems">Lecture 13: GNNs for Recommender Systems</h1>

<h1 id="lecture-14-deep-generative-models-for-graphs">Lecture 14: Deep Generative Models for Graphs</h1>

<h2 id="overview">Overview</h2>

<ul>
  <li><strong>Question</strong>: How do we generate realistic graphs?</li>
  <li>Applications: drug discovery, material design, social network modeling
    <ul>
      <li>Insights, predictions, simulations, anomaly detection</li>
    </ul>
  </li>
  <li><strong>Goal 1</strong>: Realistic graph generation; generate graphs that are similar to a
set of graphs</li>
  <li><strong>Goal 2</strong>: Goal-directed graph generation; generate graphs that optimize
given objectives/constraints</li>
  <li>$p_\text{data}(x)$ is the (unknown) data distribution</li>
  <li>$p_\text{model}(x;\theta)$ is the model, parameterized by $\theta$, that we
use to approximate $p_\text{data}(x)$</li>
  <li>The objective is to make $p_\text{model}(x;\theta)$ as close to $p_\text{data}(x)$ as
possible</li>
  <li>One possible way to do this is maximum likelihood: \(\boldsymbol{\theta}^*=\underset{\boldsymbol{\theta}}{\arg \max } \mathbb{E}_{x \sim p_{\text {data }}} \log p_{\text {model }}(\boldsymbol{x} \mid \boldsymbol{\theta})\)</li>
  <li>How do we sample from $p_\text{model}(x;\theta)$, i.e. a complex distribution?
    <ol>
      <li>Sample form a simple noise distribution $\mathbf{z}_i\sim \operatorname{Normal}\left(0,1\right)$</li>
      <li>Transform the noise $\mathbf{z}_i$ via $f(\cdot)$ so $x$ follows the
complex distribution</li>
      <li>Use Deep NN to train $f(\cdot)$</li>
    </ol>
  </li>
  <li><strong>Auto-regressive models</strong>: $p_\text{model}(x; theta)$ is used for both density estimation and sampling
    <ul>
      <li>Includes models like Variational Auto Encoders (VAEs) and Generative
Adversarial Nets (GANs)</li>
      <li>Idea: Chain rule: the joint distribution is a product of conditional
distributions: \(p_{\text {model }}(\boldsymbol{x} ; \theta)=\prod_{t=1}^n p_{\text {model }}\left(x_t \mid x_1, \ldots, x_{t-1} ; \theta\right)\)</li>
      <li>In our case $x_t$ will be the $t$-th action, i.e. add node or edge</li>
    </ul>
  </li>
</ul>

<h2 id="graph-rnn">Graph RNN</h2>

<ul>
  <li>Two RNNs, one for generating nodes, the other for edges for each node,
connecting to previous nodes in the graph, illustration on Slide 26, Lecture
14</li>
  <li>At each node-level step, a node is added to the graph until the stop token is
received by the RNN</li>
  <li>At each edge-level step, the edge RNN decides whether to connect the current
node to a previously seen node</li>
  <li>Together, this can be seen as a sequence (nodes) of sequences (edges)</li>
  <li>This can also be imagined as creating the adjacency matrix</li>
  <li>Basic RNN cell:
    <ul>
      <li>$s_t = \sigma(W\cdot x_t + U\cdot s_{t-1})$</li>
      <li>$y_t = V\cdot s_t$</li>
      <li>LSTM and GRU are more advanced RNN cells</li>
    </ul>
  </li>
  <li>How to use an RNN on graphs?
    <ul>
      <li>Let input be the previous steps output</li>
      <li>Initialize the sequence with the start of sequence (SOS) token</li>
      <li>Use end of sequence token (EOS) as an extra RNN output
        <ul>
          <li>If EOS=0, continue generation</li>
          <li>If EOS=1, stop generation</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="edge-level-rnn">Edge-level RNN</h3>

<ul>
  <li><strong>Goal</strong>: Model \(p_{\text {model }}(\boldsymbol{x} ; \theta)=\prod_{t=1}^n p_{\text {model }}\left(x_t \mid x_1, \ldots, x_{t-1} ; \theta\right)\)</li>
  <li>Let $y_t=p_\text{model}(x_t\mid x_1,\ldots,x_{t-1};\theta)$</li>
  <li>Then we need to sample $x_{t+1}$ from $y_t: x_{t+1}\sim y_t$</li>
  <li><strong>Each step outputs the probability of a single edge</strong></li>
  <li>Then we sample from that distribution and feed the sample to the next step;
illustration on Slide 31, Lecture 14</li>
  <li>To train the model, use teacher forcing, i.e. correct the output $\hat{y}$
with the true $y$ and correct the next step’s input to be correct as well,
i.e. if $y_t=0$, then $x_{t+1}=0$</li>
  <li>Use binary cross entropy loss to train RNN: \(L=-\left[y_1^* \log \left(y_1\right)+\left(1-y_1^*\right) \log \left(1-y_1\right)\right]\)</li>
  <li>If $y_1^*=1$, we minimize $-\log(y_1)$ by making $y_1$ larger</li>
  <li>If $y_1^*=-$, we minimize $-\log(1-y_1)$ by making $y_1$ smaller</li>
  <li>This fits the RNNs predictions to the true data (edges)</li>
</ul>

<h3 id="process">Process</h3>

<ol>
  <li>Add a new node: run node RNN for a step and use its output to initialize an
edge RNN.</li>
  <li>Add new edges for the new node: run edge RNN to predict if the new node will
connect to each of the previous nodes.</li>
  <li>Add a new node: we use the last hidden state of the edge RNN to run the node
RNN for another step.</li>
  <li>Stop graph generation: if the edge RNN outputs EOS at step 1, we know no
edges are connected to the new node. We stop the graph generation.</li>
</ol>

<ul>
  <li>Illustrated on slides 35-42, Lecture 14</li>
  <li>At test time, replace input with GNN’s own sampled predictions (Slide 43,
Lecture 14)</li>
</ul>

<h3 id="tractability">Tractability</h3>

<ul>
  <li>If any newly added node can connect to <em>any</em> previous node, this quickly makes
generation intractable; you need to generate a full adjacency matrix and the
dependencies are long and complex</li>
  <li><strong>Solution</strong>: use BFS node ordering, which reduces the number of possible node
orderings from $O(n!)$ to the number of distinct BFS orderings
    <ul>
      <li>Only requires memory of last two steps instead of $n-1$ steps</li>
      <li>In other words, you only consider connecting to nodes on the BFS frontier;
the nodes that are not connected to nodes on the frontier are not considered</li>
    </ul>
  </li>
</ul>

<h3 id="evaluation">Evaluation</h3>

<ul>
  <li><strong>Goal</strong>: Define similarity metrics for graphs</li>
  <li><strong>Solution</strong>:
    <ul>
      <li>Visual similarity</li>
      <li>Graph statistics similarity</li>
    </ul>
  </li>
  <li>GraphRNN is able to train grids, unlike Kronecker, MMSB, and B-A methods, and
also does well on other graphs</li>
</ul>

<h3 id="applications-drug-discovery">Applications: Drug Discovery</h3>

<ul>
  <li><strong>Question</strong>: Can we learn a model that can generate valid and realistic
molecules with optimized property scores?</li>
  <li><strong>Goal directed graph generation</strong>:
    <ul>
      <li>Optimize a given objective (high scores), e.g. drug-likeness</li>
      <li>Obey underlying rules (valid), e.g. chemical validity rules</li>
      <li>Are learned from examples (realistic), e.g. imitating a molecule graph
dataset</li>
    </ul>
  </li>
  <li>The hard part: objectives like drug-likeness are governed by physical laws
which are assumed to be unknown to us</li>
  <li><strong>Idea</strong>: Reinforcement learning
    <ul>
      <li>An ML agent observes the environment, takes an action to interact with the
environment, and receives positive or negative reward</li>
      <li>The agent learns from this loop</li>
      <li><strong>Key idea</strong>: the agent can directly learn from the environment, which is a
blackbox to the agent</li>
    </ul>
  </li>
</ul>

<h3 id="solution-graph-convolutional-policy-network-gcpn">Solution: Graph Convolutional Policy Network (GCPN)</h3>

<ul>
  <li>GNN captures graph structural information</li>
  <li>RL guides generation toward the desired objectives</li>
  <li>Supervised training imitates examples in given datasets</li>
  <li>GCPN vs GraphRNN:
    <ul>
      <li>Both generate graphs sequentially</li>
      <li>Both imitate a given graph dataset</li>
      <li>GCPN uses GNN to predict the generation action
        <ul>
          <li>Pros: GNN is more expressive than RNN</li>
          <li>Cons: GNN takes longer time to compute than RNN</li>
        </ul>
      </li>
      <li>GCPN further uses RL to direct graph generation to our goals, which enables
goal-directed graph generation</li>
      <li>Illustration on Slide 67, Lecture 14</li>
    </ul>
  </li>
  <li>GCPN process:
    <ol>
      <li>Insert nodes</li>
      <li>Use GNN to predict which nodes to connect</li>
      <li>Take an action (check chemical validity)</li>
      <li>Compute reward</li>
    </ol>
  </li>
  <li>GCPN rewards = final reward + step rewards
    <ul>
      <li>At each step, assign a small positive reward for valid actions, which trains
it to take valid actions</li>
      <li>At the end, assign positive rewards for highly desired properties</li>
    </ul>
  </li>
  <li>GCPN training (illustration on Slide 71, Lecture 14):
    <ol>
      <li>Supervised training: train the policy by imitating the action given by
real, observed graphs and use the gradient (similar to GNN)</li>
      <li>RL Training: train the policy to optimize rewards, using the standard
policy gradient algorithm.</li>
    </ol>
  </li>
  <li>Constrained optimization: edit a given molecule for a few steps to achieve
higher property score</li>
</ul>

<h2 id="summary-2">Summary</h2>

<ul>
  <li>Complex graphs can be generated using sequential generation with deep RL</li>
  <li>Each step a decision is made based on hidden state, which can be
    <ul>
      <li>Implicit: vector representation, decode with RNN</li>
      <li>Explicit: intermediate generated graphs, decode with GCN</li>
    </ul>
  </li>
  <li>Possible tasks:
    <ul>
      <li>Imitating a set of given graphs</li>
      <li>Optimizing graphs towards given goals</li>
    </ul>
  </li>
</ul>

<h1 id="lecture-15-advanced-topics-in-gnns">Lecture 15: Advanced Topics in GNNs</h1>

</article>
    <span class="print-footer"
  >CS 224W: Machine Learning with Graphs - Daniel Jenson
</span>
 <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="daniel.a.jenson@gmail.com"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="https://www.linkedin.com/in/daniel-jenson-7a002a30/"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="https://github.com/danjenson"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2023 &nbsp;&nbsp;DANIEL JENSON</span></br> <br>
<span>This site created with the <a href="//github.com/danjenson/et">Edward Tufte theme for Daniel Jenson </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>

  </body>
</html>
