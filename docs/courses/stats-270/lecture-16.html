<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    STATS 270: Bayesian Statistics
  </title>
  <meta
    name="description"
    content="A collection of work and research by Daniel jenson."
  />

  <!-- Google Fonts -->
  <link
    href="//fonts.googleapis.com/css?family=Lato:400,400italic"
    rel="stylesheet"
    type="text/css"
  />

  <!-- https://docs.mathjax.org/en/latest/input/tex/extensions/index.html -->
  <script>
    MathJax = {
      loader: {
        load: [
          "[tex]/ams",
          "[tex]/gensymb",
          "[tex]/mathtools",
          "[tex]/physics",
        ],
      },
      tex: {
        packages: { "[+]": ["ams", "gensymb", "mathtools", "physics"] },
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
      },
      svg: {
        fontCache: "global",
      },
    };
  </script>
  <!-- https://www.mathjax.org/#gettingstarted -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
  <link rel="stylesheet" type="text/css" href="/notes/assets/css/main.css">

  <link
    rel="canonical"
    href="https://danjenson.github.io/notes/courses/stats-270/lecture-16.html"
  />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
  <nav class="group">
     
    <a href="/notes/books">books</a>
      
    <a href="/notes/papers">papers</a>
      
    <a href="/notes/courses">courses</a>
      
    <a href="/notes/mindmaps">mindmaps</a>
     
    <button
      type="button"
      id="theme-toggle"
      onclick="modeSwitcher()"
      style="cursor: pointer"
    ></button>
  </nav>
  <script src="/notes/assets/js/theme-toggle.js"></script>
</header>

    <article class="group">
<h1 class="title">
  <a href=".">STATS 270: Bayesian Statistics</a>
</h1>
 
<h2 class="subtitle">Lecture 16: Importance Sampling (2022-11-29)</h2>
 <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#importance-sampling">Importance Sampling</a>
<ul>
<li class="toc-entry toc-h3"><a href="#simple-importance-sampling">Simple Importance Sampling</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#examples">Examples</a>
<ul>
<li class="toc-entry toc-h3"><a href="#example-1">Example 1</a></li>
</ul>
</li>
</ul>\[\newcommand{\op}{\operatorname}
\newcommand{\var}[1]{\op{var}\left[#1\right]}
\newcommand{\sd}[1]{\op{sd}\left[#1\right]}
\newcommand{\cov}[2]{\op{cov}\left[#1, #2\right]}\]

<h2 id="importance-sampling">
<a class="anchor" href="#importance-sampling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Importance Sampling</h2>

<ul>
  <li>Target density: $p(x)$</li>
  <li>Proposal distribution: $q(x)$</li>
  <li>Assume both $p, q$ can be evaluated</li>
</ul>

<h3 id="simple-importance-sampling">
<a class="anchor" href="#simple-importance-sampling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simple Importance Sampling</h3>

<ul>
  <li>Draw $x_i$ for $i=1,\ldots,n$ from $q()$</li>
  <li>Compute $w_i=w(x_i)$ then \(w(x)=
\begin{cases}
&amp;=p(x)/q(x) &amp;\text{if} q(x) &gt; 0 \\
&amp;=0 &amp;\text{otherwise}
\end{cases}\)</li>
  <li>The set $(x_i, w_i)$ for $i=1,\ldots,n$ is a weighted sample.</li>
  <li>If we want \(\alpha=\mathbb{E}_{p}\left[h(x)\right]\), then we can use the
weighted sample \(\hat{\alpha}=\frac{1}{n}\sum_{i=1}^n h(x_i)w_i\).</li>
  <li>
<strong>Theorem</strong>:
    <ul>
      <li>If:
        <ol>
          <li>$\{x: p(x) &gt; 0\}\subset\{x: q(x) &gt; 0\}$</li>
          <li>Variance of $w(x)$ is finite, i.e. $\sigma_w^2$</li>
          <li>$\left|h(x)\right| &lt; M &lt; \infty$</li>
        </ol>
      </li>
      <li>Then:
        <ul>
          <li>$\mathbb{E}\left[\hat{\alpha}\right]=\alpha$</li>
          <li>$\operatorname{var}\left[\hat{\alpha}\right]\le M^2(\sigma_w^2+1)/n$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<strong>Proof</strong>:</li>
</ul>

\[\begin{aligned}
\hat{\alpha}
&amp;=\frac{1}{n} \sum_{i=1}^n h(x_i)w(x_i) \\
\mathbb{E}\left[\hat{\alpha}\right]
&amp;= \mathbb{E}_{q}\left[h(x)w(x)\right] \\
&amp;= \int_{x:q(x) &gt; 0} h(x)w(x)q(x)\dd x \\
&amp;= \int_{x:q(x) &gt; 0} h(x)\frac{p(x)}{q(x)}q(x)\dd x \\
&amp;= \int_{x:p(x)&gt;0,q(x) &gt; 0} h(x)p(x)\dd x \\
&amp;= \int_{x:p(x)&gt;0} h(x)p(x)\dd x \\
&amp;= \mathbb{E}_{p}\left[h(x)\right] \\
&amp;= \alpha \\
\operatorname{var}\left[\hat{\alpha}\right]
&amp;= \frac{\operatorname{var}\left[h(x)w(x)\right]}{n} \\
&amp;\le \frac{\mathbb{E}\left[(h(x)w(x))^2\right]}{n} \\
&amp;\le \frac{M^2 \mathbb{E}\left[(w(x))^2\right]}{n} \\
&amp;= \frac{M^2(\sigma^2_w+1)}{n} \\
\mathbb{E}\left[w(x)\right]
&amp;= \operatorname{var}\left[w(x)\right]+\left(\mathbb{E}\left[w(x)\right]\right)^2 \\
&amp;= \sigma_w^2 + 1
\end{aligned}\]

<ul>
  <li>TODO finish</li>
  <li>TODO drawing</li>
  <li>The renormalized case:</li>
</ul>

\[\begin{aligned}
s_i
&amp;= s(x_i)=h(x_i)w(x_i) \\
w_i
&amp;= w(x_i) \\
r(s,w)
&amp;=\frac{s}{w} \\
\hat{\alpha}
&amp;=\frac{\sum_{i=1}^n h(x_i)w(x_i)}{\sum_{i=1}^n w(x_i)} \\
&amp;=\frac{\frac{1}{n}\sum_{i=1}^n s_i}{\frac{1}{n}\sum_{i=1}^n w_i} \\
&amp;=\frac{\bar{s}}{\bar{w}}=r(\bar{s},\bar{w}) \\
\end{aligned}\]

<ul>
  <li>Objective is to expand $r(\bar{s},\bar{w})$ around $(\mathbb{E}\left[\bar{s}\right],\mathbb{E}\left[\bar{w}\right])=(\mu_s,\mu_w)=(\alpha,1)$</li>
</ul>

\[\begin{aligned}
\hat{\alpha}
&amp;=r(\mu_s,\mu_w)+\pdv{r}{s}(\mu_s,\mu_w)(\bar{s}-\mu_s)+\frac{1}{2}\pdv[2]{r}{s}(\mu_s,\mu_r)(\bar{s}-\mu_s)^2 \\
&amp;=\alpha+\frac{1}{\mu_w}(\bar{s}-\mu_s)-\frac{\mu_s}{\mu_w}(\bar{w}-\mu_w)+\frac{1}{2}\pdv[2]{r}{w}(\mu_s,\mu_w)(\bar{w}-\mu_w)^2 \\
&amp;=\alpha+(\bar{s}-\alpha)-\alpha(\bar{w}-1)+\alpha(\bar{w}-1)^2-(\bar{s}-\alpha)(\bar{w}-1)\cdots+O_p\left(n^{-\frac{3}{2}}\right)\\
\mathbb{E}\left[\hat{\alpha}\right]
&amp;=\alpha+\alpha\cdot\frac{\sigma_w^2}{n}-\frac{1}{n}\cdot\rho\cdot\sigma_w\cdot\sigma_s \\
\operatorname{var}\left[\hat{\alpha}\right]
&amp;\approx
TODO finish
\operatorname{var}\left[\bar{s}-\alpha\bar{w}\right]=\operatorname{var}\left[\widebar{s-\alpha w}\right] \\
&amp;= \frac{\operatorname{var}\left[s-\alpha w\right]}{n} \\
&amp;= \frac{1}{n}\mathbb{E}\left[((h(x)-\alpha)w(x))^2\right] \\
&amp;\le \frac{4M^2 \mathbb{E}\left[w^2\right]}{n} \\
&amp;= \frac{4M^2(\sigma_w^2+1)}{n} \\
&amp;= \frac{4M^2}{n_\operatorname{eff}} \\
n_\operatorname{eff}
&amp;=\frac{n}{1+\sigma_w^2} \\
\end{aligned}\]

<ul>
  <li>
    <p>Compare this with the case when $x_i\sim p(\cdot)$, then
$\operatorname{var}\left[\frac{1}{n}\sum_{i=1}^n h(x_i)\right]\le
\frac{\operatorname{var}\left[h(x)\right]}{n}$ TODO finsih</p>
  </li>
  <li>
<strong>Lemma</strong>: $\sigma_w^2$ is the coefficient of variation of
$u(x)=\frac{f(x)}{g(x)}$</li>
  <li>
<strong>Proof</strong>:</li>
</ul>

\[\begin{aligned}
u(x)
&amp;= c w(x) \\
c
&amp;= \frac{Z_p}{Z_q} \\
\mathbb{E}\left[u\right]
&amp;= c\mathbb{E}\left[w\right]=c \\
(cv)^2
&amp;=\frac{\operatorname{var}\left[u\right]}{\mathbb{E}^2\left[u\right]} \\
&amp;=\frac{\operatorname{var}\left[c u\right]}{\mathbb{E}^2\left[c u\right]} \\
&amp;=\frac{\operatorname{var}\left[w\right]}{\mathbb{E}^2\left[w\right]} \\
&amp;=\frac{\sigma_w^2}{1} \\
&amp;=
\end{aligned}\]

<ul>
  <li>TODO finish</li>
  <li>Remarks:
    <ul>
      <li>Importance sampling is useful when $x$ is of low dimension and you can guess
when $p(x)$ is large.</li>
      <li>But in high dimensions, this is not feasible unless you have convexity and
good bounds</li>
      <li>Try an example when you have time:
        <ul>
          <li>$p\sim\frac{1}{2}\operatorname{Normal}\left(0,I_d\right)+\operatorname{Normal}\left(\mu,I_d\right)$</li>
          <li>Proposal $q\sim t_3(0, k^2 I_d)$</li>
          <li>See if you can control $w(x)^2$ by varying $k$.</li>
          <li>You will see that it is not possible to make $w^2$ smaller than
$O(|\mu^d|)$ (curse of dimensionality); so, in practice importance
sampling is used sequentially</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="examples">
<a class="anchor" href="#examples" aria-hidden="true"><span class="octicon octicon-link"></span></a>Examples</h2>

<ul>
  <li>Target density is $p(x)=\frac{f(x)}{Z_p}$</li>
  <li>Trial density is $q(x)=\frac{g(x)}{Z_q}$</li>
  <li>We can evaluate $f$ and $g$, but we do not know the normalizing constants
$Z_p$ and $Z_q$.</li>
  <li>Then we use importance sampling with normalization:
    <ol>
      <li>Draw $x_i$ for $i=1,\ldots,n$ iid with $q(\cdot)$</li>
      <li>Compute $u_i=\frac{f(x_i)}{g(x_i)}=c\cdot w_i$ where $c=\frac{Z_p}{Z_q}$</li>
      <li>Use $\hat{\alpha}=\frac{\sum_{i=1}^n h(x_i)w(x_i)}{\sum_{i=1}^n
w(x_i)}=\frac{\sum_{i=1}^n h(x_i)u_i}{\sum_{i=1}^n u_i}$</li>
    </ol>
  </li>
</ul>

<h3 id="example-1">
<a class="anchor" href="#example-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Example 1</h3>

<ul>
  <li>$p(Y_i=1-p(Y_i=0)=\frac{e^{\theta x_i}}{1+e^{\theta x_i}}$ for
$i=1,\ldots,n=100$</li>
  <li>$\pi(\theta)\sim \operatorname{Normal}\left(0,100\right)$</li>
  <li>Target density \(p(\theta\mid \{(x_i,y_i)\}_{i=1}^100)=cf(\theta)\)</li>
  <li>Likelihood: \(f(\theta)=\exp\left(-\frac{\theta^2}{100}+\theta\cdot \sum_{i=1}^n X_iY_i-\sum_{i=1}^n \log(1+e^{\theta x_i})\right)\)</li>
  <li>What kind of trial (proposal) distribution do you wan to use?
    <ul>
      <li>Use the prior? If $q$ is the prior density, then sampling is very
inefficient.</li>
      <li>TODO: add image</li>
      <li>Better way:
        <ul>
          <li>Note that log(f) is a convex, so find $\hat{\theta}_\operatorname{MLE}$
and $\sigma^2=-\frac{1}{\pdv[2]{\theta}\mathcal{L}_n(\theta)}$</li>
          <li>Then use $t_\nu(\hat{\theta}_\operatorname{MLE};\sigma)$ when $\nu$ is small,
e.g. 5</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

</article>
    <span class="print-footer"
  >STATS 270: Bayesian Statistics - Daniel Jenson
</span>
 <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="daniel.a.jenson@gmail.com"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="https://www.linkedin.com/in/daniel-jenson-7a002a30/"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="https://github.com/danjenson"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2022 &nbsp;&nbsp;DANIEL JENSON</span></br> <br>
<span>This site created with the <a href="//github.com/danjenson/et">Edward Tufte theme for Daniel Jenson </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>

  </body>
</html>
