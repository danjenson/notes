<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    STATS 270: Bayesian Statistics
  </title>
  <meta
    name="description"
    content="A collection of work and research by Daniel jenson."
  />

  <!-- Google Fonts -->
  <link
    href="//fonts.googleapis.com/css?family=Lato:400,400italic"
    rel="stylesheet"
    type="text/css"
  />

  <!-- https://docs.mathjax.org/en/latest/input/tex/extensions/index.html -->
  <script>
    MathJax = {
      loader: {
        load: [
          "[tex]/ams",
          "[tex]/gensymb",
          "[tex]/mathtools",
          "[tex]/physics",
        ],
      },
      tex: {
        packages: { "[+]": ["ams", "gensymb", "mathtools", "physics"] },
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
      },
      svg: {
        fontCache: "global",
      },
    };
  </script>
  <!-- https://www.mathjax.org/#gettingstarted -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
  <link rel="stylesheet" type="text/css" href="/notes/assets/css/main.css">

  <link
    rel="canonical"
    href="https://danjenson.github.io/notes/courses/stats-270/lecture-7.html"
  />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
  <nav class="group">
     
    <a href="/notes/books">books</a>
      
    <a href="/notes/papers">papers</a>
      
    <a href="/notes/courses">courses</a>
      
    <a href="/notes/mindmaps">mindmaps</a>
     
    <button
      type="button"
      id="theme-toggle"
      onclick="modeSwitcher()"
      style="cursor: pointer"
    ></button>
  </nav>
  <script src="/notes/assets/js/theme-toggle.js"></script>
</header>

    <article class="group">
<h1 class="title">
  <a href=".">STATS 270: Bayesian Statistics</a>
</h1>
 
<h2 class="subtitle">Lecture 7: Non-informative Priors (2022-10-18)</h2>
 <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#non-informative-priors">Non-informative Priors</a></li>
<li class="toc-entry toc-h1"><a href="#jeffreys-prior">Jeffrey’s Prior</a></li>
</ul><h1 id="non-informative-priors">
<a class="anchor" href="#non-informative-priors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Non-informative Priors</h1>

<ul>
  <li>Example: iid variables from $\operatorname{Bernoulli}\left(\theta\right)$
where $\theta\in[0,1]$</li>
  <li>Bayes &amp; Laplace used uniform priors to represent state of “no information.”
    <ul>
      <li>Put the same probability for $[\theta_0\pm\varepsilon]$ and
$[\theta_1\pm\varepsilon]$ (as long as intervals are of same length, they
get the same prior probability – this is the meaning of uniform prior)</li>
      <li>This was popular for about 100 years before being criticized.</li>
      <li>This was criticized by:
        <ol>
          <li>The subjective Bayesian (Ramsey, de Finetti, Savage).</li>
          <li>Frequentists (Neyman, Fisher).</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>Objections to the uniform prior:
    <ul>
      <li>Suppose $\theta\sim \operatorname{Uniform}\left(0,1\right)$ to represent “no
information on $\theta$.”</li>
      <li>Now, $\phi=-\log(\theta)\implies \theta=e^{-\theta}$</li>
      <li>If we use $\phi$ as the parameter, we should also have “no information.”</li>
      <li>A uniform prior on $\phi$ is not equivalent to a uniform prior on $\theta$.
        <ul>
          <li>If $\theta\sim \operatorname{Uniform}\left(0,1\right)$, then $\phi\sim
\operatorname{Exponential}\left(1\right)$</li>
          <li>On which scale should we put a uniform prior?</li>
        </ul>
      </li>
      <li>Suppose there a $\phi(\theta)$ for which the uniform prior is “correct.”
<label for="uniform-phi-theta" class="margin-toggle">⊕</label><input type="checkbox" id="uniform-phi-theta" class="margin-toggle"><span class="marginnote"><img class="fullwidth" src="https://media.githubusercontent.com/media/danjenson/notes/main/courses/stats-270/figures/lecture-7/uniform-phi-theta.png"><br>Uniform probability: $\phi$ vs $\theta$.</span>
</li>
      <li>So equal probability on \(\{\phi\in\phi_0\pm\varepsilon\}\) and
\(\{\phi\in\phi_1\pm\varepsilon\}\) implies that \(\{\theta\in\theta_0\pm
\sigma(\theta_0)\varepsilon\}\) has same probability as
\(\{\theta\in\theta_1\pm\sigma(\theta_1)\varepsilon\}\)
        <ul>
          <li>\(\pi(\theta_0)\sigma(\theta_0)\varepsilon=\pi(\theta_1)\sigma(\theta_1)\varepsilon\),
then \(\pi(\theta)\propto\frac{1}{\sigma(\theta)}\)</li>
          <li>$\sigma(\theta)$ is like a yardstick for measuring distance, like standard
deviation.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>What is a good “yardstick” for measuring distance in $\theta$ scale?
    <ul>
      <li>Consider $\operatorname{Bernoulli}\left(\theta\right)$, if we have $n$
observations, then $p(\theta\mid
x_1,\ldots,x_n)\propto\pi(\theta)\theta^{n\bar{x}}(1-\theta)^{n(1-\bar{x})}$
        <ul>
          <li>If $n$ is large, then the posterior converges to
$\pi(\theta)\theta^{n\theta_0}(1-\theta)^{n(1-\theta_0)}$ (since
$\bar{x}\to\theta_0$ as $n\to\infty$) where $\pi(\theta)$ becomes
irrelevant; this is a
$\operatorname{Beta}\left(n\theta_0,n(1-\theta_0)\right)$, then the
posterior mean is $\theta_0$ and posterior variance is
$\sqrt{\theta_0(1-\theta_0)/n}$, then
$\operatorname{sd}\propto\sqrt{\theta_0(1-\theta_0)}$</li>
          <li>This suggests that $\sigma(\theta_0)=\sqrt{\theta_0(1-\theta_0)}$.</li>
          <li>The non-informative prior is then
\(\pi^*(\theta)\propto\frac{1}{\sqrt{\theta(1-\theta)}}\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="jeffreys-prior">
<a class="anchor" href="#jeffreys-prior" aria-hidden="true"><span class="octicon octicon-link"></span></a>Jeffrey’s Prior</h1>

<ul>
  <li>Let $x_1,\ldots,x_n$ be iid for $f_\theta(\cdot)$.</li>
  <li>Define the score function as $\pdv{\theta}\log f_\theta(x)=S(\theta,x)$</li>
  <li>Fisher information $i(\theta)=\operatorname{Var}(S(\theta,x))=E(S^2)$
    <ul>
      <li>This is because the score function has mean 0.</li>
    </ul>
  </li>
  <li>Bernstein-von Mises Theorem: If the sample size is large, then $\theta\mid
x_1,\ldots,x_n\sim
\operatorname{Normal}\left(\theta_0,(ni(\theta))^{-1}\right)$, which is true
regardless of the prior.</li>
  <li>So, the correct measure is $\sigma(\theta)\propto i(\theta)^{-1/2}$ (square
root of variance above).</li>
  <li>\(\pi^*(\theta)\propto i(\theta)^{1/2}\), which is Jeffrey’s prior.</li>
  <li>Exercise: Check this on the $\operatorname{Bernoulli}\left(\theta\right)$</li>
  <li>
    <p>Geometric view of Jeffrey’s Prior:</p>

    <ul>
      <li>Let \(\mathcal{F}=\{f_\theta(\cdot),\theta\in[0,1]\}\) where $\mathcal{X}$
is arbitrary.</li>
      <li>Define the square root density as $v_\theta(x)=\sqrt{f_\theta(x)}$
        <ul>
          <li>This square root density is a member of the $L_2(\mathcal{X})$ function
space.</li>
        </ul>
      </li>
      <li>$L_2$ space is the inner product space, so $\langle
v_1(x),v_2(x)\rangle=\int_\mathcal{X}v_1(x)v_2(x)\dd x$</li>
      <li>\(\mathcal{G}=\{v_\theta(x):\theta\in\Omega\}\) is a curve in $L_2(\mathcal{X})$</li>
      <li>It is natural to use the arc-length as the distance measure.
<label for="sqrt-density-curve" class="margin-toggle">⊕</label><input type="checkbox" id="sqrt-density-curve" class="margin-toggle"><span class="marginnote"><img class="fullwidth" src="https://media.githubusercontent.com/media/danjenson/notes/main/courses/stats-270/figures/lecture-7/sqrt-density-curve.png"><br>Arc length in sqrt density space.</span>
</li>
      <li>This suggests that $\pi(\theta)\cdot\Delta\propto
\lVert v_{\theta+\Delta}(x)-v_\theta(x)\rVert$
        <ul>
          <li>This arc length will change depending on where you are in the curve, so
take the limit as delta approaches 0:</li>
        </ul>
      </li>
    </ul>

\[\pi(\theta)\propto \lim_{\Delta\to 0}\frac{\lVert v_{\theta+\Delta}(x)-v_\theta(x)\rVert}{\Delta}\]

    <ul>
      <li>
        <p>Bernoulli example:</p>

\[\begin{aligned}
\mathcal{X}&amp;=\{x_1,x_2\}=\{0,1\} \\
f_\theta(x)&amp;=\begin{bmatrix}\theta \\ 1-\theta\end{bmatrix} \\
v_\theta(x)&amp;=\begin{bmatrix}v_\theta(x) \\ v_{1-\theta}(x)\end{bmatrix} \\
\end{aligned}\]

        <ul>
          <li>$L_2(\mathcal{X})=\mathbb{R}^2$</li>
          <li>Then,</li>
        </ul>

\[\begin{aligned}
\pi^*(\theta)
&amp;=\lim_{\Delta\to 0}\frac{\lVert v_{\theta+\Delta}(x)-v_\theta(x)\rVert}{\Delta} \\
&amp;=\frac{1}{2}\cdot\frac{1}{\theta(1-\theta)}
\end{aligned}\]

        <ul>
          <li>This is exactly Jeffrey’s prior.
If $\mathcal{X}$ has $k$ points, then $v_\theta(x)\in\mathbb{R}^k$</li>
          <li>If $\mathcal{X}\in[0,1]$, then $v_\theta(x)=\sqrt{f_\theta(x)}$
is a $L_2$ function in $[0,1]$.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Multi-dimensional case:
    <figure><img src="https://media.githubusercontent.com/media/danjenson/notes/main/courses/stats-270/figures/lecture-7/multi-dimensional-prior.png"><figcaption class="maincolumn-figure">Prior in $\mathbb{R}^2$.</figcaption></figure>

    <ul>
      <li>The objective is to assign a uniform prior on the $L_2$ space ($B$) and then map
that probability back to $\theta$-space ($A$).</li>
      <li>$\theta\in\Omega$ is a bounded region in $\mathbb{R}^2$</li>
      <li>$\delta_1 v=\sqrt{f_{\theta_1+\Delta_1,\theta_2}(x)}-\sqrt{f_{\theta_1,\theta_2}(x)}$</li>
      <li>$\frac{\operatorname{Area}(B)}{\Delta_1\Delta_2}=\operatorname{Area}\left[\frac{\delta_1 v}{\Delta_1},\frac{\delta_2 v}{\Delta_2}\right]$</li>
      <li>Assume $\exists v^{(1)}$ such that $\frac{\delta_1 v}{\Delta_1}\to
v^{(1)}\text{ as }\Delta_1\to 0$ in $L_2$, then
$\operatorname{Area}\left[\frac{\delta_1 v}{\Delta_1},\frac{\delta_2
v}{\Delta_2}\right]\to\operatorname{Area}\left[v^{(1)},v^{(2)}\right]$</li>
      <li>Because $\operatorname{Area}$ is a continuous function of square root, inner
product, and the norm, then the area will converge too.</li>
      <li>$v_{\theta_1,\theta_2}\to v_{\theta_1+\delta_1,\delta_2}$</li>
      <li>Recall that the density is the probability of a small area divided by its
area.</li>
      <li>So, the prior we want $\pi(\theta_1,\theta_2)=\operatorname{Area}[v^{(1)},v^{(2)}]$</li>
      <li>$\operatorname{Vol}=\sqrt{\det(G)}$ where $G$ is the Graham matrix.</li>
      <li>
\[\pi(\theta_1,\theta_2)=\operatorname{Area}[v^{(1)},v^{(2)}]=\left|\det \begin{bmatrix}\langle v^{(1)},v^{(1)}\rangle &amp; \langle v^{(1)},v^{(2)}\rangle \\ \langle v^{(2)},v^{(1)}\rangle &amp; \langle v^{(2)},v^{(2)}\rangle  \end{bmatrix}\right|^{1/2}\]
      </li>
      <li>
        <p>$v^{(1)}(x)=L_2$-limit of $\frac{1}{\Delta_1}(\sqrt{f_{\theta_1+\Delta_1,\theta_2}(x)}-\sqrt{f_{\theta_1,\theta_2}(x)})$ converges to $\pdv{\theta_1}\sqrt{f_{\theta_1,\theta_2}(x)}$</p>

\[\begin{aligned}
\langle v^{(1)},v^{(2)}\rangle
&amp;=\int_\mathcal{X}
\left(\pdv{\theta_1}\sqrt{f_\theta(x)}\right)\left(\pdv{\theta_2}\sqrt{f_\theta(x)}\right)\dd x \\
&amp;=\int_\mathcal{X}
\left(\frac{1}{2}\cdot\frac{\pdv{\theta_1}f_\theta(x)}{\sqrt{f_\theta(x)}}\right)\left(\frac{1}{2}\cdot\frac{\pdv{\theta_2}f_\theta(x)}{\sqrt{f_\theta(x)}}\right)\dd x \\
&amp;=\frac{1}{4}\int_\mathcal{X}
\left(\frac{\pdv{\theta_1}f_\theta(x)}{f_\theta(x)}\right)\left(\frac{\pdv{\theta_2}f_\theta(x)}{f_\theta(x)}\right)\cdot f_\theta(x)\dd x \\
&amp;=\frac{1}{4} \mathbb{E}\left[\pdv{\theta_1}\log f_\theta(x)\cdot \pdv{\theta_2}\log f_\theta(x)\right]
\end{aligned}\]
      </li>
    </ul>
  </li>
  <li>
    <p>In general, for $\theta$ that is $k$-dimensional, we define the score
function as:</p>

\[\begin{aligned}
\dot \ell_\theta&amp;=\begin{bmatrix}
\pdv{\theta_1}\log f_\theta(x) \\
\vdots \\
\pdv{\theta_k}\log f_\theta(x) \\
\end{bmatrix}
\end{aligned}\]
  </li>
  <li>An important property of the score function is \(\mathbb{E}_\theta\left[\ell_\theta(x)\right]=0\)</li>
  <li>Fisher-information is now a $k\times k$ matrix, which is the
variance-covariance matrix of $\dot \ell$:</li>
</ul>

\[i(\theta)_{i,j}=\mathbb{E}_\theta\left[\dot\ell_i\dot\ell_j\right]\]

<ul>
  <li>Then Jeffrey’s prior is defined as
$\pi(\theta)\propto|\det(i(\theta))|^{1/2}$</li>
</ul>

</article>
    <span class="print-footer"
  >STATS 270: Bayesian Statistics - Daniel Jenson
</span>
 <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="daniel.a.jenson@gmail.com"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="https://www.linkedin.com/in/daniel-jenson-7a002a30/"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="https://github.com/danjenson"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2022 &nbsp;&nbsp;DANIEL JENSON</span></br> <br>
<span>This site created with the <a href="//github.com/danjenson/et">Edward Tufte theme for Daniel Jenson </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>

  </body>
</html>
