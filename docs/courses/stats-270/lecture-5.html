<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    STATS 270: Bayesian Statistics
  </title>
  <meta
    name="description"
    content="A collection of work and research by Daniel jenson."
  />

  <!-- Google Fonts -->
  <link
    href="//fonts.googleapis.com/css?family=Lato:400,400italic"
    rel="stylesheet"
    type="text/css"
  />

  <!-- https://docs.mathjax.org/en/latest/input/tex/extensions/index.html -->
  <script>
    MathJax = {
      loader: {
        load: [
          "[tex]/ams",
          "[tex]/gensymb",
          "[tex]/mathtools",
          "[tex]/physics",
        ],
      },
      tex: {
        packages: { "[+]": ["ams", "gensymb", "mathtools", "physics"] },
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
      },
      svg: {
        fontCache: "global",
      },
    };
  </script>
  <!-- https://www.mathjax.org/#gettingstarted -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
  <link rel="stylesheet" type="text/css" href="/notes/assets/css/main.css">

  <link
    rel="canonical"
    href="https://danjenson.github.io/notes/courses/stats-270/lecture-5.html"
  />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
  <nav class="group">
     
    <a href="/notes/books">books</a>
      
    <a href="/notes/papers">papers</a>
      
    <a href="/notes/courses">courses</a>
      
    <a href="/notes/mindmaps">mindmaps</a>
     
    <button
      type="button"
      id="theme-toggle"
      onclick="modeSwitcher()"
      style="cursor: pointer"
    ></button>
  </nav>
  <script src="/notes/assets/js/theme-toggle.js"></script>
</header>

    <article class="group">
<h1 class="title">
  <a href=".">STATS 270: Bayesian Statistics</a>
</h1>
 
<h2 class="subtitle">Lecture 5: Sufficiency (2022-10-11)</h2>
 <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#sufficiency-principle">Sufficiency principle</a></li>
<li class="toc-entry toc-h1"><a href="#bayesian-inference-automatically-satisfies-the-sufficiency-principle">Bayesian Inference Automatically Satisfies the Sufficiency Principle</a></li>
<li class="toc-entry toc-h1"><a href="#minimal-sufficiency">Minimal Sufficiency</a></li>
</ul><h1 id="sufficiency-principle">
<a class="anchor" href="#sufficiency-principle" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sufficiency principle</h1>

<ul>
  <li>Book: “Theoretical Statistics” by Cox &amp; Hinkley (Chapter 2)</li>
  <li>Sufficient statistics:
    <ul>
      <li>$Y\in\mathcal{Y}$: $y$ is distributed according to a density
$f\in\mathcal{F}$</li>
      <li>Let $S$ be a statistic, i.e. $S=S(Y): \mathcal{Y}\to\mathcal{S}$</li>
    </ul>
  </li>
  <li>
<strong>Definition</strong>: $S$ is a sufficient statistic if the conditional distribution
of $Y$ given $S$ is the same for all $f\in\mathcal{F}$ where $\mathcal{F}$ is
<em>any</em> family of densities; there does not need to be a particular parametric
family.
    <ul>
      <li>$f_{Y\mid S}(y\mid s)$</li>
      <li>$g_{Y\mid S}(y\mid s)$</li>
      <li>$\forall f,g\in\mathcal{F}\;f_{Y\mid S}(y\mid s)=g_{Y\mid S}(y\mid s)$ if
$S$ is a sufficient statistic</li>
    </ul>
  </li>
  <li>
    <p>Example 1:</p>

    <ul>
      <li>\(Z=\begin{bmatrix}x_i \\ y_i\end{bmatrix}\) for $i=1,\ldots,n$ are iid
vectors with \(\operatorname{Normal}\left(\begin{bmatrix} 0 \\
0\end{bmatrix},\begin{bmatrix}1 &amp; \rho \\ \rho &amp; 1\end{bmatrix}\right)\) density:
$p(x,y)=\frac{1}{2\pi\sqrt{1-p^2}}\exp\left(-\frac{1}{2(1-p^2)}(x^2+y^2-2\rho xy)\right)$,</li>
      <li>
\[f(z)=\left(\frac{1}{2\pi\sqrt{1-\rho^2}}\right)\exp
\left(-\frac{1}{2}\cdot\frac{1}{1-\rho^2}\left(\underbrace{\sum_{i=1}^n x_i^2 +
\sum_{i=1}^n y_i^2}_{s_2} - 2\rho \underbrace{\sum_{i=1}^n x_iy_i}_{s_1}\right)\right)\]
      </li>
      <li>Note that in the following, when integrating over the region where the
sufficient statistics equal $s_1$ and $s_2$, $f_{Z\mid S}(z\mid s)$ is constant. To
verify this, simply look at the preceding likelihood above.</li>
    </ul>

\[\begin{aligned}
f_{Z\mid S}(z\mid s)
&amp;=\frac{f_Z(z)}{\int_{\{S_1(z)=s_1,S_2(z)=s_2\}}f_Z(z)\dd z} \\
&amp;=\frac{1}{\int_{\{S_1(z)=s_1,S_2(z)=s_2\}}1\dd z} \\
&amp;=\frac{1}{\operatorname{Area}(\{S_1(z)=s_1,S_2(z)=s_2\})}
\end{aligned}\]

    <ul>
      <li>$f_{Z\mid S}=f_Z$ since $Z$ must be compatible with $S$ in order for the
density to be non-zero. In other words, $Z$ is a more detailed event of $S$.</li>
      <li><strong>This does not depend on $\rho$, so $S$ is sufficient.</strong></li>
      <li>If \(\begin{bmatrix}x_i \\ y_i\end{bmatrix}\sim
\operatorname{Normal}\left(\begin{bmatrix}\mu_1 \\
\mu_2\end{bmatrix},\begin{bmatrix}1 &amp; \rho \\ \rho &amp; 1\end{bmatrix}\right)\)
then the sufficient statistic would be:</li>
    </ul>
  </li>
</ul>

\[S=\left(\bar{X},\bar{Y},\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y}),\sum_{i=1}^n
(x_i-\bar{x})^2 + \sum_{i=1}^n (y_i-\bar{y})^2\right)\]

<ul>
  <li>
    <p>Example 2:</p>

    <ul>
      <li>Suppose $\mathcal{F}=\{f_1,f_2,\ldots,f_k\}$; the sample space, $\mathcal{Y}$, is arbitrary,
then there exists a $k-1$ dimensional sufficient statistic.</li>
      <li>To construct it, let $\bar{f}(x)=\frac{1}{k}\sum_{i=1}^k f_i(x)$, without
loss of generality, we can assume $\bar{f}(x)&gt;0\;\forall x\in\mathcal{Y}$
because that would imply that all densities for that $x$ are 0, so that $x$
point can be excluded from the sample space.</li>
      <li>Define $S_1(y)=\frac{f_1(y)}{\bar{f}(y)}$ and
$S_k(y)=\frac{f_k(y)}{\bar{f}_k(y)}$. So, assume $\bar{f}(x)&gt;0\;\forall
x\in\mathcal{Y}$.</li>
      <li>$k\bar{f}(y)=\sum_{j=1}^k f_j(y)=\sum_{j=1}^k S_j(y)\bar{f}(y)\implies
\sum_{j=1}^k S_j(y)=k$
        <ul>
          <li>This means that $S$ is a $k-1$ dimensional statistic.</li>
        </ul>
      </li>
      <li>We claim that $S$ is a sufficient statistic.</li>
      <li>
        <p>Proof:</p>

        <ul>
          <li>
\[\mathcal{Y}_s=\{y\in\mathcal{Y}: S_j(y)=s_j, j=1,\ldots,k\}=\{y:
f_i(y)=s_j\bar{f}(y), j=1,\ldots,k\}\]
          </li>
          <li>
            <p>Let $A\subset\mathcal{Y}_s$, then we can show it no longer depends on
$f_j$, i.e. it no longer depends on which density you pick:</p>

\[\begin{aligned}
  \Pr_{f_j}(Y\in A\mid S=s)
  &amp;=\Pr_{f_j}(Y\in A\mid Y\in \mathcal{Y}_s)
  \\ &amp;=\frac{\int_A f_j(y)\dd y}{\int_{\mathcal{Y}_s}f_j(y)\dd y}
  \\ &amp;=\frac{\int_A s_j\bar{f}(y)\dd y}{\int_{\mathcal{Y}_s}s_j\bar{f}(y)\dd y}
  \\ &amp;=\frac{\int_A \bar{f}(y)\dd y}{\int_{\mathcal{Y}_s}\bar{f}(y)\dd y}
\end{aligned}\]
          </li>
          <li>Because it doesn’t matter what $s_j$ is, $S$ is sufficient.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Numerical example:
<label for="line-partition" class="margin-toggle">⊕</label><input type="checkbox" id="line-partition" class="margin-toggle"><span class="marginnote"><img class="fullwidth" src="https://media.githubusercontent.com/media/danjenson/notes/main/courses/stats-270/figures/lecture-5/line-partition.png"><br>Line partitioned by $S$</span></p>

\[\begin{aligned}
\mathcal{Y}&amp;=[0,1]
\\ \mathcal{F}&amp;=\{f_1,f_2,f_3\}
\\ f_1&amp;=1\;\forall y\in [0,1]
\\ f_2&amp;=\begin{cases}1/2 &amp; y\in[0,1/2] \\ 3/2 &amp; y\in[1/2,1]\end{cases}
\\ f_3&amp;=\begin{cases}1/4 &amp; y\in[0,1/4] \\ 5/4 &amp; y\in[1/4,1]\end{cases}
\\ \bar{f}&amp;=\begin{cases}14/24 &amp; y\in [0,1/4] \\ 22/24 &amp; y\in [1/4,1/2]
\\ 30/24 &amp; y\in[1/2,1]\end{cases}
\\ S&amp;=\begin{bmatrix}f_1 / \bar{f} \\ f_2 / \bar{f}\end{bmatrix}\text{ (3rd is
 determined by first 2)}
 \\ S_1&amp;=\begin{bmatrix}12/7 \\ 6/7\end{bmatrix}
 \quad S_2=\begin{bmatrix}12/11 \\ 6/11\end{bmatrix}
 \quad S_3=\begin{bmatrix}12/15 \\ 6/15\end{bmatrix}
 \\ &amp;\text{Also sufficient:}
 \\ S_1&amp;=\begin{bmatrix}0 \\ 0\end{bmatrix}
 \quad S_2=\begin{bmatrix}0 \\ 1\end{bmatrix}
 \quad S_3=\begin{bmatrix}1 \\ 0\end{bmatrix}
 \\ &amp;\text{Another sufficient:}
\\ S_1&amp;=1\quad S_2=2\quad S_3=3
\end{aligned}\]
  </li>
  <li>Observing $S$ allows us to pick out one of the subintervals.</li>
  <li>Once you pick out a value of $S_j$, you’ve identified a subinterval. And
within that subinterval, each of the densities is different, but uniform. In
other words, the conditional density is the same for each density given the
sufficient statistic.</li>
  <li>Under any $f_j(\cdot)$, $Y$ is uniformly distributed within that
subinterval.</li>
  <li>What is important about the sufficient statistic is its ability to
partition, not the actual value. This shows that the sufficient statistic need
not be unique.</li>
  <li>Sufficiency depends on the partition of the sample space, and not on the
statistic itself.
<label for="y-partition" class="margin-toggle">⊕</label><input type="checkbox" id="y-partition" class="margin-toggle"><span class="marginnote"><img class="fullwidth" src="https://media.githubusercontent.com/media/danjenson/notes/main/courses/stats-270/figures/lecture-5/y-partition.png"><br>Partition of sample space.</span>
</li>
  <li>$\mathcal{Y}=\cup_{s\in\mathcal{S}}\mathcal{Y}_s$ is the partition of the
sample space induced by a statistic $S$.</li>
  <li>For $S$ to be sufficient, all $f\in\mathcal{F}$ must have the same conditional
distribution within each slice of $\mathcal{Y}_s$.</li>
  <li>The concept of sufficient and likelihood were developed by R.A. Fisher in the
early 20th century. He gave the following theorem for the verification of
sufficiency in parametric families.</li>
  <li>
<strong>Sufficiency Factorization Theorem</strong>: If $Y\in f_\theta(\cdot)$ and
$\theta\in\Omega$, then $S(Y)$ is sufficient iff $f_\theta(y)=g(s,\theta)h(y)$
for some $g(\cdot,\cdot)$ and $h(\cdot)$.</li>
  <li>
<strong>Sufficiency Principle</strong>: If $S$ is sufficient, then inference on the
parameter $\theta$ should depend only on $S(Y)$ and not on any other aspects
of $Y$. In other words, if we have $S(y_1)=S(y_2)$ (two possible realizations
of the data), then inference on $\theta$ based on $Y=y_1$ should be identical to
the inference based on $Y=y_2$.</li>
  <li>To consider why this is a reasonable principle, consider drawing $Y$ from
$f_\theta(\cdot)$ in two stages.
    <ol>
      <li>Generate $S=s$ from $f_{S;\theta}(s)$</li>
      <li>Generate $Y=y$ from $f_{Y\mid S;\theta}(y\mid s)$</li>
    </ol>
  </li>
  <li>Then, $Y\sim f_\theta(\cdot)$</li>
  <li>If $S$ is sufficient, then the 2nd stage of the experiment contains no
information on $\theta$ because $f_{Y\mid S;\theta}(y\mid s)$ does not depend
on $\theta$.</li>
</ul>

<h1 id="bayesian-inference-automatically-satisfies-the-sufficiency-principle">
<a class="anchor" href="#bayesian-inference-automatically-satisfies-the-sufficiency-principle" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bayesian Inference Automatically Satisfies the Sufficiency Principle</h1>

<ul>
  <li>Here, $y$ gives no new information about $\theta$:</li>
</ul>

\[p(\theta\mid y)=\frac{\pi(\theta)f_{S;\theta}(s)f_{Y\mid S}(y\mid s)}{\int\pi(\theta)f_{S;\theta}(s)f_{Y\mid S}(y\mid s)\dd \theta}
=\frac{\pi(\theta)f_{S;\theta}(s)}{\int\pi(\theta)f_{S;\theta}(s)\dd \theta}\]

<h1 id="minimal-sufficiency">
<a class="anchor" href="#minimal-sufficiency" aria-hidden="true"><span class="octicon octicon-link"></span></a>Minimal Sufficiency</h1>

<ul>
  <li>If $T$ is a statistic, and $S=g(T(Y))$ then the partition of $T$ is a
refinement of that of $S$; that is, $S$ induces a coarser partition.
<label for="coarseness" class="margin-toggle">⊕</label><input type="checkbox" id="coarseness" class="margin-toggle"><span class="marginnote"><img class="fullwidth" src="https://media.githubusercontent.com/media/danjenson/notes/main/courses/stats-270/figures/lecture-5/coarseness.png"><br>$S$
is coarser than $T$.</span>
</li>
  <li>If both $T$ and $S$ are sufficient, you should use $S$, since it is coarser.</li>
  <li>What if $S$ is not a function of $T$, i.e. they are not nested?</li>
  <li>Bahadur (1954) showed that in general there exists a unique coarsest
sufficient partition. The corresponding statistic is the “minimal sufficient
statistic”.</li>
</ul>

</article>
    <span class="print-footer"
  >STATS 270: Bayesian Statistics - Daniel Jenson
</span>
 <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="daniel.a.jenson@gmail.com"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="https://www.linkedin.com/in/daniel-jenson-7a002a30/"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="https://github.com/danjenson"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2022 &nbsp;&nbsp;DANIEL JENSON</span></br> <br>
<span>This site created with the <a href="//github.com/danjenson/et">Edward Tufte theme for Daniel Jenson </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>

  </body>
</html>
