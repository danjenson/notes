<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    Bayesian Data Analysis
  </title>
  <meta
    name="description"
    content="A collection of work and research by Daniel jenson."
  />

  <!-- Google Fonts -->
  <link
    href="//fonts.googleapis.com/css?family=Lato:400,400italic"
    rel="stylesheet"
    type="text/css"
  />

  <!-- https://docs.mathjax.org/en/latest/input/tex/extensions/index.html -->
  <script>
    MathJax = {
      loader: {
        load: [
          "[tex]/ams",
          "[tex]/gensymb",
          "[tex]/mathtools",
          "[tex]/physics",
        ],
      },
      tex: {
        packages: { "[+]": ["ams", "gensymb", "mathtools", "physics"] },
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
      },
      svg: {
        fontCache: "global",
      },
    };
  </script>
  <!-- https://www.mathjax.org/#gettingstarted -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
  <link rel="stylesheet" type="text/css" href="/notes/assets/css/main.css">

  <link
    rel="canonical"
    href="https://danjenson.github.io/notes/books/bayesian-data-analysis/chapter-2.html"
  />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
  <nav class="group">
     
    <a href="/notes/books">books</a>
      
    <a href="/notes/papers">papers</a>
      
    <a href="/notes/courses">courses</a>
      
    <a href="/notes/mindmaps">mindmaps</a>
     
    <button
      type="button"
      id="theme-toggle"
      onclick="modeSwitcher()"
      style="cursor: pointer"
    ></button>
  </nav>
  <script src="/notes/assets/js/theme-toggle.js"></script>
</header>

    <article class="group">
<h1 class="title">
  <a href=".">Bayesian Data Analysis</a>
</h1>
 
<h2 class="subtitle">Chapter 2: Single-parameter models</h2>
 <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#21-estimating-a-probability-from-binomial-data">2.1 Estimating a probability from binomial data</a></li>
<li class="toc-entry toc-h1"><a href="#22-posterior-as-compromise-between-data-and-prior-information">2.2 Posterior as compromise between data and prior information</a></li>
<li class="toc-entry toc-h1"><a href="#23-summarizing-posterior-inference">2.3 Summarizing posterior inference</a></li>
<li class="toc-entry toc-h1"><a href="#24-informative-prior-distributions">2.4 Informative prior distributions</a></li>
<li class="toc-entry toc-h1"><a href="#25-normal-distribution-with-known-variance">2.5 Normal distribution with known variance</a></li>
<li class="toc-entry toc-h1"><a href="#26-other-standard-single-parameter-models">2.6 Other standard single-parameter models</a></li>
<li class="toc-entry toc-h1"><a href="#27-example-informative-prior-distribution-for-cancer-rates">2.7 Example: informative prior distribution for cancer rates</a></li>
<li class="toc-entry toc-h1"><a href="#28-noninformative-prior-distributions">2.8 Noninformative prior distributions</a></li>
<li class="toc-entry toc-h1"><a href="#29-weakly-informative-prior-distributions">2.9 Weakly informative prior distributions</a></li>
</ul><h1 id="21-estimating-a-probability-from-binomial-data">
<a class="anchor" href="#21-estimating-a-probability-from-binomial-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1 Estimating a probability from binomial data</h1>

<ul>
  <li>For the binomial, $p(y\mid\theta)=\operatorname{Binomial}\left(y\mid
n,\theta\right)=\binom{n}{y}\theta^y(1-\theta)^{n-y}$, $n$ is suppressed on
the left hand side because it is regarded as part of the experimental design
that is considered fixed.</li>
  <li>The posterior of the binomial is $\theta\mid y\sim
\operatorname{Beta}\left(y+1,n-y+1\right)$.</li>
  <li>Jacob Bernoulli identified the “weak law of large numbers”, namely if $y\sim
\operatorname{Binomial}\left(n,\theta\right)$ then
$\Pr\left(\left|\frac{y}{n}-\theta\right|&gt;\epsilon\mid\theta\right)\to 0$ as
$n\to\infty$.</li>
  <li>Pierre Simon Laplace and Reverend Thomas Bayes inverted the probability
statement to $\Pr(\theta\in(\theta_1,\theta_2)\mid y)$.</li>
  <li>The posterior predictive distribution can be represented as follows:</li>
</ul>

\[\begin{aligned}
\operatorname{Pr}(\tilde{y}=1 \mid y)
&amp;=\int_0^1 \operatorname{Pr}(\tilde{y}=1 \mid \theta, y) p(\theta \mid y) \dd \theta \\
\\ &amp;=\int_0^1 \theta \operatorname{Beta}\left(y+1,n-y+1\right) \dd \theta
\\ &amp;=\mathbb{E}[\theta \mid y]
\\ &amp;=\frac{y+1}{n+2}
\end{aligned}\]

<ul>
  <li>This result, based on the uniform prior distribution, is known as “Laplace’s
law of succession”. When $y=0$, this law predicts $\frac{1}{n+2}$, and when
$y=n$, this law predicts $\frac{n+1}{n+2}$.</li>
</ul>

<h1 id="22-posterior-as-compromise-between-data-and-prior-information">
<a class="anchor" href="#22-posterior-as-compromise-between-data-and-prior-information" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2 Posterior as compromise between data and prior information</h1>

<ul>
  <li>The prior mean is the expectation over posterior means, i.e.
$\mathbb{E}\left[\theta\right]=\mathbb{E}\left[\mathbb{E}\left[\theta\mid
y\right]\right]$. In other words, <strong>the prior mean of $\theta$ is the average
of all possible posterior means over the distribution of possible data</strong>,
distributed as $p(y)$.</li>
  <li>In general, during Bayesian inference which progresses form $p(\theta)$ to
$p(\theta\mid y)$, we expect that the posterior will be less variable than the
prior because we have more information. This can be seen from the
decomposition of variance, which separates into two terms:
    <ol>
      <li>The mean of the posterior variances.</li>
      <li>The variance of the posterior means.</li>
    </ol>
  </li>
</ul>

\[\begin{aligned}
\var(u)&amp;=\mathbb{E}\left[\var(u\mid v)\right] + \var(\mathbb{E}\left[u\mid v\right])
\end{aligned}\]

<ul>
  <li>This suggests that on average, the posterior variance is lower than the prior
variance because does not incorporate the variance of the posterior means over
the distribution of possible data.</li>
  <li>The greater the variation in posterior means, the more potential for reducing
uncertainty regarding our estimate of $\theta$.</li>
  <li>The posterior is always a compromise between the data and the prior.</li>
</ul>

<h1 id="23-summarizing-posterior-inference">
<a class="anchor" href="#23-summarizing-posterior-inference" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3 Summarizing posterior inference</h1>

<ul>
  <li>Commonly used summaries of location are the mean, median, and mode(s) of the
distribution.</li>
  <li>Variation is commonly summarized by the standard deviation, interquartile
range, and other quantiles.</li>
  <li>The mode or most likely posterior value is often easier to compute than the
mean or median.</li>
  <li>In addition to point summaries, it is important to report posterior
uncertainty. These usually take the form of quantiles of the posterior
distribution of the estimands of interest.</li>
  <li>The <strong>central posterior interval</strong> can differ significantly from the <strong>highest
posterior density</strong> region, as shown by the following graphic:</li>
</ul>

<figure class="fullwidth"><img src="https://media.githubusercontent.com/media/danjenson/notes/main/books/bayesian-data-analysis/figures/chapter-2/cpi-vs-hpd.png"><figcaption></figcaption></figure>

<h1 id="24-informative-prior-distributions">
<a class="anchor" href="#24-informative-prior-distributions" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.4 Informative prior distributions</h1>

<ul>
  <li>Two justifications for priors:
    <ul>
      <li>
<em>Population</em> interpretation: the prior represents a population of possible
parameter values from which $\theta$ has been drawn.</li>
      <li>
<em>State of knowledge</em> interpretation: we must express our knowledge and
uncertainty about $\theta$ as if its value could be thought of as a random
realization from a prior distribution.</li>
    </ul>
  </li>
  <li>Typically, the prior should include all plausible values of $\theta$, and even
if the prior is not centered around the true value, the data will far
outweigh <em>any</em> reasonable prior.</li>
  <li>Probability distributions that belong to an <strong>exponential family</strong> have
natural conjugate prior distributions.</li>
  <li>The class $\mathcal{F}$ is an exponential family if all its members have the
form:
    <ul>
      <li>$\theta,y_i,\phi(\theta),u(y_i)\in \mathbb{R}^n$</li>
      <li>$\phi(\theta)$ is the <strong>natural parameter</strong>
</li>
    </ul>
  </li>
</ul>

\[p(y_i\mid\theta)=f(y_i)g(\theta)e^{\phi(\theta)^\intercal u(y_i)}\]

<ul>
  <li>The likelihood corresponding to i.i.d. $y_i$ is</li>
</ul>

\[p(y\theta)=\left(\prod_{i=1}^n
f(y_i)\right)g(\theta)^n\exp\left(\phi(\theta)^\intercal\sum_{i=1}^n u(y_i)\right)\]

<ul>
  <li>And for all $n$ and $y$, this has a fixed form as a function of $\theta$ where
$t(y)=\sum_{i=1}^n u(y_i)$:</li>
</ul>

\[p(y\mid\theta)\propto g(\theta)^ne^{\phi(\theta)^\intercal t(y)}\]

<ul>
  <li>Here $t(y)$ is said to be a <strong>sufficient statistic</strong> for $\theta$ because the
likelihood for $\theta$ depends on the data $y$ only through the value of
$t(y)$.</li>
  <li>If the prior density is $p(\theta)\propto g(\theta)^\eta
\exp(\phi(\theta)^\intercal \nu)$, then the posterior is $p(\theta\mid
y)\propto g(\theta)^{\eta+n}\exp(\phi(\theta)^\intercal(\nu+t(y)))$.</li>
  <li>In general, the exponential families are the only classes of distributions
that have natural conjugate prior distributions, since, apart from certain
irregular cases, the only distributions having a fixed number of sufficient
statistics for all $n$ are of the exponential type.</li>
  <li>When applying the normal to a proportion, it is useful to logit transform it,
$\log\left(\frac{\theta}{1-\theta}\right)$ so that the unit interval becomes
the real line.</li>
</ul>

<h1 id="25-normal-distribution-with-known-variance">
<a class="anchor" href="#25-normal-distribution-with-known-variance" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.5 Normal distribution with known variance</h1>

<ul>
  <li>The central limit theorem helps to justify using the normal likelihood in many
problems as an approximation to a less analytically convenient actual
likelihood.</li>
  <li>The normal density is
$\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{1}{2\sigma^2}(y-\mu)^2\right)$</li>
  <li>Considered as a function of $\mu$, the likelihood is an exponential of a
quadratic from in $\mu$, so conjugate priors look like</li>
</ul>

\[p(\mu)=\exp\left(A\mu^2+B\mu+C\right)\]

<ul>
  <li>We parameterize this family as $\mu\sim \operatorname{Normal}\left(\mu_0,\sigma_0^2\right)$:</li>
</ul>

\[p(\mu)\propto\exp\left(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\right)\]

<ul>
  <li>The posterior is then:</li>
</ul>

\[p(\theta\mid y)\propto\exp\left(-\frac{1}{2}\left(\frac{(y-\mu)^2}{\sigma^2}+\frac{(\mu-\mu_0)^2}{\sigma_0^2}\right)\right)\]

<ul>
  <li>And, combining terms, $\theta\mid y\sim \operatorname{Normal}\left(\mu_1,\sigma_1^2\right)$</li>
</ul>

\[\begin{aligned}
p(\theta\mid y)&amp;\propto\exp\left(-\frac{1}{2\sigma_1^2}(\mu-\mu_1)^2\right) \\
\mu_1&amp;=\frac{\frac{1}{\sigma_0^2}\mu_0+\frac{1}{\sigma^2}y}{\frac{1}{\sigma_0^2}+\frac{1}{\sigma^2}} \\
\frac{1}{\sigma_1^2}&amp;=\frac{1}{\sigma_0^2}+\frac{1}{\sigma^2}
\end{aligned}\]

<ul>
  <li>The posterior mean can be expressed as a weighted average of the prior mean
and the observed value, $y$, with weights proportional to the precisions.</li>
  <li>Alternatively, we can express $\mu_1$ as the prior mean adjusted toward the
observed $y$:</li>
</ul>

\[\begin{aligned}
\mu_1&amp;=\mu_0+(y-\mu_0)\frac{\sigma_0^2}{\sigma^2+\sigma_0^2} \\
\end{aligned}\]

<ul>
  <li>Or, as the data shrunk toward the prior mean:</li>
</ul>

\[\mu_1=y-(y-\mu_0)\frac{\sigma^2}{\sigma^2+\sigma_0^2}\]

<ul>
  <li>Or, even as:</li>
</ul>

\[\mu_1=\mu_0 \left(\frac{\sigma^2}{\sigma^2+\sigma_0^2}\right)+y
\left(\frac{\sigma_0^2}{\sigma^2+\sigma_0^2}\right)\]

<ul>
  <li>This makes it clear that</li>
</ul>

\[\begin{aligned}
\mu_1&amp;=\mu_0\text{ if }y=\mu_0\text{ or }\sigma_0^2=0 \\
\mu_1&amp;=y\text{ if }y=\mu_0\text{ or }\sigma_0^2=0 \\
\end{aligned}\]

<ul>
  <li>The posterior predictive distribution can be calculated using integration:</li>
</ul>

\[\begin{aligned}
p(\tilde{y}\mid y)&amp;=\int p(\tilde{y}\mid\mu)p(\mu\mid y)\dd\mu \\
&amp;\propto\int\exp\left(-\frac{1}{2\sigma^2}(\tilde{y}-\mu)^2\right)\exp\left(-\frac{1}{2\sigma_1^2}(\mu-\mu_1)^2\right)\dd\mu
\end{aligned}\]

<ul>
  <li>You can determine the mean and variance of the posterior predictive
distribution using the knowledge from the posterior distribution that
$\mathbb{E}\left[\tilde{y}\mid\mu\right]=\mu$ and
$\operatorname{var}\left[\tilde{y}\mid\theta\right]=\sigma^2$:</li>
</ul>

\[\begin{aligned}
\mathbb{E}\left[\tilde{y}\mid y\right]
&amp;=\mathbb{E}\left[\mathbb{E}\left[\tilde{y}\mid\mu,y\right]\mid
y\right]=\mathbb{E}\left[\mu\mid y\right]=\mu_1 \\
\operatorname{var}\left[\tilde{y}\mid y\right]
&amp;= \mathbb{E}\left[\operatorname{var}\left[\tilde{y}\mid \mu,y\mid y\right]\right]+ \operatorname{var}\left[\mathbb{E}\left[\tilde{y},\mu,y\right]\mid y\right] \\
&amp;= \mathbb{E}\left[\sigma^2\mid y\right]+ \operatorname{var}\left[\mu\mid y\right] \\
&amp;= \sigma^2+\sigma_1^2
\end{aligned}\]

<ul>
  <li>Thus, the posterior predictive for $\tilde{y}$ has a mean equal to the
posterior mean of $\mu$ and variance equal to the predictive variance
$\sigma^2$ and the variance $\sigma_1^2$ due to the uncertainty in $\mu_1$.</li>
  <li>With multiple $y_i$, this becomes:</li>
</ul>

\[\begin{aligned}
p(\mu \mid y) &amp; \propto p(\mu) p(y \mid \mu) \\
&amp;=p(\theta) \prod_{i=1}^n p\left(y_i \mid \theta\right) \\
&amp; \propto \exp \left(-\frac{1}{2 \sigma_0^2}\left(\mu-\mu_0\right)^2\right) \prod_{i=1}^n \exp \left(-\frac{1}{2 \sigma^2}\left(y_i-\mu\right)^2\right) \\
&amp; \propto \exp \left(-\frac{1}{2}\left(\frac{1}{\sigma_0^2}\left(\mu-\mu_0\right)^2+\frac{1}{\sigma^2} \sum_{i=1}^n\left(y_i-\mu\right)^2\right)\right)
\end{aligned}\]

<ul>
  <li>Algebraic simplification of this shows that the posterior depends on $y$ only
through the sample mean, $\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i$; namely,
$\bar{y}$ is a <strong>sufficient statistic</strong> for the model. In fact, since
$\bar{y}\mid\mu\sigma^2\sim \operatorname{Normal}\left(\mu,\sigma^2\right)$,
the results derived for the single normal observation apply immediately
(treating $\bar{y}$) as a single observation:</li>
</ul>

\[\begin{aligned}
p(\mu\mid y_1,\ldots,y_n)=p(\mu\mid\bar{y})&amp;=\operatorname{Normal}\left(\mu\mid\mu_n,\sigma_n^2\right) \\
\mu_n&amp;=\frac{\frac{1}{\sigma_0^2}\mu_0+\frac{n}{\sigma^2}\bar{y}}{\frac{1}{\sigma_0^2}+\frac{1}{\sigma^2}} \\
\frac{1}{\sigma_n^2}&amp;=\frac{1}{\sigma_0^2}+\frac{n}{\sigma^2}
\end{aligned}\]

<ul>
  <li>As $n\to\infty$, $p(\mu\mid y)\approx \operatorname{Normal}\left(\mu\mid \bar{y},\sigma^2/n\right)$.</li>
</ul>

<h1 id="26-other-standard-single-parameter-models">
<a class="anchor" href="#26-other-standard-single-parameter-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.6 Other standard single-parameter models</h1>

<ul>
  <li>The normal distribution with known mean but unknown variance provides an
introductory example of the estimation of a scale parameter.</li>
  <li>For $p(y\mid\mu,\sigma^2)=\operatorname{Normal}\left(y\mid\mu,\sigma^2\right)$
with $\mu$ known and $\sigma^2$ unknown, the likelihood for a vector $y$ of
$n$ independent i.i.d. observations is:</li>
</ul>

\[\begin{aligned}
p\left(y \mid \sigma^2\right) &amp; \propto \sigma^{-n} \exp \left(-\frac{1}{2
\sigma^2} \sum_{i=1}^n\left(y_i-\mu\right)^2\right) \\
&amp;=\left(\sigma^2\right)^{-n / 2} \exp \left(-\frac{n}{2 \sigma^2} v\right) .
\end{aligned}\]

<ul>
  <li>The <strong>sufficient statistic</strong> is $v=\frac{1}{n}\sum_{i=1}^n(y_i-\mu)^2$, and
the corresponding conjugate prior density is the inverse-gamma
$p(\sigma^2)\propto(\sigma^2)^{(-\alpha+1)}e^{-\beta/\sigma^2}$.</li>
  <li>A convenient parameterization is as a scaled inverse-$\chi^2$ distribution with
$\sigma_0^2$ and $\nu_0$ degrees of freedom; that is, the prior distribution
of $\sigma^2$ is taken to be $\sigma_0^2\nu_0/X$ where $X$ is a
$\chi_{\nu_0}^2$ random variable. Using a convenient but non-standard
notation: $\sigma^2\sim\operatorname{Inv}-\chi^2(\nu_0,\sigma_0^2)$. The
resulting posterior is:</li>
</ul>

\[\begin{aligned}
p\left(\sigma^2 \mid y\right) \propto &amp; p\left(\sigma^2\right) p\left(y \mid \sigma^2\right) \\
\propto &amp;\left(\frac{\sigma_0^2}{\sigma^2}\right)^{\nu_0 / 2+1} \exp \left(-\frac{\nu_0 \sigma_0^2}{2 \sigma^2}\right) \cdot\left(\sigma^2\right)^{-n / 2} \exp \left(-\frac{n}{2} \frac{v}{\sigma^2}\right) \\
\propto &amp;\left(\sigma^2\right)^{-\left(\left(n+\nu_0\right) / 2+1\right)} \exp \left(-\frac{1}{2 \sigma^2}\left(\nu_0 \sigma_0^2+n v\right)\right) \\
&amp; \sigma^2 \mid y \sim \operatorname{Inv-} \chi^2\left(\nu_0+n, \frac{\nu_0 \sigma_0^2+n v}{\nu_0+n}\right)
\end{aligned}\]

<ul>
  <li>The Poisson density is $p(y\mid\theta)=\frac{\theta^y e^{-\theta}}{y!}$ for
$y=0,1,2,\ldots$ and the likelihood is</li>
</ul>

\[p(y\mid\theta)=\prod_{i=1}^n\frac{1}{y_i!}\theta^{y_i}e^{-\theta}\propto\theta^{t(y)}e^{-n\theta}\]

<ul>
  <li>Thus, $t(y)-\sum_{i=1}^ny_i$ is the <strong>sufficient statistic</strong>.</li>
  <li>As an exponential, this can be written as:</li>
</ul>

\[p(y\mid\theta)\propto e^{-n\theta}e^{t(y)\log\theta}\]

<ul>
  <li>This suggests that the natural parameter $\phi(\theta)=\log\theta$ and the
natural conjugate prior distribution is</li>
</ul>

\[p(\theta)\propto (e^{-\theta})^\eta e^{\nu\log\theta}\]

<ul>
  <li>The likelihood is of the form $\theta^ae^{-b\theta}$, so the conjugate prior
density must be of the form $p(\theta)\propto\theta^Ae^{-B\theta}$. A more
conventional parameterization would be the gamma:</li>
</ul>

\[p(\theta)\propto e^{-\beta\theta}\theta^{\alpha-1}\]

<ul>
  <li>
    <p>Thus, the posterior density is $\theta\mid y\sim
\operatorname{Gamma}\left(\alpha+n\bar{y},\beta+n\right)$</p>
  </li>
  <li>
    <p>With conjugate families, the known form of the prior and posterior densities
can be used to find the marginal distribution, $p(y)$, using the formula</p>
  </li>
</ul>

\[p(y)=\frac{p(y\mid\theta)p(\theta)}{p(\theta\mid
y)}=\frac{p(y,\theta)p(y)}{p(y,\theta)}\]

<ul>
  <li>For the Poisson with a single observation $y$, the prior predictive
distribution is $y\sim\operatorname{Negative-binomial}(\alpha,\beta)$:</li>
</ul>

\[\begin{aligned}
p(y) &amp;=\frac{\operatorname{Poisson}(y \mid \theta) \operatorname{Gamma}(\theta \mid \alpha, \beta)}{\operatorname{Gamma}(\theta \mid \alpha+y, 1+\beta)} \\
&amp;=\frac{\Gamma(\alpha+y) \beta^\alpha}{\Gamma(\alpha) y !(1+\beta)^{\alpha+y}} \\
p(y)&amp;=\binom{\alpha+y-1}{y}\left(\frac{\beta}{\beta+1}\right)^\alpha\left(\frac{1}{\beta+1}\right)^y
\end{aligned}\]

<ul>
  <li>This illustrates that the negative binomial distribution is a <em>mixture</em> of
Poisson distributions with rates, $\theta$, that follow the gamma
distribution:</li>
</ul>

\[\operatorname{Negative-binomial}(y\mid\alpha,\beta)=\int
\operatorname{Poisson}\left(y\mid\theta\right)\operatorname{Gamma}\left(\theta\mid\alpha,\beta\right)\dd\theta\]

<ul>
  <li>The Poisson can be extended to the form $y_i\sim
\operatorname{Poisson}\left(x_i\lambda\right)$ where $x_i$ are the values of
an explanatory variable called the <em>exposure</em>. $\theta$ is often called the
<em>rate</em>. Ignoring non-$\theta$, the likelihood then becomes:</li>
</ul>

\[p(y\mid\theta)\propto \theta^{\left(\sum_{i=1}^n
y_i\right)}e^{-\left(\sum_{i=1}^n x_i\right)\theta}\]

<ul>
  <li>
    <p>So, the gamma distribution for $\theta$ is conjugate, so with the prior
$\theta\sim \operatorname{Gamma}\left(\alpha,\beta\right)$, the posterior
becomes</p>

\[\theta\mid y\sim \operatorname{gamma}\left(\alpha+\sum_{i=1}^ny_i,\beta
\sum_{i=1}^nx_i\right)\]
  </li>
  <li>the exponential distribution is often used to model waiting times.
    <ul>
      <li>the density is $p(y\mid\theta)=\theta\exp(-y\theta)$ for $y&gt;0$.</li>
      <li>the exponential is a special case of the gamma distribution with parameters
$(\alpha,\beta)=(1,\theta)$.</li>
      <li>the exponential is <strong>memoryless</strong>: $\Pr(y&gt;t+s\mid
y&gt;s,\theta)=\Pr(y&gt;t\mid\theta)$.</li>
      <li>The conjugate prior for $\theta$ is $\operatorname{Gamma}\left(\alpha,\beta\right)$.</li>
      <li>The posterior is $\operatorname{Gamma}\left(\alpha+1,\beta+y\right)$</li>
    </ul>
  </li>
  <li>The sampling distribution of $n$ independent exponential observations is
$p(y\mid\theta)=\theta^n\exp(-n\bar{y}\theta)$ for $\bar{y}\ge 0$
    <ul>
      <li>When viewed as the likelihood of $\theta$ for fixed $y$, this is
proportional to $\operatorname{Gamma}\left(n+1,n\bar{y}\right)$; this can be
viewed as $\alpha-1$ exponential observations with total waiting time $\beta$.</li>
    </ul>
  </li>
</ul>

<h1 id="27-example-informative-prior-distribution-for-cancer-rates">
<a class="anchor" href="#27-example-informative-prior-distribution-for-cancer-rates" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.7 Example: informative prior distribution for cancer rates</h1>

<ul>
  <li>Introduces hierarchical modeling</li>
</ul>

<h1 id="28-noninformative-prior-distributions">
<a class="anchor" href="#28-noninformative-prior-distributions" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.8 Noninformative prior distributions</h1>

<ul>
  <li>When prior distributions have no population basis, they can be difficult to
construct, so there is a desire for “reference prior distributions”, which are
described as vague, flat, diffuse, or <em>noninformative</em>.</li>
  <li>Jeffreys’ invariance principle: by considering one-to-one transformations of
the parameter $\phi=h(\theta)$. By transformation of variables, the prior
density $p(\theta)$ is equivalent to the following:</li>
</ul>

\[p(\phi)=p(\theta)\left|\dv{\theta}{\phi}\right|=p(\theta)\left|h'(\theta)\right|^{-1}\]

<ul>
  <li>The transformed model should determine the same distribution:
$p(y,\phi)=p(\phi)p(y\mid\phi)$. This leads to the noninformative prior
density $p(\theta)\propto[J(\theta)]^{1/2}$ where $J(\theta)$ is the <strong>Fisher
information</strong> for $\theta$:</li>
</ul>

\[J(\theta)=\mathrm{E}\left(\left(\frac{d \log p(y \mid \theta)}{d \theta}\right)^2 \big\lvert \theta\right)=-\mathrm{E}\left(\frac{d^2 \log p(y \mid \theta)}{d \theta^2} \big\lvert \theta\right)\]

<ul>
  <li>To see that Jeffreys’ prior model is invariant to parameterization, evaluate
$J(\phi)$ at $\theta=$ $h^{-1}(\phi)$ :</li>
</ul>

\[\begin{aligned}
J(\phi) &amp;=-\mathrm{E}\left(\frac{d^2 \log p(y \mid \phi)}{d \phi^2}\right) \\
&amp;=-\mathrm{E}\left(\frac{d^2 \log p\left(y \mid \theta=h^{-1}(\phi)\right)}{d \theta^2}\left|\frac{d \theta}{d \phi}\right|^2\right) \\
&amp;=J(\theta)\left|\frac{d \theta}{d \phi}\right|^2
\end{aligned}\]

<ul>
  <li>When the number of parameters in a problem is large, its useful to use
hierarchical models over noninformative priors.</li>
  <li>Pivotal quantities, location and scale parameters on p.54.</li>
  <li>Problems with noninformative priors:
    <ul>
      <li>Searching for a prior is misguided if the likelihood is dominant. Blindingly
applying a reference prior is possibly inappropriate.</li>
      <li>Noninformative priors may be flat or uniform in one parameterization but not
in another.</li>
      <li>Difficulties arise when averaging over a set of competing models that have
improper prior distributions.</li>
    </ul>
  </li>
  <li>Noninformative priors are still useful when it does not seem to be worth the
effort to codify real prior knowledge.</li>
</ul>

<h1 id="29-weakly-informative-prior-distributions">
<a class="anchor" href="#29-weakly-informative-prior-distributions" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.9 Weakly informative prior distributions</h1>

<ul>
  <li>Weakly informative means that it is proper but weaker than any actual prior
knowledge that is available.</li>
  <li>Rather than trying to model complete ignorance, we prefer to use weakly
informative priors that include a small amount of real-world information –
enough to make sure the posterior makes sense.</li>
  <li>Guidance for weakly informative priors:
    <ul>
      <li>Start with some version of a noninformative prior and then add enough
information so that inferences are constrained to be reasonable.</li>
      <li>Start with a strong, highly informative prior and broaden it to account for
uncertainty in one’s prior beliefs and in the applicability of any
historically based prior distribution to new data.</li>
    </ul>
  </li>
  <li>Prior distributions should not pull inferences in any predetermined
direction. If anything, a prior that leans <em>against</em> a hypothesis might be
advisable.</li>
</ul>

</article>
    <span class="print-footer"
  >Bayesian Data Analysis - Daniel Jenson
</span>
 <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="daniel.a.jenson@gmail.com"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="https://www.linkedin.com/in/daniel-jenson-7a002a30/"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="https://github.com/danjenson"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2023 &nbsp;&nbsp;DANIEL JENSON</span></br> <br>
<span>This site created with the <a href="//github.com/danjenson/et">Edward Tufte theme for Daniel Jenson </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>

  </body>
</html>
