<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    Bayesian Data Analysis
  </title>
  <meta
    name="description"
    content="A collection of work and research by Daniel jenson."
  />

  <!-- Google Fonts -->
  <link
    href="//fonts.googleapis.com/css?family=Lato:400,400italic"
    rel="stylesheet"
    type="text/css"
  />

  <!-- https://docs.mathjax.org/en/latest/input/tex/extensions/index.html -->
  <script>
    MathJax = {
      loader: {
        load: [
          "[tex]/ams",
          "[tex]/gensymb",
          "[tex]/mathtools",
          "[tex]/physics",
        ],
      },
      tex: {
        packages: { "[+]": ["ams", "gensymb", "mathtools", "physics"] },
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
      },
      svg: {
        fontCache: "global",
      },
    };
  </script>
  <!-- https://www.mathjax.org/#gettingstarted -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
  <link rel="stylesheet" type="text/css" href="/notes/assets/css/main.css">

  <link
    rel="canonical"
    href="https://danjenson.github.io/notes/books/bayesian-data-analysis/chapter-1.html"
  />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
  <nav class="group">
     
    <a href="/notes/books">books</a>
      
    <a href="/notes/papers">papers</a>
      
    <a href="/notes/courses">courses</a>
      
    <a href="/notes/mindmaps">mindmaps</a>
     
    <button
      type="button"
      id="theme-toggle"
      onclick="modeSwitcher()"
      style="cursor: pointer"
    ></button>
  </nav>
  <script src="/notes/assets/js/theme-toggle.js"></script>
</header>

    <article class="group">
<h1 class="title">
  <a href=".">Bayesian Data Analysis</a>
</h1>
 
<h2 class="subtitle">Chapter 1: Probability and inference</h2>
 <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#11-the-three-steps-of-bayesian-data-analysis">1.1 The three steps of Bayesian data analysis</a></li>
<li class="toc-entry toc-h1"><a href="#12-general-notation-for-statistical-inference">1.2 General notation for statistical inference</a></li>
<li class="toc-entry toc-h1"><a href="#13-bayesian-inference">1.3 Bayesian inference</a></li>
<li class="toc-entry toc-h1"><a href="#14-discrete-examples-genetics-and-spell-checking">1.4 Discrete examples: genetics and spell checking</a></li>
<li class="toc-entry toc-h1"><a href="#15-probability-as-a-measure-of-uncertainty">1.5 Probability as a measure of uncertainty</a></li>
<li class="toc-entry toc-h1"><a href="#16-example-probabilities-from-football-point-spreads">1.6 Example: probabilities from football point spreads</a></li>
<li class="toc-entry toc-h1"><a href="#17-example-calibration-for-record-linkage">1.7 Example: calibration for record linkage</a></li>
<li class="toc-entry toc-h1"><a href="#18-some-useful-results-from-probability-theory">1.8 Some useful results from probability theory</a></li>
<li class="toc-entry toc-h1"><a href="#19-computation-and-software">1.9 Computation and software</a></li>
<li class="toc-entry toc-h1"><a href="#110-bayesian-inference-in-applied-statistics">1.10 Bayesian inference in applied statistics</a></li>
</ul>\[\newcommand{\op}{\operatorname}
\newcommand{\sd}{\op{sd}}
\newcommand{\var}{\op{var}}
\newcommand{\logit}{\op{logit}}
\newcommand{\J}{\op{J}}\]

<h1 id="11-the-three-steps-of-bayesian-data-analysis">
<a class="anchor" href="#11-the-three-steps-of-bayesian-data-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.1 The three steps of Bayesian data analysis</h1>

<ol>
  <li>Setting up a full probability model informed by knowledge, the problem, and
the data collection process.</li>
  <li>Conditioning on observed data and calculating the posterior distribution.</li>
  <li>Evaluating the fit and implications of the posterior.</li>
</ol>

<h1 id="12-general-notation-for-statistical-inference">
<a class="anchor" href="#12-general-notation-for-statistical-inference" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.2 General notation for statistical inference</h1>

<ul>
  <li>Two kinds of estimands:
    <ol>
      <li>Potential observable quantities, i.e. future outcomes or outcomes under
treatments not received.</li>
      <li>Parameters governing the data generating process.</li>
    </ol>
  </li>
  <li>Often, data is assumed to be exchangeable, i.e. the order doesnâ€™t matter: $y =
(y_1,y_2,\ldots,y_n)\equiv(y_2,y_n,\ldots,y_1)$.</li>
</ul>

<h1 id="13-bayesian-inference">
<a class="anchor" href="#13-bayesian-inference" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3 Bayesian inference</h1>

<ul>
  <li>Prior predictive distribution: $p(y)$.</li>
  <li>Posterior predictive distribution: $p(\tilde{y}\mid y)$. Assuming the
conditional indpendence of $y$ and $\tilde{y}$ given $\theta$:</li>
</ul>

\[\begin{aligned}
p(\tilde{y}\mid y)
&amp;=\int p(\tilde{y},\theta\mid y)\dd\theta
\\ &amp;=\int p(\tilde{y}\mid\theta, y)p(\theta\mid y)\dd\theta
\\ &amp;=\int p(\tilde{y}\mid\theta)p(\theta\mid y)\dd\theta
\end{aligned}\]

<ul>
  <li>You can rarely be sure that the model you have selected is correct.</li>
</ul>

<h1 id="14-discrete-examples-genetics-and-spell-checking">
<a class="anchor" href="#14-discrete-examples-genetics-and-spell-checking" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.4 Discrete examples: genetics and spell checking</h1>

<ul>
  <li>Spelling example is excellent, p.9-11.</li>
</ul>

<h1 id="15-probability-as-a-measure-of-uncertainty">
<a class="anchor" href="#15-probability-as-a-measure-of-uncertainty" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.5 Probability as a measure of uncertainty</h1>

<ul>
  <li>Two common notions:
    <ul>
      <li>Symmetry or exchangeability: assuming equally likely possibilities, it is
the number of favorable outcomes over total number of possibilities.</li>
      <li>Frequency: With a large number of repeated trials, one would expect this
event to happen in proportion to the number of favorable outcomes over the
total number of outcomes.</li>
    </ul>
  </li>
  <li>The Frequentist perspective embeds probability questions in a long sequence
of identical events, which runs into difficultly for rare events.</li>
  <li>Probability is a reasonable way of quantifying uncertainty for the following
reasons:
    <ol>
      <li>By analogy: physical randomness induces uncertainty, so it seems
reasonable to describe uncertainty in the language of random events.</li>
      <li>Axiomatic or normative approach: related to decision theory, this approach
places all statistical inference in the context of decision-making with
gains and losses. Then reasonable axioms (ordering, transitivity, etc)
imply that uncertainty <em>must</em> be represented in terms of probability.</li>
      <li>Coherence of bets. <em>Define</em> the probability $p$ attached by you to an
event $E$ as the fraction $p\in[0,1]$ at which you would bet \$p for a
return of \$1 if $E$ occurs. Namely, if $E$ occurs, you get \$$(1-p)$
and if $\neg E$ occurs, you lose \$p.</li>
    </ol>
  </li>
  <li><strong>Whenever there is replication, in the sense of many exchangeable units
observed, there is scope for estimating features of a probability
distribution from data and thus making the analysis more objective.</strong></li>
</ul>

<h1 id="16-example-probabilities-from-football-point-spreads">
<a class="anchor" href="#16-example-probabilities-from-football-point-spreads" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.6 Example: probabilities from football point spreads</h1>

<ul>
  <li>p. 13</li>
</ul>

<h1 id="17-example-calibration-for-record-linkage">
<a class="anchor" href="#17-example-calibration-for-record-linkage" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.7 Example: calibration for record linkage</h1>

<ul>
  <li>p.16</li>
  <li>The distribution can be thought of as a mixture of two distributions:
matching and non-matching distributions:
$p(y)=\Pr(\text{match})p(y\mid\text{match})+\Pr(\text{non-match})p(y\mid\text{non-match})$.</li>
</ul>

<h1 id="18-some-useful-results-from-probability-theory">
<a class="anchor" href="#18-some-useful-results-from-probability-theory" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.8 Some useful results from probability theory</h1>

<ul>
  <li>When $H$ refers to the set of hypotheses or assumptions used to define the
model, $p(\theta,y\mid H)=p(\theta\mid H)p(y\mid\theta,H)$.</li>
  <li>In general, we prefer to model complexity with hierarchical structure using
additional variables rather than with complicated marginal distributions,
even when the additional variables are unobserved or even unobservable.</li>
  <li>Iterated expectation first averages over the target random variable
conditional on the second and then over the conditional variable, averaging
the conditional averages:</li>
</ul>

\[\begin{aligned}
\mathbb{E}\left[u\right]
&amp;=\mathbb{E}\left[\mathbb{E}\left[u\mid v\right]\right]
\\ &amp;= \int\int u\cdot p(u,v)\dd u\dd v
\\ &amp;=\int p(v)\int u\cdot p(u\mid v)\dd u\dd v
\\ &amp;= \int \mathbb{E}\left[u\mid v\right]p(v)\dd v
\end{aligned}\]

<ul>
  <li>Equivalently:</li>
</ul>

\[\begin{aligned}
\mathbb{E}\left[u\right]
&amp;=\mathbb{E}\left[\mathbb{E}\left[u\mid v\right]\right]
\\ &amp;= \int \left[\int u\cdot f_{U\mid V}(u\mid v)\dd u\right] f_V(v)\dd v
\\ &amp;= \int \left[\int u\cdot \frac{f_{U,V}(u,v)}{f_V(v)}\dd u\right]f_V(v)\dd v
\end{aligned}\]

<ul>
  <li>
    <p>Law of total variance: $\var(u)=\mathbb{E}\left[\var(u\mid v)\right] +
\var(\mathbb{E}\left[u\mid v\right])$ (also holds for vectors/matrices).</p>

\[\begin{aligned}
\mathrm{\mathbb{E}}(\operatorname{var}(u \mid v))+\operatorname{var}(\mathrm{\mathbb{E}}(u \mid v)) &amp;=\mathrm{\mathbb{E}}\left(\mathrm{\mathbb{E}}\left(u^2 \mid v\right)-(\mathrm{\mathbb{E}}(u \mid v))^2\right)+\mathrm{\mathbb{E}}\left((\mathrm{\mathbb{E}}(u \mid v))^2\right)-(\mathrm{\mathbb{E}}(\mathrm{\mathbb{E}}(u \mid v)))^2 \\
&amp;=\mathrm{\mathbb{E}}\left(u^2\right)-\mathrm{\mathbb{E}}\left((\mathrm{\mathbb{E}}(u \mid v))^2\right)+\mathrm{\mathbb{E}}\left((\mathrm{\mathbb{E}}(u \mid v))^2\right)-(\mathrm{\mathbb{E}}(u))^2 \\
&amp;=\mathrm{\mathbb{E}}\left(u^2\right)-(\mathrm{\mathbb{E}}(u))^2 \\
&amp;=\operatorname{var}(u)
\end{aligned}\]
  </li>
  <li>Transformation of variables:
    <ul>
      <li>Let $p_u(u)$ be the density of vector $u$.</li>
      <li>Let $v=f(u)$ where $f:\mathbb{R}^n\to \mathbb{R}^n$.</li>
      <li>If $p_u$ is a discrete distribution and $f$ is a one-to-one function, then $p_v(v)=p_u(f^{-1(v)})$.</li>
      <li>If $f$ is a many-to-one function, then a sum appears on the right hand side
with one term corresponding to each of the branches of the inverse function.</li>
      <li>If $p_u$ is a continuous distribution and $f$ is a one-to-one
transformation, then the joint density of the transformed vector is
$p_v(v)=|\det\J|p_u(f^{-1}(v))$ where the $(i,j)$the entry in $\J$ is
$\pdv{u_i}{v_j}$.</li>
    </ul>
  </li>
  <li>In one dimension, the logarithm is often used to transform the parameter space
from $(0,\infty)$ to $(-\infty,\infty)$.</li>
  <li>When working with parameters defined on the open unit interval, $(0,1)$, we
often use the logistic transformation
$\logit(u)=\log\left(\frac{u}{1-u}\right)$ whose inverse is
$\logit^{-1}(v)=\frac{e^v}{1+e^v}$.</li>
  <li>Another common choice is the probit transformation, $\Phi^{-1}(u)$ where
$\Phi$ is the standard normal cumulative distribution function, to transform
from $(0,1)$ to $(-\infty,\infty)$.</li>
</ul>

<h1 id="19-computation-and-software">
<a class="anchor" href="#19-computation-and-software" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.9 Computation and software</h1>

<ul>
  <li>General approach is to fit many models, gradually increasing the complexity.</li>
  <li>
    <p>The cumulative density function:</p>

\[\begin{aligned}
F\left(v_*\right) &amp;=\operatorname{Pr}\left(v \leq v_*\right) \\
&amp;= \begin{cases}\sum_{v \leq v_*} p(v) &amp; \text { if } p \text { is discrete } \\
\int_{-\infty}^{v_*} p(v) d v &amp; \text { if } p \text { is continuous. }\end{cases}
\end{aligned}\]
  </li>
  <li>Simple example of sampling an exponential:
    <ul>
      <li>Solve $U=F(v)=1-e^{-\lambda v}$ for $v$, which yields
$-\frac{\log(1-U)}{\lambda}$, but since $1-U$ has the same distribution as
$U$, you can say $v=-\frac{\log(U)}{\lambda}$. Then, you can simulate random
uniforms to generate exponentials.</li>
    </ul>
  </li>
  <li>Chart of sample indexing on p.24.</li>
</ul>

<h1 id="110-bayesian-inference-in-applied-statistics">
<a class="anchor" href="#110-bayesian-inference-in-applied-statistics" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.10 Bayesian inference in applied statistics</h1>

<ul>
  <li>Benefits of a Bayesian approach:
    <ul>
      <li>Flexibility in combining multiple levels of uncertainty and sources of information.</li>
      <li>Most intervals are interpreted naturally in a Bayesian sense.</li>
      <li>If the Bayesian answers vary dramatically over a range of scientifically
reasonable assumptions athat are unassailable by the data, then the resultant
range of possible conclusions must be entertained as legitimate.</li>
    </ul>
  </li>
  <li>Other important themes:
    <ul>
      <li>A willingness to use many parameters.</li>
      <li>Hierarchical modeling, which is essential for partial pooling of estimates
and compromising scientifically between alternative sources of information.</li>
      <li>Model checking.</li>
      <li>An emphasis on inference in the form of distributions or at least intervals
rather than point estimates.</li>
      <li>The use of simulation as the primary method of computation.</li>
      <li>The importance of including as much background information as possible.</li>
      <li>The importance of designing studies that have the property that inferences
for estimands of interest will be robust to model assumptions.</li>
    </ul>
  </li>
</ul>

</article>
    <span class="print-footer"
  >Bayesian Data Analysis - Daniel Jenson
</span>
 <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="daniel.a.jenson@gmail.com"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="https://www.linkedin.com/in/daniel-jenson-7a002a30/"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="https://github.com/danjenson"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2023 &nbsp;&nbsp;DANIEL JENSON</span></br> <br>
<span>This site created with the <a href="//github.com/danjenson/et">Edward Tufte theme for Daniel Jenson </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>

  </body>
</html>
