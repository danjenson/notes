---
title: Papers
---

- [Attention is All You Need](attention-is-all-you-need): Details the transformer architecture and describes the key-value-based attention mechanism.
- [Deep Recurrent Q-Learning with Double Q-Learning](deep-recurrent-q-learning-with-double-q-learning): Using the same network to select and evaluate the maximum value action in a given state leads to overestimation. This paper corrects that by training separate networks for selection and evaluation and periodically swapping them.
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](dropout)
- [Dueling Network Architectures for Deep Reinforcement Learning](dueling-network-architectures-for-deep-reinforcement-learning): This paper improves performance on the Atari benchmark by training two models -- one for the value of a state and another for the advantage ($$Q_\pi(s,a) - V_\pi(s)$$ for a given policy). This allows sharing the value of a state between many actions and allows training to scale better as the number of actions per state increases.
- [Efficient Estimation of Word Representations in Vector Space](word2vec)
- [Flow Network based Generative Models for Non Iterative Diverse Candidate Generation](flow-network-based-generative-models-for-non-iterative-diverse-candidate-generation)
- [GFlowNet Foundations](gflownet-foundations)
- [Graph Representations for Higher-Order Logic and Theorem Proving](graph-representations-for-higher-order-logic-and-theorem-proving): Use GNNs to embed goals and premises and use it to select tactics and premises at each step.
- [HOList: An Environment for Machine Learning of Higher-Order Theorem Proving](holist): Presents an open source environment for higher-order theorem proving and a reinforcement learning based model trained on it.
- [How to Read a Paper](how-to-read-a-paper)
- [Layer Normalization](layer-normalization)
- [Learning to Prove Theorems via Interacting with Proof Assistants](learning-to-prove-theorems-via-interacting-with-proof-assistants)
- [Learning to Reason in Large Theories without Imitation](learning-to-reason-in-large-theories-without-imitation)
- [Magnetic control of tokamak plasmas through deep reinforcement learning](magnetic-control-of-tokamak-plasmas-through-deep-reinforcement-learning)
- [Multi-Objective Bayesian Optimization over High-Dimensional Search Spaces](mulit-objective-bayesian-optimization-over-high-dimensional-search-spaces)
- [Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems](neurocompositional-computing): The two types of thinking are Compositional and Continuous and incorporating the former will enable neural networks to reason. This discusses a novel embedding using Tensor Product Representations (TPR) that allows continuous vector representations of compositional objects. This leads to a type of computing called Neurally-Encoded Compositionally-Structured Tensor (NECST) computing. The fundamental idea is that structural roles are encoded along with the terms.
- [Playing Atari with Deep Reinforcement Learning](playing-atari-with-deep-reinforcement-learning): Presents the first Deep Q-Network (DQN) to learn control policies for the Atari benchmark.
- [Prioritized Experience Replay](prioritized-experience-replay): Prioritizing the replay buffer by the magnitude of the temporal-difference error leads to better performance.
- [Rainbow: Combining Improvements in Deep Reinforcement Learning](rainbow)
- [Tabular Data: Deep Learning is Not All You Need](tabular-data)
- [The Consciousness Prior](the-consciousness-prior)
