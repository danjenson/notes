---
title: Papers
---

- [$\pi$VAE: a stochastic process prior for Bayesian deep learning with MCMC](pi-vae): Trains an encoder using variational autoencoding to represent a low dimensional, non-correlated latent space, and then uses the decoder to generate samples from the prior. Once trained, this can be far more efficient for estimating Bayesian posteriors.
- [A large-scale analysis of racial disparities in police stops across the United States](100M): Tests for racial bias using the veil of darkness, outcome, threshold, and difference-in-difference tests and shows consistent bias against Black Americans.
- [Attention is All You Need](attention-is-all-you-need): Details the transformer architecture and describes the key-value-based attention mechanism.
- [Bayesian Probabilistic Numerical Integration with Tree-Based Models](bart): This uses tree-based models to solve numerical integration problems and quantify their uncertainty about the solution.
- [Comments on "Statistical inference with non-probability survey samples"](ddc): This paper delineates the difference between design, divine, and device probabilities. I believe the central idea is to use partial information of a sample to build a sampler that can counterbalance the bias in the original sample. It suggests this should be done through a metric termed data defect correlation (ddc).
- [Continual Learning via Local Module Composition](lcm): This paper explores composing modules with soft attention to solve various tasks where the task type or ID is not available at training or test time. Using a new type of model called Local Module Composition (LMC), they achieve competitive performance to models that have access to an oracle with true task types. One interesting aspect of this model's architecture is that it has functional and structural components and each submodule estimates for each input how relevant that module is to the input and then performs a softmax attention over these likelihood scores between modules.
- [Deep Recurrent Q-Learning with Double Q-Learning](deep-recurrent-q-learning-with-double-q-learning): Using the same network to select and evaluate the maximum value action in a given state leads to overestimation. This paper corrects that by training separate networks for selection and evaluation and periodically swapping them.
- [DeepWalk: Online Learning of Social Representations](deepwalk): Generates node embeddings using random walks on a graph. These word embeddings are based on the skip-gram word2vec model, where nodes are used to predict their neighbors on a random walk. This uses hierarchical softmax for the objective function optimized with stochastic gradient descent. This can likely be sped up further with noise contrastive estimation, i.e. negative sampling in the objective function.
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](dropout): Dropout is a form of regularization for neural networks that prevents overfitting and approximately combines exponentially many different neural network architectures (ensembles). At training time, nodes and their connections are dropped with probability $p$ and then at test time, their weights are multiplied by $p$. The insight was gleaned from analyzing sexual vs asexual reproduction and noting that sexual reproduction has been highly successful in developing advanced organisms, suggesting that complex co-adaptation can harm the adaptability of an organism. Dropout networks take 2-3 times longer to train due to the noisy gradient updates. In a linear regression, dropout can be shown to be equivalent to ridge regression. Dropout combined with maxnorm regularization, large decaying learning rates, and high momentum provides significant boosts over dropout alone, due to more aggressive exploration that is tempered by dropout and maxnorm.
- [Dueling Network Architectures for Deep Reinforcement Learning](dueling-network-architectures-for-deep-reinforcement-learning): This paper improves performance on the Atari benchmark by training two models -- one for the value of a state and another for the advantage ($$Q_\pi(s,a) - V_\pi(s)$$ for a given policy). This allows sharing the value of a state between many actions and allows training to scale better as the number of actions per state increases.
- [Efficient Estimation of Word Representations in Vector Space](word2vec): Creating word vectors based on the continuous bag-of-words (CBOW) and skip-gram models dramatically improves performance on word representations in NLP tasks. CBOW predicts the current word from context and skip-gram predicts the context from the current word.
- [Efficient graphlet kernels for large graph comparison](graphlets): Describes efficient algorithms for calculating graphlets of size 2-5 for use as feature vectors in a graph kernel, i.e. $k(G, G')=\phi(G)^\intercal\phi(G')$. This improved performance slightly relative to random walks on some datasets, but still required a lot of time to compute.
- [Flow Network based Generative Models for Non Iterative Diverse Candidate Generation](flow-network-based-generative-models-for-non-iterative-diverse-candidate-generation): Original paper on generative flow networks.
- [GFlowNet Foundations](gflownet-foundations): Foundations of losses and uses for GFlowNets.
- [GloVe: Global Vectors for Word Representations](glove): Uses global statistics to create embeddings for words, which improves performance over word2vecs local context-based method.
- [Global disparities in SARS-Cov-2 genomic surveillance](genomic-surveillance): Details the genomic surveillance across countries and demonstrates that low/middle-income counties sequence at a much lower rate, decreasing their likelihood of catching and addressing a variant before it spreads.
- [Graph Attention Networks](gat): A Graph Neural Network that differentially attends to its neighbors based on features. Unlike GraphSAGE, this does not require sampling the local neighborhood. Attention is from every node to every other node, but can be constrained using masking to attend only to nodes in the neighborhood. Operations can be parallelized across nodes and edges.
- [Graph Representations for Higher-Order Logic and Theorem Proving](graph-representations-for-higher-order-logic-and-theorem-proving): Use GNNs to embed goals and premises and use it to select tactics and premises at each step.
- [Hierarchical Graph Representation Learning with Differentiable Pooling](hierarchical-gnns): This introduces Differential Pooling (DiffPool) where by nodes from the previous layer are coarsened to produce learned clusters. This can be applied several times to achieve the desired level of resolution. The final graph embedding can be used for downstream prediction tasks.
- [HOList: An Environment for Machine Learning of Higher-Order Theorem Proving](holist): Presents an open source environment for higher-order theorem proving and a reinforcement learning based model trained on it. 4-hop GNNs dramatically improve performance on creating embeddings for automated theorem provers (ATPs).
- [Inductive Representation Learning on Large Graphs](graphsage): Introduces GraphSAGE, which trains K aggregator functions and their weight functions, which are used to propagate information between different layers or depths of the model. This enables induction at test time since there are only embeddings for functions, rather than nodes themselves. To speed up training, a nodes neighborhood is sampled.
- [How to Read a Paper](how-to-read-a-paper)
- [Layer Normalization](layer-normalization): Layer normalization or normalizing across the input to a given hidden layer is faster and more effective than batch normalization (normalizing by feature across the batch). Both methods combat "covariate shift" and stabilize learning, but layer normalization is more easily applied with varying batch sizes and for RNNs that have indeterminate sequence lengths.
- [Learning to Prove Theorems via Interacting with Proof Assistants](learning-to-prove-theorems-via-interacting-with-proof-assistants): Introduces CoqGym for training deep learning models using the interactive theorem prover Coq. It also introduces ASTactic, a deep neural network that generates tactics and hence proofs in Coq using a top-down TreeLTSM.
- [Learning to Reason in Large Theories without Imitation](learning-to-reason-in-large-theories-without-imitation): Premise selection and exploration using TF-IDF outperforms models trained solely on human proofs and approaches hybrid performance.
- [Magnetic control of tokamak plasmas through deep reinforcement learning](magnetic-control-of-tokamak-plasmas-through-deep-reinforcement-learning): This used reinforcement for a continuous control situation to keep plasma in a state that could be used for power generation.
- [Mobility network models of COVID-19 explain inequities and inform reopening](covid-gnn): Using mobility data on a graph neural network overlaid with a basic epidemiological model, the authors will able to predict infection rates by location with high accuracy. These models also allowed estimating policy impacts on economic and health metrics.
- [Multi-Objective Bayesian Optimization over High-Dimensional Search Spaces](mulit-objective-bayesian-optimization-over-high-dimensional-search-spaces)
- [Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems](neurocompositional-computing): The two types of thinking are Compositional and Continuous and incorporating the former will enable neural networks to reason. This discusses a novel embedding using Tensor Product Representations (TPR) that allows continuous vector representations of compositional objects. This leads to a type of computing called Neurally-Encoded Compositionally-Structured Tensor (NECST) computing. The fundamental idea is that structural roles are encoded along with the terms.
- [Posterior samples of source galaxies in strong gravitational lenses with score-based-priors](galaxy-scores): Uses score based variational training techniques to create priors for images of galaxies corrupted by gravitational lenses.
- [Playing Atari with Deep Reinforcement Learning](playing-atari-with-deep-reinforcement-learning): Presents the first Deep Q-Network (DQN) to learn control policies for the Atari benchmark.
- [Pretraining Representations for Data-Efficient Reinforcement Learning](pretraining-rl): Pretrains an encoder using Self-predictive Representations (SPR), Inverse Modeling, and Goal-conditioned-RL, together denoted SGI (SPR, Goal-conditioned RL, and Inverse Modeling). It performs competitively and often better than other recent frameworks such as APT and CPT. Inverse modeling seems to be the most important aspect; the authors hypothesize this prevents representational collapse.
- [Prioritized Experience Replay](prioritized-experience-replay): Prioritizing the replay buffer by the magnitude of the temporal-difference error leads to better performance.
- [RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space](rotate): Uses a complex embedding space where relations are represented as rotations in the complex space. These models are capable of symmetry, antisymmetry, inversion, and composition, unlike many predecessor architectures and achieves SOTA on a number of benchmarks.
- [Seq2Seq Surrogates of Epidemic Models to Facilitate Bayesian Inference](seq2seq-bayes): Because epidemic models can scale poorly with the number of parameters, particularly when simulating Individual-Based Model (IBM) frameworks, parameter inference can take a long time. Training a surrogate seq2seq model on longitudinal (daily) data can generate aggregate predictions that can be used for Bayesian inference and faster policy exploration.
- [Structure Learning of Probabilistic Graphical Models: A Comprehensive Survey](sl-pgm-survey): Overview of constraint-based, score-based, and regression-based algorithms for structural learning as of 2007.
- [The Consciousness Prior](the-consciousness-prior): the consciousness prior can be thought of as a bottleneck that routes information among mental modules. "Whereas the sparse factor graph constraint is about the underlying beliefs about the world (when expressed with the high-level variables), the attention mechanisms used to build conscious thoughts are part of the inference mechanisms used to compute efficiently according to the consciousness prior."
- [Toward Causal Representation Learning](causal-representation-learning): Attempts an overview of causal learning from a slightly machine learning-theoretic perspective and makes a key distinction between statistical and causal learning, which is that statistical learning relies on IID assumptions, while casual learning can handle out of distribution changes. A curious observation is using counterfactuals to better train RL agents. Promotes the notions of Independent Causal Mechansims (ICMs), which states that independent modules shouldn't affect others, and Sparse Mechanism Shift (SMS), which suggests that localized changes should only affect the particular casual module that is in play and not all factors. Also makes a distinction between casual induction (inferring structure) and causal inference (applying a causal model). The top 3 proposed problems are: (1) learning disentangled representations, i.e. correct Bayesian factorizations, (2) learning transferable mechanisms, and (3) learning interventional world models and reasoning. It bears mentioning that this paper adopts the interventionist perspective on causality.
- [Weisfeiler-Lehman Graph Kernels](WL-kernel): Proposes the Weisfeiler-Lehman subtree, edge-based, and shortest-path graph kernels. These are all based on the color-refinement algorithm but use different calculated features.
